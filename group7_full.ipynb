{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "group7-full.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Yghh1G-_lasS",
        "UzYWAobwKQUm",
        "Eaa3lWEwLwb5",
        "ttiBQ8_bi3Tb"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/morcellinus/Introduction_to_Deep_Learning/blob/main/group7_full.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yghh1G-_lasS"
      },
      "source": [
        "# **1. Important Module Explanation**\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5ztvxCg0Oi9"
      },
      "source": [
        "## <mark>1. Attention module to predict Publisher & User Credibility</mark> </br>\n",
        "---\n",
        "\n",
        "</br>\n",
        "\n",
        "## $ Z_h = Attention(Q, K, V)  = softmax\\Big(\\frac{QW_hK^T}{\\sqrt{d}} \\odot(D^p)^\\frac{-1}{2}A^{pn}(D^n)^\\frac{-1}{2}\\Big)V$\n",
        "</br>\n",
        "\n",
        "* #### `linear1 = torch.einsum(\"bd,dd,sd->bs\", X_user, Wcm, M) / self.scale` : inner product of matrices Q, W, and K  ($\\frac{QW_hK^T}{\\sqrt{d}} $). </br>\n",
        "* #### `A_us = self.A_us[X_user_id.cpu(), :].todense()`: node adjacency matrix in the equation ($A^{pn}$).\n",
        "\n",
        "</br>\n",
        "\n",
        "#### For our paper, Q is publisher embeddings, K and V are news embeddings\n",
        "</br>\n",
        "\n",
        "```python\n",
        "     def retweet_user_multi_head(self, X_ruser, X_ruser_id, Wam):\n",
        "        # News embeddings as well for Key (4.2_equation(6))\n",
        "        M = self.user_embedding.weight\n",
        "\n",
        "        # Linear inner product inside Attention(R, U, U)\n",
        "        linear1 = torch.einsum(\"bnd,dd,md->bnm\", X_ruser, Wam, M) / self.scale # m x bsz\n",
        "        # X_ruser = Query, Wam = W, M = Key\n",
        "        linear1 = self.relu(linear1)\n",
        "\n",
        "        s1, s2 = X_ruser_id.size()\n",
        "\n",
        "        idx = X_ruser_id.view(-1).cpu()\n",
        "\n",
        "        # A_uu: Adjecency matrix (A_uu in 4.2_equation(6))\n",
        "        A_uu = self.A_uu[idx, :].todense()\n",
        "        A_uu = torch.FloatTensor(A_uu).view(s1, s2, -1).cuda()\n",
        "        # Attention(R_j, U, U), and get Z_h\n",
        "        alpha = F.softmax(linear1 * A_uu, dim=-1) \n",
        "        alpha = self.dropout(alpha)\n",
        "        return alpha.matmul(M)\n",
        "```\n",
        "\n",
        "</br>\n",
        "\n",
        "```python\n",
        "def retweet_user_multi_head(self, X_ruser, X_ruser_id, Wam):\n",
        "        # News embeddings as well for Key (4.2_equation(6))\n",
        "        M = self.user_embedding.weight\n",
        "\n",
        "        # Linear inner product inside Attention(R, U, U)\n",
        "        linear1 = torch.einsum(\"bnd,dd,md->bnm\", X_ruser, Wam, M) / self.scale # m x bsz\n",
        "        # X_ruser = Query, Wam = W, M = Key\n",
        "        linear1 = self.relu(linear1)\n",
        "\n",
        "        s1, s2 = X_ruser_id.size()\n",
        "\n",
        "        idx = X_ruser_id.view(-1).cpu()\n",
        "\n",
        "        # A_uu: Adjecency matrix (A_uu in 4.2_equation(6))\n",
        "        A_uu = self.A_uu[idx, :].todense()\n",
        "        A_uu = torch.FloatTensor(A_uu).view(s1, s2, -1).cuda()\n",
        "        # Attention(R_j, U, U), and get Z_h\n",
        "        alpha = F.softmax(linear1 * A_uu, dim=-1) \n",
        "        alpha = self.dropout(alpha)\n",
        "        return alpha.matmul(M)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2809gPK06Hmh"
      },
      "source": [
        "## <mark>2. Publisher & User Representation Encoder</mark> </br>\n",
        "---\n",
        "### **Publisher Representation**\n",
        "## $ \\tilde{P} = ELU([Z_1;Z_2; ...;Z_H]W_o) + P $\n",
        "</br>\n",
        "\n",
        "### **User Representation**\n",
        "## $\\tilde{R_j} = ELU([Z_1;Z_2;...;Z_H]W_o) + R_j$\n",
        "\n",
        "</br>\n",
        "\n",
        "#### Here, $Z_i$'s are the results from Attention module above.</br>\n",
        "#### Then these results go through $ELU$ activation function, then added with Publisher(user) embeddings\n",
        "\n",
        "* #### `m_hat = torch.cat(m_hat, dim=-1).matmul(self.W1)` : Concat $Z_i$'s </br> \n",
        "* #### `m_hat = self.elu(m_hat)`: $ELU$ activation function </br> \n",
        "* #### `U_hat = m_hat + X_user`: Add Publisher(User) embedding\n",
        "</br>\n",
        "\n",
        "```python\n",
        "    # Publisher Representation Encoder from Publisher Attention result Z_h\n",
        "    def publisher_encoder(self, X_user, X_user_id):\n",
        "        m_hat = []\n",
        "        # Get Z_h from Attention\n",
        "        for i in range(self.n_heads):\n",
        "            m_hat.append(self.user_multi_head(X_user, X_user_id, self.Wcm[i]))\n",
        "\n",
        "        # Concat Z1 to ZJ\n",
        "        m_hat = torch.cat(m_hat, dim=-1).matmul(self.W1)\n",
        "        # ELU activation function\n",
        "        m_hat = self.elu(m_hat)\n",
        "        m_hat = self.dropout(m_hat)\n",
        "        #U_hat = publisher's representation\n",
        "        U_hat = m_hat + X_user  \n",
        "        return U_hat\n",
        "\n",
        "    # User Representation Encoder from User Attention result Z_h\n",
        "    def retweet_user_encoder(self, X_ruser, X_ruser_id): \n",
        "        '''\n",
        "        :param X_ruser:  (bsz, num_users, d)\n",
        "        :param X_ruser_id: (bsz, num_users)\n",
        "        :return:\n",
        "        '''\n",
        "        m_hat = []\n",
        "        # Get Z_h from Attention\n",
        "        for i in range(self.n_heads):\n",
        "            m_hat.append(self.retweet_user_multi_head(X_ruser, X_ruser_id, self.Wam[i]))\n",
        "\n",
        "        # Concat Z_1j to Z_Hj\n",
        "        m_hat = torch.cat(m_hat, dim=-1).matmul(self.W2)\n",
        "        # ELU activation function\n",
        "        m_hat = self.elu(m_hat)\n",
        "        m_hat = self.dropout(m_hat)\n",
        "\n",
        "        # a_hat = retweet users' representation\n",
        "        a_hat = m_hat + X_ruser  # bsz x 20 x d\n",
        "        return a_hat\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCu7WqZ-AURw"
      },
      "source": [
        "## <mark>3. Publisher & User Representation Fusion</mark> </br>\n",
        "---\n",
        "\n",
        "### **Source representation (Fusion between Publisher and User representation)**\n",
        "\n",
        "</br>\n",
        "\n",
        "## $\\tilde{m_j} = [\\tilde{P};R';\\tilde{P} \\odot R';\\tilde{P}-R']W_F + b_F$ <br>\n",
        "where $R' = \\sum_{k=1}^{K}{a_k\\tilde{{R_i}^T}}$\n",
        "\n",
        "</br>\n",
        "\n",
        "#### This finds the differences between fake and true news from the graph-based publisher & User credibility\n",
        "\n",
        "\n",
        "* #### `retweet_rep = torch.einsum(\"bn,bnd->bd\", alpha, r_user_rep)` :  Get  $R'$</br> \n",
        "\n",
        "* \n",
        "```python\n",
        "def source_encoder(self, X_source, r_user_rep, user_rep): \n",
        "\n",
        "        # Combine many user representation to one news into one user representation to one news, which is R'\n",
        "        # linear1 = R_k_tilda\n",
        "        linear1 = torch.einsum(\"bd,bnd->bn\", X_source, r_user_rep) # / self.scale\n",
        "        alpha = F.softmax(linear1, dim=-1)\n",
        "        # retweet_rep = R'\n",
        "        retweet_rep = torch.einsum(\"bn,bnd->bd\", alpha, r_user_rep)\n",
        "\n",
        "        # get the combined representation (Fusion)\n",
        "        source_rep = torch.cat([retweet_rep, user_rep,\n",
        "                                retweet_rep * user_rep,\n",
        "                                retweet_rep - user_rep], dim=-1)  # .mm(self.W) #\n",
        "        source_rep = self.linear(source_rep)\n",
        "        source_rep = self.dropout(source_rep)\n",
        "        return source_rep\n",
        "```\n",
        "#### : Get $\\tilde{m_j}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGTNDz2ZFA5Y"
      },
      "source": [
        "## <mark>4. Combined Loss Function to Optimize Each Process</mark> </br>\n",
        "---\n",
        "</br>\n",
        "\n",
        "#### There exist three different losses in this paper since there are three things to predict \n",
        "\n",
        "</br>\n",
        "\n",
        "**1) Publisher Credibility Class (reliable, uncretain, unreliable)** </br>\n",
        "**2) User Credibility Class (as same as Publisher)**</br>\n",
        "**3) News class (NR, FR, UR, TR)**\n",
        "\n",
        "</br>\n",
        "\n",
        "#### They are combined together to optimize the credibility prediction and fake news detection\n",
        "\n",
        "</br>\n",
        "\n",
        "## $\\mathcal{L}\\big(c|\\mathcal{G}(V_p, E), \\mathcal{G}(V_u, E), \\mathcal{N};\\theta\\big) = \\mathcal{L_p} + \\mathcal{L_u} + \\mathcal{L_n}$\n",
        "</br>\n",
        "\n",
        "#### $\\mathcal{L_p}$: Cross Entropy Loss for Publisher Credibility \n",
        "#### $\\mathcal{L_u}$: Cross Entropy Loss for User Credibility\n",
        "#### $\\mathcal{L_n}$: Cross Entropy Loss for Fake News Detection\n",
        "\n",
        "</br>\n",
        "\n",
        "* #### `loss = loss1 + pub_loss + uloss`: Combining loss functions\n",
        "\n",
        "</br>\n",
        "\n",
        "```python\n",
        "# News Class Prediction, Publisher & Retweet User Credibilities Prediction\n",
        "                logit, ulogit, rulogit = self.forward(X_source_wid, X_source_id, X_user_id, X_ruid)\n",
        "\n",
        "                # loss for fake news detection\n",
        "                loss1 = loss_func(logit, batch_y)\n",
        "\n",
        "                # loss for publisher's credibility prediction\n",
        "                pub_loss = loss_func(ulogit, batch_y_cred)\n",
        "\n",
        "                # loss for user's credibility prediction\n",
        "                uloss = loss_func2(rulogit.view(-1, rulogit.size(-1)), batch_y_rucred.view(-1))\n",
        "\n",
        "                # Loss sum\n",
        "                loss = loss1 + pub_loss + uloss\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzYWAobwKQUm"
      },
      "source": [
        "# 2. **Entire Model Definition**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K74AKsESL7mK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c76f6493-781f-4e4b-856b-9777b3791d75"
      },
      "source": [
        "!git clone https://github.com/chunyuanY/FakeNewsDetection\n",
        "\n",
        "%cd FakeNewsDetection"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'FakeNewsDetection'...\n",
            "remote: Enumerating objects: 73, done.\u001b[K\n",
            "remote: Total 73 (delta 0), reused 0 (delta 0), pack-reused 73\u001b[K\n",
            "Unpacking objects: 100% (73/73), done.\n",
            "Checking out files: 100% (59/59), done.\n",
            "/content/FakeNewsDetection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8sP0mAhXJ1E"
      },
      "source": [
        "\n",
        "## 2-1. Neural Network class Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbIPoPdnKLWK"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils as utils\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import TensorDataset\n",
        "class NeuralNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.best_acc = 0\n",
        "        self.patience = 0\n",
        "\n",
        "        # Gradient Clipping condition set\n",
        "        self.init_clip_max_norm = 5.0\n",
        "        self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    def forward(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    # Fitting the Model\n",
        "    def fit(self, X_train_source_wid, X_train_source_id, X_train_user_id, X_train_ruid, y_train, y_train_cred, y_train_rucred,\n",
        "            X_dev_source_wid, X_dev_source_id, X_dev_user_id, X_dev_ruid, y_dev):\n",
        "      \n",
        "      # X_train_source_wid : Source Representation word ID\n",
        "      # X_train_source_id : Source Representation ID\n",
        "      # X_train_user_id : Publisher Representation ID\n",
        "      # X_train_ruid : User Representation ID\n",
        "      # y_train : News Class\n",
        "      # y_train_cred: Publisher Credibility Class\n",
        "      # y_train_rucred: User Credibility Class\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            self.cuda()\n",
        "\n",
        "        # batch size\n",
        "        batch_size = self.config['batch_size']\n",
        "\n",
        "        # User Adam optimizer as the optimizer, and use learning rate and weight_decay to prevent overfitting\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.config['lr'], weight_decay=self.config['reg'], amsgrad=True)  # self.optimizer = torch.optim.Adadelta(self.parameters(), weight_decay=self.config['reg'])\n",
        "\n",
        "        # Tensorize ID Embeddings\n",
        "        X_train_source_wid = torch.LongTensor(X_train_source_wid)\n",
        "        X_train_source_id = torch.LongTensor(X_train_source_id)\n",
        "        X_train_user_id = torch.LongTensor(X_train_user_id)\n",
        "        X_train_ruid = torch.LongTensor(X_train_ruid)\n",
        "\n",
        "        y_train = torch.LongTensor(y_train)\n",
        "        y_train_cred = torch.LongTensor(y_train_cred)\n",
        "        y_train_rucred = torch.LongTensor(y_train_rucred)\n",
        "\n",
        "        dataset = TensorDataset(X_train_source_wid, X_train_source_id, X_train_user_id, X_train_ruid, y_train, y_train_cred, y_train_rucred)\n",
        "\n",
        "        # dataloader to split the data into batch size\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        # Cross Entropy Loss for Fake News Detection and Publisher Credibility Scores Prediction\n",
        "        loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Cross Entropy Loss for User Credibility Scores Prediction\n",
        "        loss_func2 = nn.CrossEntropyLoss(ignore_index=3)\n",
        "\n",
        "        ###############\n",
        "        # Let's Train #\n",
        "        ###############\n",
        "\n",
        "        for epoch in range(1, self.config['epochs']+1):\n",
        "            print(\"\\nEpoch \", epoch, \"/\", self.config['epochs'])\n",
        "            self.train()\n",
        "            avg_loss = 0\n",
        "            avg_acc = 0\n",
        "            for i, data in enumerate(dataloader):\n",
        "                with torch.no_grad():\n",
        "                    X_source_wid, X_source_id, X_user_id, X_ruid, batch_y, batch_y_cred, batch_y_rucred = (item.cuda(device=self.device) for item in data)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                # News Class Prediction, Publisher & Retweet User Credibilities Prediction\n",
        "                logit, ulogit, rulogit = self.forward(X_source_wid, X_source_id, X_user_id, X_ruid)\n",
        "\n",
        "                # loss for fake news detection\n",
        "                loss1 = loss_func(logit, batch_y)\n",
        "\n",
        "                # loss for publisher's credibility prediction\n",
        "                pub_loss = loss_func(ulogit, batch_y_cred)\n",
        "\n",
        "                # loss for user's credibility prediction\n",
        "                uloss = loss_func2(rulogit.view(-1, rulogit.size(-1)), batch_y_rucred.view(-1))\n",
        "\n",
        "                # Loss sum\n",
        "                loss = loss1 + pub_loss + uloss\n",
        "\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                # Calculating accuracy\n",
        "                corrects = (torch.max(logit, 1)[1].view(batch_y.size()).data == batch_y.data).sum()\n",
        "                accuracy = 100*corrects/len(batch_y)\n",
        "                print('Batch[{}] - loss: {:.6f}  acc: {:.4f}%({}/{})'.format(i, loss.item(), accuracy, corrects, batch_y.size(0)))\n",
        "\n",
        "                avg_loss += loss.item()\n",
        "                avg_acc += accuracy\n",
        "\n",
        "                # Gradient Clipping\n",
        "                if self.init_clip_max_norm is not None:\n",
        "                    utils.clip_grad_norm_(self.parameters(), max_norm=self.init_clip_max_norm)\n",
        "\n",
        "            # Print out average loss and accuracy after each epochs\n",
        "            cnt = y_train.size(0) // batch_size + 1\n",
        "            print(\"Average loss:{:.6f} average acc:{:.6f}%\".format(avg_loss/cnt, avg_acc/cnt))\n",
        "\n",
        "            # If the epoch is half way done but still the model updates do not occur, redesign learning rate\n",
        "            if epoch > self.config['epochs']//2 and self.patience > 2: #\n",
        "                print(\"Reload the best model...\")\n",
        "                self.load_state_dict(torch.load(self.config['save_path']))\n",
        "                now_lr = self.adjust_learning_rate(self.optimizer)\n",
        "                print(now_lr)\n",
        "                self.patience = 0\n",
        "            \n",
        "            # Evaluating validation set\n",
        "            self.evaluate(X_dev_source_wid, X_dev_source_id, X_dev_user_id, X_dev_ruid, y_dev, epoch)\n",
        "     \n",
        "\n",
        "    # Function needed to adjust learning rate\n",
        "    def adjust_learning_rate(self, optimizer, decay_rate=.5):\n",
        "        now_lr = 0\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = param_group['lr'] * decay_rate\n",
        "            now_lr = param_group['lr']\n",
        "        return now_lr\n",
        "\n",
        "\n",
        "    # Function needed to evaluate validation set\n",
        "    def evaluate(self, X_dev_source_wid, X_dev_source_id, X_dev_user_id, X_dev_ruid, y_dev, epoch):\n",
        "        y_pred = self.predict(X_dev_source_wid, X_dev_source_id, X_dev_user_id, X_dev_ruid)\n",
        "        acc = accuracy_score(y_dev, y_pred)\n",
        "        print(\"Val set acc:\", acc)\n",
        "        print(\"Best val set acc:\", self.best_acc)\n",
        "\n",
        "        # save the model when the performance gets better & reset patience\n",
        "        if epoch >= self.config['epochs']//2 and acc > self.best_acc:  #\n",
        "            self.best_acc = acc\n",
        "            self.patience = 0\n",
        "            torch.save(self.state_dict(), self.config['save_path'])\n",
        "            print(classification_report(y_dev, y_pred, target_names=self.config['target_names'], digits=5))\n",
        "            print(\"save model!!!\")\n",
        "        \n",
        "        \n",
        "        else:\n",
        "            self.patience += 1\n",
        "\n",
        "\n",
        "    # Function to evaluate test dataset\n",
        "    def predict(self, X_dev_source_wid, X_dev_source_id, X_dev_user_id, X_dev_ruid):\n",
        "        if torch.cuda.is_available():\n",
        "            self.cuda()\n",
        "\n",
        "        self.eval()\n",
        "        y_pred = []\n",
        "\n",
        "        # Tensorize ID embeddings\n",
        "        X_dev_source_wid = torch.LongTensor(X_dev_source_wid)\n",
        "        X_dev_source_id = torch.LongTensor(X_dev_source_id)\n",
        "        X_dev_user_id = torch.LongTensor(X_dev_user_id)\n",
        "        X_dev_ruid = torch.LongTensor(X_dev_ruid)\n",
        "\n",
        "        # Set Dataset\n",
        "        dataset = TensorDataset(X_dev_source_wid, X_dev_source_id, X_dev_user_id, X_dev_ruid)\n",
        "        dataloader = DataLoader(dataset, batch_size=32)\n",
        "\n",
        "        for i, data in enumerate(dataloader):\n",
        "            with torch.no_grad():\n",
        "                X_source_wid, X_source_id, X_user_id, \\\n",
        "                X_ruid = (item.cuda(device=self.device) for item in data)\n",
        "\n",
        "            logits, _, _ = self.forward(X_source_wid, X_source_id, X_user_id, X_ruid)\n",
        "            predicted = torch.max(logits, dim=1)[1]\n",
        "            y_pred += predicted.data.cpu().numpy().tolist()\n",
        "        return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5BZkfyPXNkT"
      },
      "source": [
        "## 2-2. Structure-aware Multi-head Attention Network (SMAN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXND9NFcKLWB"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "from model.NeuralNetwork import NeuralNetwork\n",
        "\n",
        "class PGAN(NeuralNetwork):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(PGAN, self).__init__()\n",
        "        self.config = config\n",
        "        embedding_weights = config['embedding_weights']\n",
        "        V, D = embedding_weights.shape\n",
        "        self.n_heads = config['n_heads']\n",
        "\n",
        "        # source adjacency matrix(publisher i and news j), User adjecency matrix(user i and publisher j)\n",
        "        self.A_us = config['A_us']\n",
        "        self.A_uu = config['A_uu']\n",
        "        embeding_size = config['embeding_size']\n",
        "\n",
        "        # Word Embedding, Publisher Embedding, and Source Embedding(Combined embedding between publisher & User representation)\n",
        "        self.word_embedding = nn.Embedding(V, D, padding_idx=0, _weight=torch.from_numpy(embedding_weights))\n",
        "        self.user_embedding = nn.Embedding(config['A_us'].shape[0], embeding_size, padding_idx=0)\n",
        "        self.source_embedding = nn.Embedding(config['A_us'].shape[1], embeding_size)\n",
        "\n",
        "        # Convolution and pooling\n",
        "        self.convs = nn.ModuleList([nn.Conv1d(300, 100, kernel_size=K) for K in config['kernel_sizes']])\n",
        "        self.max_poolings = nn.ModuleList([nn.MaxPool1d(kernel_size=config['maxlen'] - K + 1) for K in config['kernel_sizes']])\n",
        "\n",
        "        # Transformation matrix used in attention inner product for publisher (4.1_equation (1))\n",
        "        self.Wcm = [nn.Parameter(torch.FloatTensor(embeding_size, embeding_size)).cuda() for _ in\n",
        "                    range(self.n_heads)]\n",
        "        \n",
        "        # Transformation matrix used in attention ineer product for user (4.2_equation(6))\n",
        "        self.Wam = [nn.Parameter(torch.FloatTensor(embeding_size, embeding_size)).cuda() for _ in\n",
        "                    range(self.n_heads)]\n",
        "\n",
        "        # Diagonal matrices applied to normalize the adjacency matrix A_us & A_uu \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([embeding_size])).cuda() #  // self.n_heads\n",
        "\n",
        "        # Linear transformation matrix for Publisher in ELU activation function (4.1_equation(3))\n",
        "        self.W1 = nn.Parameter(torch.FloatTensor(embeding_size * self.n_heads, embeding_size))\n",
        "\n",
        "        # Linear transformation matrix for User in ELU activation function (4.2)\n",
        "        self.W2 = nn.Parameter(torch.FloatTensor(embeding_size * self.n_heads, embeding_size))\n",
        "        self.linear = nn.Linear(400, 200)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(config['dropout'])\n",
        "        # Relu activation function\n",
        "        self.relu = nn.ReLU()\n",
        "        # ELU activation function\n",
        "        self.elu = nn.ELU()\n",
        "\n",
        "        # News class(Non-fake, Fake, Unverified, True) classification result\n",
        "        self.fc_out = nn.Sequential(\n",
        "            nn.Linear(300 + 2 * embeding_size, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config['dropout']),\n",
        "            nn.Linear(100, config[\"num_classes\"])\n",
        "        )\n",
        "\n",
        "        # Publisher credibility classification result\n",
        "        self.fc_user_out = nn.Sequential(\n",
        "            nn.Linear(embeding_size, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config['dropout']),\n",
        "            nn.Linear(100, 3)\n",
        "        )\n",
        "\n",
        "        # User credibility classification result\n",
        "        self.fc_ruser_out = nn.Sequential(\n",
        "            nn.Linear(embeding_size, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config['dropout']),\n",
        "            nn.Linear(100, 3)\n",
        "        )\n",
        "        print(self)\n",
        "        self.init_weights()\n",
        "\n",
        "    # Weight normalization\n",
        "    # https://eda-ai-lab.tistory.com/404\n",
        "    # Xavier normalizes weights according to the characteristic of each layers\n",
        "    def init_weights(self):\n",
        "        init.xavier_normal_(self.user_embedding.weight)\n",
        "        init.xavier_normal_(self.source_embedding.weight)\n",
        "        for i in range(self.n_heads):\n",
        "            init.xavier_normal_(self.Wcm[i])\n",
        "            init.xavier_normal_(self.Wam[i])\n",
        "\n",
        "        init.xavier_normal_(self.W1)\n",
        "        init.xavier_normal_(self.W2)\n",
        "        init.xavier_normal_(self.linear.weight)\n",
        "        for name, param in self.fc_out.named_parameters():\n",
        "            if name.__contains__(\"weight\"):\n",
        "                init.xavier_normal_(param)\n",
        "        for name, param in self.fc_user_out.named_parameters():\n",
        "            if name.__contains__(\"weight\"):\n",
        "                init.xavier_normal_(param)\n",
        "        for name, param in self.fc_ruser_out.named_parameters():\n",
        "            if name.__contains__(\"weight\"):\n",
        "                init.xavier_normal_(param)\n",
        "\n",
        "    # Multi-head attention module to get publisher's representation\n",
        "    def user_multi_head(self, X_user, X_user_id, Wcm):\n",
        "        # M = source_embedding's weight\n",
        "        M = self.source_embedding.weight\n",
        "\n",
        "        # matrix product inside the Attention(P, N, N) (4.1_equation(1))\n",
        "        # X_user = Query, Wcm = W, M = Key\n",
        "        linear1 = torch.einsum(\"bd,dd,sd->bs\", X_user, Wcm, M) / self.scale\n",
        "        \n",
        "        linear1 = self.relu(linear1) # (batch, |S|)\n",
        "\n",
        "        # Publisher Adjacency matrix (A_pn in 4.1_equation(1))\n",
        "        A_us = self.A_us[X_user_id.cpu(), :].todense()\n",
        "        A_us = torch.FloatTensor(A_us).cuda()   # (batch, |S|)\n",
        "\n",
        "        # Attention(P, N, N), and get Z_h (4.1_equation(2))\n",
        "        alpha = F.softmax(linear1 * A_us, dim=-1)\n",
        "        alpha = self.dropout(alpha)\n",
        "        return alpha.matmul(M)\n",
        "\n",
        "    # Multi-head attention module to get user's representation\n",
        "    def retweet_user_multi_head(self, X_ruser, X_ruser_id, Wam):\n",
        "        # News embeddings as well for Key (4.2_equation(6))\n",
        "        M = self.user_embedding.weight\n",
        "\n",
        "        # Linear inner product inside Attention(R, U, U)\n",
        "        linear1 = torch.einsum(\"bnd,dd,md->bnm\", X_ruser, Wam, M) / self.scale # m x bsz\n",
        "        # X_ruser = Query, Wam = W, M = Key\n",
        "        linear1 = self.relu(linear1)\n",
        "\n",
        "        s1, s2 = X_ruser_id.size()\n",
        "\n",
        "        idx = X_ruser_id.view(-1).cpu()\n",
        "\n",
        "        # A_uu: Adjecency matrix (A_uu in 4.2_equation(6))\n",
        "        A_uu = self.A_uu[idx, :].todense()\n",
        "        A_uu = torch.FloatTensor(A_uu).view(s1, s2, -1).cuda()\n",
        "        # Attention(R_j, U, U), and get Z_h\n",
        "        alpha = F.softmax(linear1 * A_uu, dim=-1) \n",
        "        alpha = self.dropout(alpha)\n",
        "        return alpha.matmul(M)\n",
        "\n",
        "    # Publisher Representation Encoder from Publisher Attention result Z_h\n",
        "    def publisher_encoder(self, X_user, X_user_id):\n",
        "        m_hat = []\n",
        "        # Get Z_h from Attention\n",
        "        for i in range(self.n_heads):\n",
        "            m_hat.append(self.user_multi_head(X_user, X_user_id, self.Wcm[i]))\n",
        "\n",
        "        # Concat Z1 to ZJ\n",
        "        m_hat = torch.cat(m_hat, dim=-1).matmul(self.W1)\n",
        "        # ELU activation function\n",
        "        m_hat = self.elu(m_hat)\n",
        "        m_hat = self.dropout(m_hat)\n",
        "        #U_hat = publisher's representation\n",
        "        U_hat = m_hat + X_user  \n",
        "        return U_hat\n",
        "\n",
        "    # User Representation Encoder from User Attention result Z_h\n",
        "    def retweet_user_encoder(self, X_ruser, X_ruser_id): \n",
        "        '''\n",
        "        :param X_ruser:  (bsz, num_users, d)\n",
        "        :param X_ruser_id: (bsz, num_users)\n",
        "        :return:\n",
        "        '''\n",
        "        m_hat = []\n",
        "        # Get Z_h from Attention\n",
        "        for i in range(self.n_heads):\n",
        "            m_hat.append(self.retweet_user_multi_head(X_ruser, X_ruser_id, self.Wam[i]))\n",
        "\n",
        "        # Concat Z_1j to Z_Hj\n",
        "        m_hat = torch.cat(m_hat, dim=-1).matmul(self.W2)\n",
        "        # ELU activation function\n",
        "        m_hat = self.elu(m_hat)\n",
        "        m_hat = self.dropout(m_hat)\n",
        "\n",
        "        # a_hat = retweet users' representation\n",
        "        a_hat = m_hat + X_ruser  # bsz x 20 x d\n",
        "        return a_hat\n",
        "\n",
        "    # Combine Publisher's Representation and User's Representatino to get Combined Representation called Source\n",
        "\n",
        "    ###################################\n",
        "    # This process is called \"FUSION\" #\n",
        "    ###################################\n",
        "\n",
        "    def source_encoder(self, X_source, r_user_rep, user_rep): \n",
        "\n",
        "        # Combine many user representation to one news into one user representation to one news, which is R'\n",
        "        # linear1 = R_k_tilda\n",
        "        linear1 = torch.einsum(\"bd,bnd->bn\", X_source, r_user_rep) # / self.scale\n",
        "        alpha = F.softmax(linear1, dim=-1)\n",
        "        # retweet_rep = R'\n",
        "        retweet_rep = torch.einsum(\"bn,bnd->bd\", alpha, r_user_rep)\n",
        "\n",
        "        # get the combined representation (Fusion)\n",
        "        source_rep = torch.cat([retweet_rep, user_rep,\n",
        "                                retweet_rep * user_rep,\n",
        "                                retweet_rep - user_rep], dim=-1)  # .mm(self.W) #\n",
        "        source_rep = self.linear(source_rep)\n",
        "        source_rep = self.dropout(source_rep)\n",
        "        return source_rep\n",
        "\n",
        "    # Convolution layer to get news representation m_j\n",
        "    def text_representation(self, X_word):\n",
        "        # Get news content representation from CNN Module\n",
        "        X_word = X_word.permute(0, 2, 1)\n",
        "        conv_block = []\n",
        "        for Conv, max_pooling in zip(self.convs, self.max_poolings):\n",
        "            act = self.relu(Conv(X_word))\n",
        "            pool = max_pooling(act).squeeze()\n",
        "            conv_block.append(pool)\n",
        "\n",
        "        features = torch.cat(conv_block, dim=1)\n",
        "        features = self.dropout(features)\n",
        "        return features\n",
        "\n",
        "    # Let's get credibility scores\n",
        "    def forward(self, X_source_wid, X_source_id, X_user_id, X_ruser_id):  # , X_composer_id, X_reviewer_id\n",
        "        '''\n",
        "        :param X_source_wid size: (batch_size, max_words)\n",
        "                X_source_id size: (batch_size, )\n",
        "                X_user_id  size: (batch_size, )\n",
        "                X_retweet_id  size: (batch_size, max_retweets)\n",
        "                X_retweet_uid  size: (batch_size, max_retweets)\n",
        "        :return:\n",
        "        '''\n",
        "        X_word = self.word_embedding(X_source_wid)\n",
        "        X_user = self.user_embedding(X_user_id)\n",
        "        X_ruser = self.user_embedding(X_ruser_id)\n",
        "        X_source = self.source_embedding(X_source_id)\n",
        "        # News content representation\n",
        "        X_text = self.text_representation(X_word)\n",
        "\n",
        "        # Publisher representation\n",
        "        user_rep = self.publisher_encoder(X_user, X_user_id)\n",
        "        # User representation\n",
        "        r_user_rep = self.retweet_user_encoder(X_ruser, X_ruser_id)  #\n",
        "        # Fusion Representation\n",
        "        source_rep = self.source_encoder(X_source, r_user_rep, user_rep)  #\n",
        "        # News Representation + Source Representation to get news classification\n",
        "        tweet_rep = torch.cat([X_text, source_rep], dim=-1)\n",
        "\n",
        "        # Final representation\n",
        "        Xt_logit = self.fc_out(tweet_rep)\n",
        "        # Publisher representation\n",
        "        Xu_logit = self.fc_user_out(user_rep)\n",
        "        # Retweet User representation\n",
        "        Xru_logit = self.fc_ruser_out(r_user_rep)\n",
        "\n",
        "        return Xt_logit, Xu_logit, Xru_logit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eaa3lWEwLwb5"
      },
      "source": [
        "# **3. Training the Model**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZjfE97EMI34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "18b50b27-31c3-4826-8a2b-748b35984e33"
      },
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "from sklearn.metrics import classification_report\n",
        "from model.Mymodel import PGAN\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "# Get Preprocessed Embedding Data\n",
        "def load_dataset(task):\n",
        "    print(\"task: \", task)\n",
        "\n",
        "    ##################################################################\n",
        "    # Data and Matrix Weights are all given AFTER ALREADY PROCESSED #\n",
        "    ##################################################################\n",
        "\n",
        "    A_us, A_uu = pickle.load(open(\"dataset/\"+task+\"/relations.pkl\", 'rb'))\n",
        "    X_train_source_wid, X_train_source_id, X_train_user_id, X_train_ruid, y_train, y_train_cred, y_train_rucred, word_embeddings = pickle.load(open(\"dataset/\"+task+\"/train.pkl\", 'rb'))\n",
        "    X_dev_source_wid, X_dev_source_id, X_dev_user_id, X_dev_ruid, y_dev = pickle.load(open(\"dataset/\"+task+\"/dev.pkl\", 'rb'))\n",
        "    X_test_source_wid, X_test_source_id, X_test_user_id, X_test_ruid, y_test = pickle.load(open(\"dataset/\"+task+\"/test.pkl\", 'rb'))\n",
        "    config['maxlen'] = len(X_train_source_wid[0])\n",
        "\n",
        "    # Set number of head for attention and any other parameters to best fit the dataset\n",
        "    if task == 'twitter15':\n",
        "        config['n_heads'] = 10\n",
        "    elif task == 'twitter16':\n",
        "        config['n_heads'] = 8\n",
        "    else:\n",
        "        config['n_heads'] = 7\n",
        "        config['batch_size'] = 128\n",
        "        config['num_classes'] = 2\n",
        "        config['target_names'] = ['NR', 'FR']\n",
        "    print(config)\n",
        "\n",
        "    config['embedding_weights'] = word_embeddings\n",
        "    config['A_us'] = A_us\n",
        "    config['A_uu'] = A_uu\n",
        "\n",
        "    # Return Embedded Datasets\n",
        "    return X_train_source_wid, X_train_source_id, X_train_user_id, X_train_ruid, y_train, y_train_cred, y_train_rucred, \\\n",
        "           X_dev_source_wid, X_dev_source_id, X_dev_user_id, X_dev_ruid, y_dev, \\\n",
        "           X_test_source_wid, X_test_source_id, X_test_user_id, X_test_ruid, y_test\n",
        "\n",
        "\n",
        "def train_and_test(model, task):\n",
        "    model_suffix = model.__name__.lower().strip(\"text\")\n",
        "    # These weights are not model weights, but weights inside the graph between nodes (publisher-news, and user-news)\n",
        "    config['save_path'] = 'checkpoint/weights.best.' + task + \".\" + model_suffix\n",
        "\n",
        "    X_train_source_wid, X_train_source_id, X_train_user_id, X_train_ruid, y_train, y_train_cred, y_train_rucred, \\\n",
        "    X_dev_source_wid, X_dev_source_id, X_dev_user_id, X_dev_ruid, y_dev, \\\n",
        "    X_test_source_wid, X_test_source_id, X_test_user_id, X_test_ruid, y_test = load_dataset(task)\n",
        "    # Model\n",
        "    nn = model(config)\n",
        "    # Train !\n",
        "    nn.fit(X_train_source_wid, X_train_source_id, X_train_user_id, X_train_ruid, y_train, y_train_cred, y_train_rucred,\n",
        "            X_dev_source_wid, X_dev_source_id, X_dev_user_id, X_dev_ruid, y_dev)  \n",
        "\n",
        "    print(\"================================\")\n",
        "    nn.load_state_dict(torch.load(config['save_path']))\n",
        "    # Predict with test data\n",
        "    y_pred = nn.predict(X_test_source_wid, X_test_source_id, X_test_user_id, X_test_ruid)\n",
        "    # Result Confusion Matrix\n",
        "    report = classification_report(y_test, y_pred, target_names=config['target_names'], digits=3, output_dict = True)\n",
        "    df = pd.DataFrame(report).transpose()\n",
        "    return(df)\n",
        "\n",
        "\n",
        "config = {\n",
        "    'lr':1e-3,\n",
        "    'reg':1e-6,\n",
        "    'embeding_size': 100,\n",
        "    'batch_size':16,\n",
        "    'nb_filters':100,\n",
        "    'kernel_sizes':[3, 4, 5],\n",
        "    'dropout':0.5,\n",
        "    'epochs':18,\n",
        "    'num_classes':4,\n",
        "    'target_names':['NR', 'FR', 'TR', 'UR']\n",
        "}\n",
        "\n",
        "'''\n",
        "if __name__ == '__main__':\n",
        "    task = 'twitter15'\n",
        "    # task = 'twitter16'\n",
        "    # task = 'weibo'\n",
        "    model = PGAN\n",
        "    train_and_test(model, task)\n",
        "    '''\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nif __name__ == '__main__':\\n    task = 'twitter15'\\n    # task = 'twitter16'\\n    # task = 'weibo'\\n    model = PGAN\\n    train_and_test(model, task)\\n    \""
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XV2EErOfV-xn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc2ebb82-d402-45c7-9200-880fb9314025"
      },
      "source": [
        "df_twt15 = train_and_test(PGAN, 'twitter15')\n",
        "df_twt16 = train_and_test(PGAN, 'twitter16')\n",
        "df_wb = train_and_test(PGAN, 'weibo')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "task:  twitter15\n",
            "{'lr': 0.001, 'reg': 1e-06, 'embeding_size': 100, 'batch_size': 16, 'nb_filters': 100, 'kernel_sizes': [3, 4, 5], 'dropout': 0.5, 'epochs': 18, 'num_classes': 4, 'target_names': ['NR', 'FR', 'TR', 'UR'], 'save_path': 'checkpoint/weights.best.twitter15.pgan', 'maxlen': 50, 'n_heads': 10}\n",
            "PGAN(\n",
            "  (word_embedding): Embedding(2246, 300, padding_idx=0)\n",
            "  (user_embedding): Embedding(2213, 100, padding_idx=0)\n",
            "  (source_embedding): Embedding(1490, 100)\n",
            "  (convs): ModuleList(\n",
            "    (0): Conv1d(300, 100, kernel_size=(3,), stride=(1,))\n",
            "    (1): Conv1d(300, 100, kernel_size=(4,), stride=(1,))\n",
            "    (2): Conv1d(300, 100, kernel_size=(5,), stride=(1,))\n",
            "  )\n",
            "  (max_poolings): ModuleList(\n",
            "    (0): MaxPool1d(kernel_size=48, stride=48, padding=0, dilation=1, ceil_mode=False)\n",
            "    (1): MaxPool1d(kernel_size=47, stride=47, padding=0, dilation=1, ceil_mode=False)\n",
            "    (2): MaxPool1d(kernel_size=46, stride=46, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (linear): Linear(in_features=400, out_features=200, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (relu): ReLU()\n",
            "  (elu): ELU(alpha=1.0)\n",
            "  (fc_out): Sequential(\n",
            "    (0): Linear(in_features=500, out_features=100, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=100, out_features=4, bias=True)\n",
            "  )\n",
            "  (fc_user_out): Sequential(\n",
            "    (0): Linear(in_features=100, out_features=100, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=100, out_features=3, bias=True)\n",
            "  )\n",
            "  (fc_ruser_out): Sequential(\n",
            "    (0): Linear(in_features=100, out_features=100, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=100, out_features=3, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "Epoch  1 / 18\n",
            "Batch[0] - loss: 3.584126  acc: 12.5000%(2/16)\n",
            "Batch[1] - loss: 3.494028  acc: 25.0000%(4/16)\n",
            "Batch[2] - loss: 3.539976  acc: 31.2500%(5/16)\n",
            "Batch[3] - loss: 3.513631  acc: 31.2500%(5/16)\n",
            "Batch[4] - loss: 3.627162  acc: 18.7500%(3/16)\n",
            "Batch[5] - loss: 3.451387  acc: 6.2500%(1/16)\n",
            "Batch[6] - loss: 3.518422  acc: 37.5000%(6/16)\n",
            "Batch[7] - loss: 3.431798  acc: 56.2500%(9/16)\n",
            "Batch[8] - loss: 3.454168  acc: 25.0000%(4/16)\n",
            "Batch[9] - loss: 3.644558  acc: 18.7500%(3/16)\n",
            "Batch[10] - loss: 3.462370  acc: 50.0000%(8/16)\n",
            "Batch[11] - loss: 3.443342  acc: 37.5000%(6/16)\n",
            "Batch[12] - loss: 3.484083  acc: 25.0000%(4/16)\n",
            "Batch[13] - loss: 3.338505  acc: 12.5000%(2/16)\n",
            "Batch[14] - loss: 3.496939  acc: 31.2500%(5/16)\n",
            "Batch[15] - loss: 3.330853  acc: 12.5000%(2/16)\n",
            "Batch[16] - loss: 3.019087  acc: 31.2500%(5/16)\n",
            "Batch[17] - loss: 3.470772  acc: 12.5000%(2/16)\n",
            "Batch[18] - loss: 3.206072  acc: 43.7500%(7/16)\n",
            "Batch[19] - loss: 3.289970  acc: 12.5000%(2/16)\n",
            "Batch[20] - loss: 3.944406  acc: 6.2500%(1/16)\n",
            "Batch[21] - loss: 3.166454  acc: 43.7500%(7/16)\n",
            "Batch[22] - loss: 3.382967  acc: 43.7500%(7/16)\n",
            "Batch[23] - loss: 3.632495  acc: 18.7500%(3/16)\n",
            "Batch[24] - loss: 3.118383  acc: 31.2500%(5/16)\n",
            "Batch[25] - loss: 3.347083  acc: 37.5000%(6/16)\n",
            "Batch[26] - loss: 3.376096  acc: 18.7500%(3/16)\n",
            "Batch[27] - loss: 3.128585  acc: 25.0000%(4/16)\n",
            "Batch[28] - loss: 3.061627  acc: 43.7500%(7/16)\n",
            "Batch[29] - loss: 3.561198  acc: 6.2500%(1/16)\n",
            "Batch[30] - loss: 3.280910  acc: 25.0000%(4/16)\n",
            "Batch[31] - loss: 3.199660  acc: 12.5000%(2/16)\n",
            "Batch[32] - loss: 3.035101  acc: 31.2500%(5/16)\n",
            "Batch[33] - loss: 3.104721  acc: 31.2500%(5/16)\n",
            "Batch[34] - loss: 3.631026  acc: 43.7500%(7/16)\n",
            "Batch[35] - loss: 3.356131  acc: 12.5000%(2/16)\n",
            "Batch[36] - loss: 3.203896  acc: 25.0000%(4/16)\n",
            "Batch[37] - loss: 3.494082  acc: 50.0000%(8/16)\n",
            "Batch[38] - loss: 3.019172  acc: 43.7500%(7/16)\n",
            "Batch[39] - loss: 2.966236  acc: 18.7500%(3/16)\n",
            "Batch[40] - loss: 3.095138  acc: 43.7500%(7/16)\n",
            "Batch[41] - loss: 3.202245  acc: 25.0000%(4/16)\n",
            "Batch[42] - loss: 3.462239  acc: 12.5000%(2/16)\n",
            "Batch[43] - loss: 3.034991  acc: 18.7500%(3/16)\n",
            "Batch[44] - loss: 3.253058  acc: 25.0000%(4/16)\n",
            "Batch[45] - loss: 2.938986  acc: 37.5000%(6/16)\n",
            "Batch[46] - loss: 3.165387  acc: 37.5000%(6/16)\n",
            "Batch[47] - loss: 3.578058  acc: 31.2500%(5/16)\n",
            "Batch[48] - loss: 3.620116  acc: 18.7500%(3/16)\n",
            "Batch[49] - loss: 2.972311  acc: 25.0000%(4/16)\n",
            "Batch[50] - loss: 3.174874  acc: 43.7500%(7/16)\n",
            "Batch[51] - loss: 3.486197  acc: 31.2500%(5/16)\n",
            "Batch[52] - loss: 2.871922  acc: 31.2500%(5/16)\n",
            "Batch[53] - loss: 2.780683  acc: 31.2500%(5/16)\n",
            "Batch[54] - loss: 3.400858  acc: 25.0000%(4/16)\n",
            "Batch[55] - loss: 3.167866  acc: 43.7500%(7/16)\n",
            "Batch[56] - loss: 2.639089  acc: 50.0000%(8/16)\n",
            "Batch[57] - loss: 3.938355  acc: 37.5000%(6/16)\n",
            "Batch[58] - loss: 3.866264  acc: 6.2500%(1/16)\n",
            "Batch[59] - loss: 3.157432  acc: 43.7500%(7/16)\n",
            "Batch[60] - loss: 2.940531  acc: 50.0000%(8/16)\n",
            "Batch[61] - loss: 2.925001  acc: 37.5000%(6/16)\n",
            "Batch[62] - loss: 3.454530  acc: 53.8462%(7/13)\n",
            "Average loss:3.316470 average acc:29.525337%\n",
            "Val set acc: 0.44966442953020136\n",
            "Best val set acc: 0\n",
            "\n",
            "Epoch  2 / 18\n",
            "Batch[0] - loss: 3.291983  acc: 31.2500%(5/16)\n",
            "Batch[1] - loss: 3.230644  acc: 31.2500%(5/16)\n",
            "Batch[2] - loss: 3.058206  acc: 31.2500%(5/16)\n",
            "Batch[3] - loss: 2.985987  acc: 62.5000%(10/16)\n",
            "Batch[4] - loss: 2.970093  acc: 31.2500%(5/16)\n",
            "Batch[5] - loss: 2.722404  acc: 43.7500%(7/16)\n",
            "Batch[6] - loss: 3.099449  acc: 50.0000%(8/16)\n",
            "Batch[7] - loss: 3.316567  acc: 50.0000%(8/16)\n",
            "Batch[8] - loss: 3.096541  acc: 62.5000%(10/16)\n",
            "Batch[9] - loss: 3.085675  acc: 50.0000%(8/16)\n",
            "Batch[10] - loss: 2.643049  acc: 81.2500%(13/16)\n",
            "Batch[11] - loss: 3.078289  acc: 43.7500%(7/16)\n",
            "Batch[12] - loss: 2.968486  acc: 31.2500%(5/16)\n",
            "Batch[13] - loss: 3.238966  acc: 31.2500%(5/16)\n",
            "Batch[14] - loss: 2.855710  acc: 56.2500%(9/16)\n",
            "Batch[15] - loss: 2.700129  acc: 68.7500%(11/16)\n",
            "Batch[16] - loss: 2.656313  acc: 37.5000%(6/16)\n",
            "Batch[17] - loss: 3.283510  acc: 25.0000%(4/16)\n",
            "Batch[18] - loss: 2.746271  acc: 62.5000%(10/16)\n",
            "Batch[19] - loss: 2.918236  acc: 31.2500%(5/16)\n",
            "Batch[20] - loss: 3.030427  acc: 56.2500%(9/16)\n",
            "Batch[21] - loss: 3.096198  acc: 31.2500%(5/16)\n",
            "Batch[22] - loss: 2.647607  acc: 50.0000%(8/16)\n",
            "Batch[23] - loss: 3.124336  acc: 43.7500%(7/16)\n",
            "Batch[24] - loss: 2.743020  acc: 50.0000%(8/16)\n",
            "Batch[25] - loss: 2.852242  acc: 56.2500%(9/16)\n",
            "Batch[26] - loss: 2.909992  acc: 87.5000%(14/16)\n",
            "Batch[27] - loss: 2.885492  acc: 56.2500%(9/16)\n",
            "Batch[28] - loss: 2.400589  acc: 43.7500%(7/16)\n",
            "Batch[29] - loss: 2.569001  acc: 68.7500%(11/16)\n",
            "Batch[30] - loss: 2.644129  acc: 56.2500%(9/16)\n",
            "Batch[31] - loss: 2.574082  acc: 75.0000%(12/16)\n",
            "Batch[32] - loss: 3.317685  acc: 68.7500%(11/16)\n",
            "Batch[33] - loss: 2.573436  acc: 37.5000%(6/16)\n",
            "Batch[34] - loss: 2.724108  acc: 56.2500%(9/16)\n",
            "Batch[35] - loss: 2.564156  acc: 43.7500%(7/16)\n",
            "Batch[36] - loss: 2.674147  acc: 62.5000%(10/16)\n",
            "Batch[37] - loss: 2.950679  acc: 62.5000%(10/16)\n",
            "Batch[38] - loss: 2.574096  acc: 37.5000%(6/16)\n",
            "Batch[39] - loss: 2.805929  acc: 50.0000%(8/16)\n",
            "Batch[40] - loss: 2.294252  acc: 68.7500%(11/16)\n",
            "Batch[41] - loss: 2.804852  acc: 50.0000%(8/16)\n",
            "Batch[42] - loss: 2.278719  acc: 68.7500%(11/16)\n",
            "Batch[43] - loss: 2.463160  acc: 50.0000%(8/16)\n",
            "Batch[44] - loss: 2.914775  acc: 43.7500%(7/16)\n",
            "Batch[45] - loss: 2.577335  acc: 50.0000%(8/16)\n",
            "Batch[46] - loss: 2.790592  acc: 37.5000%(6/16)\n",
            "Batch[47] - loss: 2.905027  acc: 68.7500%(11/16)\n",
            "Batch[48] - loss: 2.690139  acc: 56.2500%(9/16)\n",
            "Batch[49] - loss: 2.437848  acc: 62.5000%(10/16)\n",
            "Batch[50] - loss: 2.327173  acc: 50.0000%(8/16)\n",
            "Batch[51] - loss: 2.467785  acc: 75.0000%(12/16)\n",
            "Batch[52] - loss: 2.532668  acc: 68.7500%(11/16)\n",
            "Batch[53] - loss: 2.199646  acc: 81.2500%(13/16)\n",
            "Batch[54] - loss: 2.553669  acc: 62.5000%(10/16)\n",
            "Batch[55] - loss: 2.403472  acc: 62.5000%(10/16)\n",
            "Batch[56] - loss: 1.943660  acc: 75.0000%(12/16)\n",
            "Batch[57] - loss: 2.201359  acc: 68.7500%(11/16)\n",
            "Batch[58] - loss: 2.194477  acc: 56.2500%(9/16)\n",
            "Batch[59] - loss: 2.073314  acc: 56.2500%(9/16)\n",
            "Batch[60] - loss: 2.567249  acc: 56.2500%(9/16)\n",
            "Batch[61] - loss: 3.021064  acc: 56.2500%(9/16)\n",
            "Batch[62] - loss: 2.132454  acc: 84.6154%(11/13)\n",
            "Average loss:2.736231 average acc:54.220089%\n",
            "Val set acc: 0.6241610738255033\n",
            "Best val set acc: 0\n",
            "\n",
            "Epoch  3 / 18\n",
            "Batch[0] - loss: 1.974442  acc: 75.0000%(12/16)\n",
            "Batch[1] - loss: 2.094702  acc: 75.0000%(12/16)\n",
            "Batch[2] - loss: 1.906907  acc: 81.2500%(13/16)\n",
            "Batch[3] - loss: 2.082341  acc: 68.7500%(11/16)\n",
            "Batch[4] - loss: 2.028505  acc: 87.5000%(14/16)\n",
            "Batch[5] - loss: 2.050143  acc: 68.7500%(11/16)\n",
            "Batch[6] - loss: 2.058431  acc: 62.5000%(10/16)\n",
            "Batch[7] - loss: 1.907456  acc: 87.5000%(14/16)\n",
            "Batch[8] - loss: 2.441222  acc: 75.0000%(12/16)\n",
            "Batch[9] - loss: 1.997304  acc: 75.0000%(12/16)\n",
            "Batch[10] - loss: 1.488900  acc: 87.5000%(14/16)\n",
            "Batch[11] - loss: 2.058791  acc: 75.0000%(12/16)\n",
            "Batch[12] - loss: 2.199396  acc: 75.0000%(12/16)\n",
            "Batch[13] - loss: 1.820911  acc: 68.7500%(11/16)\n",
            "Batch[14] - loss: 1.884311  acc: 75.0000%(12/16)\n",
            "Batch[15] - loss: 1.864190  acc: 87.5000%(14/16)\n",
            "Batch[16] - loss: 1.994040  acc: 56.2500%(9/16)\n",
            "Batch[17] - loss: 2.264593  acc: 81.2500%(13/16)\n",
            "Batch[18] - loss: 2.043252  acc: 81.2500%(13/16)\n",
            "Batch[19] - loss: 1.657686  acc: 81.2500%(13/16)\n",
            "Batch[20] - loss: 1.592760  acc: 81.2500%(13/16)\n",
            "Batch[21] - loss: 1.373098  acc: 93.7500%(15/16)\n",
            "Batch[22] - loss: 1.822998  acc: 68.7500%(11/16)\n",
            "Batch[23] - loss: 1.491855  acc: 75.0000%(12/16)\n",
            "Batch[24] - loss: 1.713127  acc: 87.5000%(14/16)\n",
            "Batch[25] - loss: 2.045089  acc: 81.2500%(13/16)\n",
            "Batch[26] - loss: 1.399912  acc: 75.0000%(12/16)\n",
            "Batch[27] - loss: 1.492710  acc: 81.2500%(13/16)\n",
            "Batch[28] - loss: 2.078627  acc: 68.7500%(11/16)\n",
            "Batch[29] - loss: 1.570508  acc: 87.5000%(14/16)\n",
            "Batch[30] - loss: 2.197963  acc: 62.5000%(10/16)\n",
            "Batch[31] - loss: 1.708435  acc: 75.0000%(12/16)\n",
            "Batch[32] - loss: 1.580616  acc: 56.2500%(9/16)\n",
            "Batch[33] - loss: 1.735739  acc: 75.0000%(12/16)\n",
            "Batch[34] - loss: 1.458549  acc: 56.2500%(9/16)\n",
            "Batch[35] - loss: 1.443704  acc: 75.0000%(12/16)\n",
            "Batch[36] - loss: 1.642932  acc: 62.5000%(10/16)\n",
            "Batch[37] - loss: 2.132936  acc: 68.7500%(11/16)\n",
            "Batch[38] - loss: 1.278449  acc: 93.7500%(15/16)\n",
            "Batch[39] - loss: 1.798344  acc: 87.5000%(14/16)\n",
            "Batch[40] - loss: 1.403895  acc: 87.5000%(14/16)\n",
            "Batch[41] - loss: 1.180913  acc: 93.7500%(15/16)\n",
            "Batch[42] - loss: 1.625044  acc: 75.0000%(12/16)\n",
            "Batch[43] - loss: 1.578087  acc: 68.7500%(11/16)\n",
            "Batch[44] - loss: 1.978626  acc: 75.0000%(12/16)\n",
            "Batch[45] - loss: 1.435998  acc: 75.0000%(12/16)\n",
            "Batch[46] - loss: 1.256658  acc: 75.0000%(12/16)\n",
            "Batch[47] - loss: 1.414034  acc: 93.7500%(15/16)\n",
            "Batch[48] - loss: 1.302591  acc: 93.7500%(15/16)\n",
            "Batch[49] - loss: 1.693690  acc: 81.2500%(13/16)\n",
            "Batch[50] - loss: 1.130446  acc: 87.5000%(14/16)\n",
            "Batch[51] - loss: 1.350336  acc: 87.5000%(14/16)\n",
            "Batch[52] - loss: 1.464810  acc: 87.5000%(14/16)\n",
            "Batch[53] - loss: 1.370116  acc: 93.7500%(15/16)\n",
            "Batch[54] - loss: 1.621854  acc: 68.7500%(11/16)\n",
            "Batch[55] - loss: 1.408626  acc: 87.5000%(14/16)\n",
            "Batch[56] - loss: 1.625422  acc: 68.7500%(11/16)\n",
            "Batch[57] - loss: 1.066076  acc: 93.7500%(15/16)\n",
            "Batch[58] - loss: 1.378619  acc: 87.5000%(14/16)\n",
            "Batch[59] - loss: 1.310081  acc: 68.7500%(11/16)\n",
            "Batch[60] - loss: 1.265174  acc: 87.5000%(14/16)\n",
            "Batch[61] - loss: 1.143003  acc: 87.5000%(14/16)\n",
            "Batch[62] - loss: 1.716441  acc: 92.3077%(12/13)\n",
            "Average loss:1.684070 average acc:78.647743%\n",
            "Val set acc: 0.7449664429530202\n",
            "Best val set acc: 0\n",
            "\n",
            "Epoch  4 / 18\n",
            "Batch[0] - loss: 1.220834  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 1.058864  acc: 87.5000%(14/16)\n",
            "Batch[2] - loss: 1.064473  acc: 87.5000%(14/16)\n",
            "Batch[3] - loss: 0.841572  acc: 87.5000%(14/16)\n",
            "Batch[4] - loss: 1.013007  acc: 93.7500%(15/16)\n",
            "Batch[5] - loss: 0.906862  acc: 93.7500%(15/16)\n",
            "Batch[6] - loss: 0.748110  acc: 93.7500%(15/16)\n",
            "Batch[7] - loss: 0.697700  acc: 100.0000%(16/16)\n",
            "Batch[8] - loss: 0.762496  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.922626  acc: 93.7500%(15/16)\n",
            "Batch[10] - loss: 0.641023  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.797424  acc: 100.0000%(16/16)\n",
            "Batch[12] - loss: 0.838442  acc: 93.7500%(15/16)\n",
            "Batch[13] - loss: 1.034914  acc: 81.2500%(13/16)\n",
            "Batch[14] - loss: 0.807160  acc: 93.7500%(15/16)\n",
            "Batch[15] - loss: 0.729025  acc: 87.5000%(14/16)\n",
            "Batch[16] - loss: 1.318673  acc: 81.2500%(13/16)\n",
            "Batch[17] - loss: 0.775948  acc: 87.5000%(14/16)\n",
            "Batch[18] - loss: 1.033916  acc: 75.0000%(12/16)\n",
            "Batch[19] - loss: 0.577583  acc: 93.7500%(15/16)\n",
            "Batch[20] - loss: 0.906676  acc: 87.5000%(14/16)\n",
            "Batch[21] - loss: 0.855699  acc: 87.5000%(14/16)\n",
            "Batch[22] - loss: 0.983522  acc: 81.2500%(13/16)\n",
            "Batch[23] - loss: 0.713466  acc: 93.7500%(15/16)\n",
            "Batch[24] - loss: 0.352506  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.758678  acc: 93.7500%(15/16)\n",
            "Batch[26] - loss: 0.639725  acc: 93.7500%(15/16)\n",
            "Batch[27] - loss: 0.882601  acc: 87.5000%(14/16)\n",
            "Batch[28] - loss: 0.953938  acc: 93.7500%(15/16)\n",
            "Batch[29] - loss: 0.680636  acc: 93.7500%(15/16)\n",
            "Batch[30] - loss: 0.651748  acc: 75.0000%(12/16)\n",
            "Batch[31] - loss: 0.841442  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 1.077721  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 1.280641  acc: 87.5000%(14/16)\n",
            "Batch[34] - loss: 0.783863  acc: 87.5000%(14/16)\n",
            "Batch[35] - loss: 0.563812  acc: 87.5000%(14/16)\n",
            "Batch[36] - loss: 0.538116  acc: 100.0000%(16/16)\n",
            "Batch[37] - loss: 0.809534  acc: 87.5000%(14/16)\n",
            "Batch[38] - loss: 0.529528  acc: 100.0000%(16/16)\n",
            "Batch[39] - loss: 1.056167  acc: 81.2500%(13/16)\n",
            "Batch[40] - loss: 0.652877  acc: 87.5000%(14/16)\n",
            "Batch[41] - loss: 0.710521  acc: 100.0000%(16/16)\n",
            "Batch[42] - loss: 0.702642  acc: 93.7500%(15/16)\n",
            "Batch[43] - loss: 0.620359  acc: 93.7500%(15/16)\n",
            "Batch[44] - loss: 0.499671  acc: 100.0000%(16/16)\n",
            "Batch[45] - loss: 0.595971  acc: 100.0000%(16/16)\n",
            "Batch[46] - loss: 0.732840  acc: 81.2500%(13/16)\n",
            "Batch[47] - loss: 0.545031  acc: 93.7500%(15/16)\n",
            "Batch[48] - loss: 0.935988  acc: 87.5000%(14/16)\n",
            "Batch[49] - loss: 0.465827  acc: 100.0000%(16/16)\n",
            "Batch[50] - loss: 0.541987  acc: 100.0000%(16/16)\n",
            "Batch[51] - loss: 1.015466  acc: 93.7500%(15/16)\n",
            "Batch[52] - loss: 0.532871  acc: 87.5000%(14/16)\n",
            "Batch[53] - loss: 0.559146  acc: 100.0000%(16/16)\n",
            "Batch[54] - loss: 0.649798  acc: 87.5000%(14/16)\n",
            "Batch[55] - loss: 0.728373  acc: 87.5000%(14/16)\n",
            "Batch[56] - loss: 0.777798  acc: 93.7500%(15/16)\n",
            "Batch[57] - loss: 0.620375  acc: 87.5000%(14/16)\n",
            "Batch[58] - loss: 0.518831  acc: 93.7500%(15/16)\n",
            "Batch[59] - loss: 0.439771  acc: 100.0000%(16/16)\n",
            "Batch[60] - loss: 0.755805  acc: 93.7500%(15/16)\n",
            "Batch[61] - loss: 0.763582  acc: 93.7500%(15/16)\n",
            "Batch[62] - loss: 0.593787  acc: 84.6154%(11/13)\n",
            "Average loss:0.771587 average acc:91.918503%\n",
            "Val set acc: 0.7449664429530202\n",
            "Best val set acc: 0\n",
            "\n",
            "Epoch  5 / 18\n",
            "Batch[0] - loss: 0.294353  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 0.456554  acc: 100.0000%(16/16)\n",
            "Batch[2] - loss: 0.392437  acc: 93.7500%(15/16)\n",
            "Batch[3] - loss: 0.360879  acc: 93.7500%(15/16)\n",
            "Batch[4] - loss: 0.632962  acc: 93.7500%(15/16)\n",
            "Batch[5] - loss: 0.346426  acc: 100.0000%(16/16)\n",
            "Batch[6] - loss: 0.374381  acc: 100.0000%(16/16)\n",
            "Batch[7] - loss: 0.396290  acc: 100.0000%(16/16)\n",
            "Batch[8] - loss: 0.350718  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.691008  acc: 93.7500%(15/16)\n",
            "Batch[10] - loss: 0.271797  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.389116  acc: 100.0000%(16/16)\n",
            "Batch[12] - loss: 0.414116  acc: 93.7500%(15/16)\n",
            "Batch[13] - loss: 0.436638  acc: 87.5000%(14/16)\n",
            "Batch[14] - loss: 0.264508  acc: 100.0000%(16/16)\n",
            "Batch[15] - loss: 0.348153  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.575300  acc: 87.5000%(14/16)\n",
            "Batch[17] - loss: 0.294768  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.264384  acc: 93.7500%(15/16)\n",
            "Batch[19] - loss: 0.300398  acc: 100.0000%(16/16)\n",
            "Batch[20] - loss: 0.533418  acc: 87.5000%(14/16)\n",
            "Batch[21] - loss: 0.391535  acc: 100.0000%(16/16)\n",
            "Batch[22] - loss: 0.364028  acc: 87.5000%(14/16)\n",
            "Batch[23] - loss: 0.170804  acc: 100.0000%(16/16)\n",
            "Batch[24] - loss: 0.195622  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.426529  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.458405  acc: 93.7500%(15/16)\n",
            "Batch[27] - loss: 0.396745  acc: 100.0000%(16/16)\n",
            "Batch[28] - loss: 0.518091  acc: 93.7500%(15/16)\n",
            "Batch[29] - loss: 0.365403  acc: 100.0000%(16/16)\n",
            "Batch[30] - loss: 0.169018  acc: 100.0000%(16/16)\n",
            "Batch[31] - loss: 0.130543  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.247742  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 0.367513  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.429821  acc: 93.7500%(15/16)\n",
            "Batch[35] - loss: 0.224978  acc: 93.7500%(15/16)\n",
            "Batch[36] - loss: 0.413030  acc: 93.7500%(15/16)\n",
            "Batch[37] - loss: 0.219277  acc: 100.0000%(16/16)\n",
            "Batch[38] - loss: 0.288653  acc: 93.7500%(15/16)\n",
            "Batch[39] - loss: 0.213612  acc: 100.0000%(16/16)\n",
            "Batch[40] - loss: 0.311363  acc: 100.0000%(16/16)\n",
            "Batch[41] - loss: 0.639329  acc: 93.7500%(15/16)\n",
            "Batch[42] - loss: 0.271533  acc: 100.0000%(16/16)\n",
            "Batch[43] - loss: 0.229692  acc: 100.0000%(16/16)\n",
            "Batch[44] - loss: 0.303182  acc: 100.0000%(16/16)\n",
            "Batch[45] - loss: 0.270689  acc: 100.0000%(16/16)\n",
            "Batch[46] - loss: 0.251040  acc: 100.0000%(16/16)\n",
            "Batch[47] - loss: 0.304464  acc: 93.7500%(15/16)\n",
            "Batch[48] - loss: 0.264825  acc: 93.7500%(15/16)\n",
            "Batch[49] - loss: 0.297640  acc: 100.0000%(16/16)\n",
            "Batch[50] - loss: 0.497159  acc: 93.7500%(15/16)\n",
            "Batch[51] - loss: 0.110747  acc: 100.0000%(16/16)\n",
            "Batch[52] - loss: 0.225355  acc: 100.0000%(16/16)\n",
            "Batch[53] - loss: 0.386892  acc: 100.0000%(16/16)\n",
            "Batch[54] - loss: 0.284335  acc: 100.0000%(16/16)\n",
            "Batch[55] - loss: 0.390825  acc: 100.0000%(16/16)\n",
            "Batch[56] - loss: 0.272960  acc: 93.7500%(15/16)\n",
            "Batch[57] - loss: 0.425361  acc: 93.7500%(15/16)\n",
            "Batch[58] - loss: 0.199594  acc: 93.7500%(15/16)\n",
            "Batch[59] - loss: 0.569542  acc: 93.7500%(15/16)\n",
            "Batch[60] - loss: 0.274534  acc: 93.7500%(15/16)\n",
            "Batch[61] - loss: 0.494390  acc: 93.7500%(15/16)\n",
            "Batch[62] - loss: 0.126122  acc: 100.0000%(13/13)\n",
            "Average loss:0.345739 average acc:97.023811%\n",
            "Val set acc: 0.7583892617449665\n",
            "Best val set acc: 0\n",
            "\n",
            "Epoch  6 / 18\n",
            "Batch[0] - loss: 0.123080  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 0.265513  acc: 93.7500%(15/16)\n",
            "Batch[2] - loss: 0.261629  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.303771  acc: 100.0000%(16/16)\n",
            "Batch[4] - loss: 0.401026  acc: 93.7500%(15/16)\n",
            "Batch[5] - loss: 0.052527  acc: 100.0000%(16/16)\n",
            "Batch[6] - loss: 0.288129  acc: 100.0000%(16/16)\n",
            "Batch[7] - loss: 0.230393  acc: 93.7500%(15/16)\n",
            "Batch[8] - loss: 0.071025  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.220015  acc: 100.0000%(16/16)\n",
            "Batch[10] - loss: 0.157406  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.217026  acc: 93.7500%(15/16)\n",
            "Batch[12] - loss: 0.160626  acc: 100.0000%(16/16)\n",
            "Batch[13] - loss: 0.079925  acc: 100.0000%(16/16)\n",
            "Batch[14] - loss: 0.259845  acc: 93.7500%(15/16)\n",
            "Batch[15] - loss: 0.149384  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.128559  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 0.274336  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.214116  acc: 100.0000%(16/16)\n",
            "Batch[19] - loss: 0.118126  acc: 100.0000%(16/16)\n",
            "Batch[20] - loss: 0.077875  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 0.264095  acc: 93.7500%(15/16)\n",
            "Batch[22] - loss: 0.231042  acc: 100.0000%(16/16)\n",
            "Batch[23] - loss: 0.141764  acc: 100.0000%(16/16)\n",
            "Batch[24] - loss: 0.178519  acc: 93.7500%(15/16)\n",
            "Batch[25] - loss: 0.179080  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.168927  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 0.250358  acc: 100.0000%(16/16)\n",
            "Batch[28] - loss: 0.117689  acc: 100.0000%(16/16)\n",
            "Batch[29] - loss: 0.052844  acc: 100.0000%(16/16)\n",
            "Batch[30] - loss: 0.113026  acc: 100.0000%(16/16)\n",
            "Batch[31] - loss: 0.114127  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.152443  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 0.136253  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.082508  acc: 100.0000%(16/16)\n",
            "Batch[35] - loss: 0.181723  acc: 100.0000%(16/16)\n",
            "Batch[36] - loss: 0.192211  acc: 100.0000%(16/16)\n",
            "Batch[37] - loss: 0.124897  acc: 100.0000%(16/16)\n",
            "Batch[38] - loss: 0.127065  acc: 100.0000%(16/16)\n",
            "Batch[39] - loss: 0.269944  acc: 93.7500%(15/16)\n",
            "Batch[40] - loss: 0.082907  acc: 100.0000%(16/16)\n",
            "Batch[41] - loss: 0.217410  acc: 100.0000%(16/16)\n",
            "Batch[42] - loss: 0.096627  acc: 100.0000%(16/16)\n",
            "Batch[43] - loss: 0.136744  acc: 100.0000%(16/16)\n",
            "Batch[44] - loss: 0.073296  acc: 100.0000%(16/16)\n",
            "Batch[45] - loss: 0.063343  acc: 100.0000%(16/16)\n",
            "Batch[46] - loss: 0.133807  acc: 100.0000%(16/16)\n",
            "Batch[47] - loss: 0.232833  acc: 93.7500%(15/16)\n",
            "Batch[48] - loss: 0.091533  acc: 100.0000%(16/16)\n",
            "Batch[49] - loss: 0.114303  acc: 100.0000%(16/16)\n",
            "Batch[50] - loss: 0.186233  acc: 100.0000%(16/16)\n",
            "Batch[51] - loss: 0.084212  acc: 100.0000%(16/16)\n",
            "Batch[52] - loss: 0.102773  acc: 100.0000%(16/16)\n",
            "Batch[53] - loss: 0.122826  acc: 100.0000%(16/16)\n",
            "Batch[54] - loss: 0.257806  acc: 93.7500%(15/16)\n",
            "Batch[55] - loss: 0.136642  acc: 100.0000%(16/16)\n",
            "Batch[56] - loss: 0.116859  acc: 100.0000%(16/16)\n",
            "Batch[57] - loss: 0.051275  acc: 100.0000%(16/16)\n",
            "Batch[58] - loss: 0.133604  acc: 100.0000%(16/16)\n",
            "Batch[59] - loss: 0.194259  acc: 93.7500%(15/16)\n",
            "Batch[60] - loss: 0.273522  acc: 93.7500%(15/16)\n",
            "Batch[61] - loss: 0.302952  acc: 87.5000%(14/16)\n",
            "Batch[62] - loss: 0.065996  acc: 100.0000%(13/13)\n",
            "Average loss:0.165152 average acc:98.611115%\n",
            "Val set acc: 0.7651006711409396\n",
            "Best val set acc: 0\n",
            "\n",
            "Epoch  7 / 18\n",
            "Batch[0] - loss: 0.068644  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 0.163800  acc: 93.7500%(15/16)\n",
            "Batch[2] - loss: 0.157576  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.050546  acc: 100.0000%(16/16)\n",
            "Batch[4] - loss: 0.109746  acc: 100.0000%(16/16)\n",
            "Batch[5] - loss: 0.282606  acc: 93.7500%(15/16)\n",
            "Batch[6] - loss: 0.082310  acc: 100.0000%(16/16)\n",
            "Batch[7] - loss: 0.070786  acc: 100.0000%(16/16)\n",
            "Batch[8] - loss: 0.085215  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.089035  acc: 100.0000%(16/16)\n",
            "Batch[10] - loss: 0.127076  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.111200  acc: 100.0000%(16/16)\n",
            "Batch[12] - loss: 0.029792  acc: 100.0000%(16/16)\n",
            "Batch[13] - loss: 0.110531  acc: 100.0000%(16/16)\n",
            "Batch[14] - loss: 0.044221  acc: 100.0000%(16/16)\n",
            "Batch[15] - loss: 0.087855  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.228769  acc: 93.7500%(15/16)\n",
            "Batch[17] - loss: 0.056832  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.067085  acc: 100.0000%(16/16)\n",
            "Batch[19] - loss: 0.040466  acc: 100.0000%(16/16)\n",
            "Batch[20] - loss: 0.097033  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 0.058974  acc: 100.0000%(16/16)\n",
            "Batch[22] - loss: 0.102312  acc: 100.0000%(16/16)\n",
            "Batch[23] - loss: 0.093422  acc: 100.0000%(16/16)\n",
            "Batch[24] - loss: 0.106923  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.023651  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.054707  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 0.102095  acc: 100.0000%(16/16)\n",
            "Batch[28] - loss: 0.165594  acc: 93.7500%(15/16)\n",
            "Batch[29] - loss: 0.091950  acc: 100.0000%(16/16)\n",
            "Batch[30] - loss: 0.166253  acc: 93.7500%(15/16)\n",
            "Batch[31] - loss: 0.103943  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.144807  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 0.060691  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.124423  acc: 100.0000%(16/16)\n",
            "Batch[35] - loss: 0.083955  acc: 100.0000%(16/16)\n",
            "Batch[36] - loss: 0.033672  acc: 100.0000%(16/16)\n",
            "Batch[37] - loss: 0.097606  acc: 100.0000%(16/16)\n",
            "Batch[38] - loss: 0.057480  acc: 100.0000%(16/16)\n",
            "Batch[39] - loss: 0.051355  acc: 100.0000%(16/16)\n",
            "Batch[40] - loss: 0.063740  acc: 100.0000%(16/16)\n",
            "Batch[41] - loss: 0.172797  acc: 93.7500%(15/16)\n",
            "Batch[42] - loss: 0.137847  acc: 100.0000%(16/16)\n",
            "Batch[43] - loss: 0.103158  acc: 100.0000%(16/16)\n",
            "Batch[44] - loss: 0.107991  acc: 93.7500%(15/16)\n",
            "Batch[45] - loss: 0.069096  acc: 100.0000%(16/16)\n",
            "Batch[46] - loss: 0.042092  acc: 100.0000%(16/16)\n",
            "Batch[47] - loss: 0.027778  acc: 100.0000%(16/16)\n",
            "Batch[48] - loss: 0.128019  acc: 100.0000%(16/16)\n",
            "Batch[49] - loss: 0.068651  acc: 100.0000%(16/16)\n",
            "Batch[50] - loss: 0.021690  acc: 100.0000%(16/16)\n",
            "Batch[51] - loss: 0.124039  acc: 93.7500%(15/16)\n",
            "Batch[52] - loss: 0.072089  acc: 100.0000%(16/16)\n",
            "Batch[53] - loss: 0.054684  acc: 100.0000%(16/16)\n",
            "Batch[54] - loss: 0.079887  acc: 100.0000%(16/16)\n",
            "Batch[55] - loss: 0.027205  acc: 100.0000%(16/16)\n",
            "Batch[56] - loss: 0.071080  acc: 100.0000%(16/16)\n",
            "Batch[57] - loss: 0.038550  acc: 100.0000%(16/16)\n",
            "Batch[58] - loss: 0.077005  acc: 100.0000%(16/16)\n",
            "Batch[59] - loss: 0.112260  acc: 100.0000%(16/16)\n",
            "Batch[60] - loss: 0.059154  acc: 100.0000%(16/16)\n",
            "Batch[61] - loss: 0.051949  acc: 100.0000%(16/16)\n",
            "Batch[62] - loss: 0.039820  acc: 100.0000%(13/13)\n",
            "Average loss:0.089453 average acc:99.206352%\n",
            "Val set acc: 0.785234899328859\n",
            "Best val set acc: 0\n",
            "\n",
            "Epoch  8 / 18\n",
            "Batch[0] - loss: 0.076894  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 0.023270  acc: 100.0000%(16/16)\n",
            "Batch[2] - loss: 0.079275  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.065215  acc: 100.0000%(16/16)\n",
            "Batch[4] - loss: 0.056587  acc: 100.0000%(16/16)\n",
            "Batch[5] - loss: 0.055080  acc: 100.0000%(16/16)\n",
            "Batch[6] - loss: 0.342865  acc: 93.7500%(15/16)\n",
            "Batch[7] - loss: 0.049733  acc: 100.0000%(16/16)\n",
            "Batch[8] - loss: 0.040292  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.042979  acc: 100.0000%(16/16)\n",
            "Batch[10] - loss: 0.052368  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.104172  acc: 100.0000%(16/16)\n",
            "Batch[12] - loss: 0.045414  acc: 100.0000%(16/16)\n",
            "Batch[13] - loss: 0.049448  acc: 100.0000%(16/16)\n",
            "Batch[14] - loss: 0.081051  acc: 93.7500%(15/16)\n",
            "Batch[15] - loss: 0.043155  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.023438  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 0.112488  acc: 93.7500%(15/16)\n",
            "Batch[18] - loss: 0.082322  acc: 100.0000%(16/16)\n",
            "Batch[19] - loss: 0.056464  acc: 100.0000%(16/16)\n",
            "Batch[20] - loss: 0.016260  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 0.050408  acc: 100.0000%(16/16)\n",
            "Batch[22] - loss: 0.033280  acc: 100.0000%(16/16)\n",
            "Batch[23] - loss: 0.053253  acc: 100.0000%(16/16)\n",
            "Batch[24] - loss: 0.050602  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.036537  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.028667  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 0.106570  acc: 100.0000%(16/16)\n",
            "Batch[28] - loss: 0.026551  acc: 100.0000%(16/16)\n",
            "Batch[29] - loss: 0.050908  acc: 100.0000%(16/16)\n",
            "Batch[30] - loss: 0.039179  acc: 100.0000%(16/16)\n",
            "Batch[31] - loss: 0.046458  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.090442  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 0.047731  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.065250  acc: 100.0000%(16/16)\n",
            "Batch[35] - loss: 0.055119  acc: 100.0000%(16/16)\n",
            "Batch[36] - loss: 0.042692  acc: 100.0000%(16/16)\n",
            "Batch[37] - loss: 0.027949  acc: 100.0000%(16/16)\n",
            "Batch[38] - loss: 0.077415  acc: 100.0000%(16/16)\n",
            "Batch[39] - loss: 0.024524  acc: 100.0000%(16/16)\n",
            "Batch[40] - loss: 0.060611  acc: 100.0000%(16/16)\n",
            "Batch[41] - loss: 0.031033  acc: 100.0000%(16/16)\n",
            "Batch[42] - loss: 0.138177  acc: 93.7500%(15/16)\n",
            "Batch[43] - loss: 0.034605  acc: 100.0000%(16/16)\n",
            "Batch[44] - loss: 0.044842  acc: 100.0000%(16/16)\n",
            "Batch[45] - loss: 0.080514  acc: 100.0000%(16/16)\n",
            "Batch[46] - loss: 0.056282  acc: 100.0000%(16/16)\n",
            "Batch[47] - loss: 0.028747  acc: 100.0000%(16/16)\n",
            "Batch[48] - loss: 0.012079  acc: 100.0000%(16/16)\n",
            "Batch[49] - loss: 0.041295  acc: 100.0000%(16/16)\n",
            "Batch[50] - loss: 0.046575  acc: 100.0000%(16/16)\n",
            "Batch[51] - loss: 0.046613  acc: 100.0000%(16/16)\n",
            "Batch[52] - loss: 0.020661  acc: 100.0000%(16/16)\n",
            "Batch[53] - loss: 0.026185  acc: 100.0000%(16/16)\n",
            "Batch[54] - loss: 0.025773  acc: 100.0000%(16/16)\n",
            "Batch[55] - loss: 0.050099  acc: 100.0000%(16/16)\n",
            "Batch[56] - loss: 0.035468  acc: 100.0000%(16/16)\n",
            "Batch[57] - loss: 0.031876  acc: 100.0000%(16/16)\n",
            "Batch[58] - loss: 0.076451  acc: 100.0000%(16/16)\n",
            "Batch[59] - loss: 0.032408  acc: 100.0000%(16/16)\n",
            "Batch[60] - loss: 0.029237  acc: 100.0000%(16/16)\n",
            "Batch[61] - loss: 0.019959  acc: 100.0000%(16/16)\n",
            "Batch[62] - loss: 0.021387  acc: 100.0000%(13/13)\n",
            "Average loss:0.054654 average acc:99.603180%\n",
            "Val set acc: 0.7986577181208053\n",
            "Best val set acc: 0\n",
            "\n",
            "Epoch  9 / 18\n",
            "Batch[0] - loss: 0.035458  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 0.072608  acc: 100.0000%(16/16)\n",
            "Batch[2] - loss: 0.050305  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.017823  acc: 100.0000%(16/16)\n",
            "Batch[4] - loss: 0.029175  acc: 100.0000%(16/16)\n",
            "Batch[5] - loss: 0.052027  acc: 100.0000%(16/16)\n",
            "Batch[6] - loss: 0.012749  acc: 100.0000%(16/16)\n",
            "Batch[7] - loss: 0.021230  acc: 100.0000%(16/16)\n",
            "Batch[8] - loss: 0.022845  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.027869  acc: 100.0000%(16/16)\n",
            "Batch[10] - loss: 0.016632  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.036275  acc: 100.0000%(16/16)\n",
            "Batch[12] - loss: 0.020898  acc: 100.0000%(16/16)\n",
            "Batch[13] - loss: 0.009753  acc: 100.0000%(16/16)\n",
            "Batch[14] - loss: 0.015326  acc: 100.0000%(16/16)\n",
            "Batch[15] - loss: 0.025870  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.028201  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 0.035762  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.024598  acc: 100.0000%(16/16)\n",
            "Batch[19] - loss: 0.018109  acc: 100.0000%(16/16)\n",
            "Batch[20] - loss: 0.021906  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 0.022137  acc: 100.0000%(16/16)\n",
            "Batch[22] - loss: 0.012953  acc: 100.0000%(16/16)\n",
            "Batch[23] - loss: 0.084829  acc: 93.7500%(15/16)\n",
            "Batch[24] - loss: 0.031088  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.042462  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.035399  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 0.068601  acc: 100.0000%(16/16)\n",
            "Batch[28] - loss: 0.069768  acc: 100.0000%(16/16)\n",
            "Batch[29] - loss: 0.023285  acc: 100.0000%(16/16)\n",
            "Batch[30] - loss: 0.024407  acc: 100.0000%(16/16)\n",
            "Batch[31] - loss: 0.019234  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.038832  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 0.027064  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.015706  acc: 100.0000%(16/16)\n",
            "Batch[35] - loss: 0.064794  acc: 100.0000%(16/16)\n",
            "Batch[36] - loss: 0.025758  acc: 100.0000%(16/16)\n",
            "Batch[37] - loss: 0.013021  acc: 100.0000%(16/16)\n",
            "Batch[38] - loss: 0.029675  acc: 100.0000%(16/16)\n",
            "Batch[39] - loss: 0.031116  acc: 100.0000%(16/16)\n",
            "Batch[40] - loss: 0.011619  acc: 100.0000%(16/16)\n",
            "Batch[41] - loss: 0.011956  acc: 100.0000%(16/16)\n",
            "Batch[42] - loss: 0.016174  acc: 100.0000%(16/16)\n",
            "Batch[43] - loss: 0.028176  acc: 100.0000%(16/16)\n",
            "Batch[44] - loss: 0.103037  acc: 93.7500%(15/16)\n",
            "Batch[45] - loss: 0.027891  acc: 100.0000%(16/16)\n",
            "Batch[46] - loss: 0.010854  acc: 100.0000%(16/16)\n",
            "Batch[47] - loss: 0.040643  acc: 100.0000%(16/16)\n",
            "Batch[48] - loss: 0.031295  acc: 100.0000%(16/16)\n",
            "Batch[49] - loss: 0.015048  acc: 100.0000%(16/16)\n",
            "Batch[50] - loss: 0.025136  acc: 100.0000%(16/16)\n",
            "Batch[51] - loss: 0.047841  acc: 100.0000%(16/16)\n",
            "Batch[52] - loss: 0.045435  acc: 100.0000%(16/16)\n",
            "Batch[53] - loss: 0.010400  acc: 100.0000%(16/16)\n",
            "Batch[54] - loss: 0.016482  acc: 100.0000%(16/16)\n",
            "Batch[55] - loss: 0.046024  acc: 100.0000%(16/16)\n",
            "Batch[56] - loss: 0.031112  acc: 100.0000%(16/16)\n",
            "Batch[57] - loss: 0.037754  acc: 100.0000%(16/16)\n",
            "Batch[58] - loss: 0.029979  acc: 100.0000%(16/16)\n",
            "Batch[59] - loss: 0.009813  acc: 100.0000%(16/16)\n",
            "Batch[60] - loss: 0.016071  acc: 100.0000%(16/16)\n",
            "Batch[61] - loss: 0.014169  acc: 100.0000%(16/16)\n",
            "Batch[62] - loss: 0.015706  acc: 100.0000%(13/13)\n",
            "Average loss:0.030447 average acc:99.801590%\n",
            "Val set acc: 0.785234899328859\n",
            "Best val set acc: 0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          NR    0.80645   0.65789   0.72464        38\n",
            "          FR    0.69048   0.78378   0.73418        37\n",
            "          TR    0.72093   0.83784   0.77500        37\n",
            "          UR    0.96970   0.86486   0.91429        37\n",
            "\n",
            "    accuracy                        0.78523       149\n",
            "   macro avg    0.79689   0.78610   0.78703       149\n",
            "weighted avg    0.79695   0.78523   0.78661       149\n",
            "\n",
            "save model!!!\n",
            "\n",
            "Epoch  10 / 18\n",
            "Batch[0] - loss: 0.025321  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 0.016587  acc: 100.0000%(16/16)\n",
            "Batch[2] - loss: 0.026605  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.023201  acc: 100.0000%(16/16)\n",
            "Batch[4] - loss: 0.014986  acc: 100.0000%(16/16)\n",
            "Batch[5] - loss: 0.021534  acc: 100.0000%(16/16)\n",
            "Batch[6] - loss: 0.033394  acc: 100.0000%(16/16)\n",
            "Batch[7] - loss: 0.013525  acc: 100.0000%(16/16)\n",
            "Batch[8] - loss: 0.019868  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.017073  acc: 100.0000%(16/16)\n",
            "Batch[10] - loss: 0.011726  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.028595  acc: 100.0000%(16/16)\n",
            "Batch[12] - loss: 0.034158  acc: 100.0000%(16/16)\n",
            "Batch[13] - loss: 0.023276  acc: 100.0000%(16/16)\n",
            "Batch[14] - loss: 0.012226  acc: 100.0000%(16/16)\n",
            "Batch[15] - loss: 0.010373  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.022378  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 0.023533  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.003057  acc: 100.0000%(16/16)\n",
            "Batch[19] - loss: 0.030141  acc: 100.0000%(16/16)\n",
            "Batch[20] - loss: 0.030557  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 0.013625  acc: 100.0000%(16/16)\n",
            "Batch[22] - loss: 0.048081  acc: 100.0000%(16/16)\n",
            "Batch[23] - loss: 0.009317  acc: 100.0000%(16/16)\n",
            "Batch[24] - loss: 0.027940  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.011586  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.072052  acc: 93.7500%(15/16)\n",
            "Batch[27] - loss: 0.016460  acc: 100.0000%(16/16)\n",
            "Batch[28] - loss: 0.023301  acc: 100.0000%(16/16)\n",
            "Batch[29] - loss: 0.021468  acc: 100.0000%(16/16)\n",
            "Batch[30] - loss: 0.012548  acc: 100.0000%(16/16)\n",
            "Batch[31] - loss: 0.017106  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.021681  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 0.027348  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.025081  acc: 100.0000%(16/16)\n",
            "Batch[35] - loss: 0.028723  acc: 100.0000%(16/16)\n",
            "Batch[36] - loss: 0.018651  acc: 100.0000%(16/16)\n",
            "Batch[37] - loss: 0.026526  acc: 100.0000%(16/16)\n",
            "Batch[38] - loss: 0.123582  acc: 93.7500%(15/16)\n",
            "Batch[39] - loss: 0.020027  acc: 100.0000%(16/16)\n",
            "Batch[40] - loss: 0.014588  acc: 100.0000%(16/16)\n",
            "Batch[41] - loss: 0.019882  acc: 100.0000%(16/16)\n",
            "Batch[42] - loss: 0.016451  acc: 100.0000%(16/16)\n",
            "Batch[43] - loss: 0.010864  acc: 100.0000%(16/16)\n",
            "Batch[44] - loss: 0.025321  acc: 100.0000%(16/16)\n",
            "Batch[45] - loss: 0.026969  acc: 100.0000%(16/16)\n",
            "Batch[46] - loss: 0.013245  acc: 100.0000%(16/16)\n",
            "Batch[47] - loss: 0.014303  acc: 100.0000%(16/16)\n",
            "Batch[48] - loss: 0.051920  acc: 100.0000%(16/16)\n",
            "Batch[49] - loss: 0.021840  acc: 100.0000%(16/16)\n",
            "Batch[50] - loss: 0.016953  acc: 100.0000%(16/16)\n",
            "Batch[51] - loss: 0.018349  acc: 100.0000%(16/16)\n",
            "Batch[52] - loss: 0.038791  acc: 100.0000%(16/16)\n",
            "Batch[53] - loss: 0.030593  acc: 100.0000%(16/16)\n",
            "Batch[54] - loss: 0.015656  acc: 100.0000%(16/16)\n",
            "Batch[55] - loss: 0.010540  acc: 100.0000%(16/16)\n",
            "Batch[56] - loss: 0.022066  acc: 100.0000%(16/16)\n",
            "Batch[57] - loss: 0.028165  acc: 100.0000%(16/16)\n",
            "Batch[58] - loss: 0.016467  acc: 100.0000%(16/16)\n",
            "Batch[59] - loss: 0.005264  acc: 100.0000%(16/16)\n",
            "Batch[60] - loss: 0.009301  acc: 100.0000%(16/16)\n",
            "Batch[61] - loss: 0.014171  acc: 100.0000%(16/16)\n",
            "Batch[62] - loss: 0.013083  acc: 100.0000%(13/13)\n",
            "Average loss:0.023206 average acc:99.801590%\n",
            "Val set acc: 0.7919463087248322\n",
            "Best val set acc: 0.785234899328859\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          NR    0.77143   0.71053   0.73973        38\n",
            "          FR    0.73684   0.75676   0.74667        37\n",
            "          TR    0.76923   0.81081   0.78947        37\n",
            "          UR    0.89189   0.89189   0.89189        37\n",
            "\n",
            "    accuracy                        0.79195       149\n",
            "   macro avg    0.79235   0.79250   0.79194       149\n",
            "weighted avg    0.79221   0.79195   0.79159       149\n",
            "\n",
            "save model!!!\n",
            "\n",
            "Epoch  11 / 18\n",
            "Batch[0] - loss: 0.014610  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 0.023913  acc: 100.0000%(16/16)\n",
            "Batch[2] - loss: 0.038546  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.013686  acc: 100.0000%(16/16)\n",
            "Batch[4] - loss: 0.017496  acc: 100.0000%(16/16)\n",
            "Batch[5] - loss: 0.017454  acc: 100.0000%(16/16)\n",
            "Batch[6] - loss: 0.023516  acc: 100.0000%(16/16)\n",
            "Batch[7] - loss: 0.015380  acc: 100.0000%(16/16)\n",
            "Batch[8] - loss: 0.036059  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.010662  acc: 100.0000%(16/16)\n",
            "Batch[10] - loss: 0.006548  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.011980  acc: 100.0000%(16/16)\n",
            "Batch[12] - loss: 0.048053  acc: 100.0000%(16/16)\n",
            "Batch[13] - loss: 0.008925  acc: 100.0000%(16/16)\n",
            "Batch[14] - loss: 0.009141  acc: 100.0000%(16/16)\n",
            "Batch[15] - loss: 0.024659  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.012630  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 0.023904  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.008667  acc: 100.0000%(16/16)\n",
            "Batch[19] - loss: 0.026712  acc: 100.0000%(16/16)\n",
            "Batch[20] - loss: 0.013241  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 0.005266  acc: 100.0000%(16/16)\n",
            "Batch[22] - loss: 0.006257  acc: 100.0000%(16/16)\n",
            "Batch[23] - loss: 0.018719  acc: 100.0000%(16/16)\n",
            "Batch[24] - loss: 0.014367  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.011984  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.004070  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 0.006544  acc: 100.0000%(16/16)\n",
            "Batch[28] - loss: 0.037723  acc: 100.0000%(16/16)\n",
            "Batch[29] - loss: 0.018857  acc: 100.0000%(16/16)\n",
            "Batch[30] - loss: 0.009876  acc: 100.0000%(16/16)\n",
            "Batch[31] - loss: 0.010804  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.010910  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 0.017208  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.020759  acc: 100.0000%(16/16)\n",
            "Batch[35] - loss: 0.376438  acc: 93.7500%(15/16)\n",
            "Batch[36] - loss: 0.020304  acc: 100.0000%(16/16)\n",
            "Batch[37] - loss: 0.008423  acc: 100.0000%(16/16)\n",
            "Batch[38] - loss: 0.011744  acc: 100.0000%(16/16)\n",
            "Batch[39] - loss: 0.012852  acc: 100.0000%(16/16)\n",
            "Batch[40] - loss: 0.034052  acc: 100.0000%(16/16)\n",
            "Batch[41] - loss: 0.009523  acc: 100.0000%(16/16)\n",
            "Batch[42] - loss: 0.015130  acc: 100.0000%(16/16)\n",
            "Batch[43] - loss: 0.006840  acc: 100.0000%(16/16)\n",
            "Batch[44] - loss: 0.014381  acc: 100.0000%(16/16)\n",
            "Batch[45] - loss: 0.021361  acc: 100.0000%(16/16)\n",
            "Batch[46] - loss: 0.017407  acc: 100.0000%(16/16)\n",
            "Batch[47] - loss: 0.024661  acc: 100.0000%(16/16)\n",
            "Batch[48] - loss: 0.014528  acc: 100.0000%(16/16)\n",
            "Batch[49] - loss: 0.020239  acc: 100.0000%(16/16)\n",
            "Batch[50] - loss: 0.004371  acc: 100.0000%(16/16)\n",
            "Batch[51] - loss: 0.018016  acc: 100.0000%(16/16)\n",
            "Batch[52] - loss: 0.012968  acc: 100.0000%(16/16)\n",
            "Batch[53] - loss: 0.022659  acc: 100.0000%(16/16)\n",
            "Batch[54] - loss: 0.026176  acc: 100.0000%(16/16)\n",
            "Batch[55] - loss: 0.004641  acc: 100.0000%(16/16)\n",
            "Batch[56] - loss: 0.015002  acc: 100.0000%(16/16)\n",
            "Batch[57] - loss: 0.003063  acc: 100.0000%(16/16)\n",
            "Batch[58] - loss: 0.021144  acc: 100.0000%(16/16)\n",
            "Batch[59] - loss: 0.023371  acc: 100.0000%(16/16)\n",
            "Batch[60] - loss: 0.024173  acc: 100.0000%(16/16)\n",
            "Batch[61] - loss: 0.025330  acc: 100.0000%(16/16)\n",
            "Batch[62] - loss: 0.008097  acc: 100.0000%(13/13)\n",
            "Average loss:0.022476 average acc:99.900803%\n",
            "Val set acc: 0.7785234899328859\n",
            "Best val set acc: 0.7919463087248322\n",
            "\n",
            "Epoch  12 / 18\n",
            "Batch[0] - loss: 0.013765  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 0.009631  acc: 100.0000%(16/16)\n",
            "Batch[2] - loss: 0.022947  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.012515  acc: 100.0000%(16/16)\n",
            "Batch[4] - loss: 0.074527  acc: 93.7500%(15/16)\n",
            "Batch[5] - loss: 0.010021  acc: 100.0000%(16/16)\n",
            "Batch[6] - loss: 0.011961  acc: 100.0000%(16/16)\n",
            "Batch[7] - loss: 0.008327  acc: 100.0000%(16/16)\n",
            "Batch[8] - loss: 0.022147  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.006138  acc: 100.0000%(16/16)\n",
            "Batch[10] - loss: 0.010171  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.011637  acc: 100.0000%(16/16)\n",
            "Batch[12] - loss: 0.019881  acc: 100.0000%(16/16)\n",
            "Batch[13] - loss: 0.018379  acc: 100.0000%(16/16)\n",
            "Batch[14] - loss: 0.062053  acc: 93.7500%(15/16)\n",
            "Batch[15] - loss: 0.020586  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.011487  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 0.011114  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.007886  acc: 100.0000%(16/16)\n",
            "Batch[19] - loss: 0.020651  acc: 100.0000%(16/16)\n",
            "Batch[20] - loss: 0.016518  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 0.007374  acc: 100.0000%(16/16)\n",
            "Batch[22] - loss: 0.018223  acc: 100.0000%(16/16)\n",
            "Batch[23] - loss: 0.011046  acc: 100.0000%(16/16)\n",
            "Batch[24] - loss: 0.005136  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.004544  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.022152  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 0.009973  acc: 100.0000%(16/16)\n",
            "Batch[28] - loss: 0.015431  acc: 100.0000%(16/16)\n",
            "Batch[29] - loss: 0.006339  acc: 100.0000%(16/16)\n",
            "Batch[30] - loss: 0.016042  acc: 100.0000%(16/16)\n",
            "Batch[31] - loss: 0.041444  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.004587  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 0.006443  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.019131  acc: 100.0000%(16/16)\n",
            "Batch[35] - loss: 0.160184  acc: 93.7500%(15/16)\n",
            "Batch[36] - loss: 0.007197  acc: 100.0000%(16/16)\n",
            "Batch[37] - loss: 0.007463  acc: 100.0000%(16/16)\n",
            "Batch[38] - loss: 0.007389  acc: 100.0000%(16/16)\n",
            "Batch[39] - loss: 0.019806  acc: 100.0000%(16/16)\n",
            "Batch[40] - loss: 0.010676  acc: 100.0000%(16/16)\n",
            "Batch[41] - loss: 0.006561  acc: 100.0000%(16/16)\n",
            "Batch[42] - loss: 0.007890  acc: 100.0000%(16/16)\n",
            "Batch[43] - loss: 0.008950  acc: 100.0000%(16/16)\n",
            "Batch[44] - loss: 0.014497  acc: 100.0000%(16/16)\n",
            "Batch[45] - loss: 0.005202  acc: 100.0000%(16/16)\n",
            "Batch[46] - loss: 0.010168  acc: 100.0000%(16/16)\n",
            "Batch[47] - loss: 0.022016  acc: 100.0000%(16/16)\n",
            "Batch[48] - loss: 0.004844  acc: 100.0000%(16/16)\n",
            "Batch[49] - loss: 0.006694  acc: 100.0000%(16/16)\n",
            "Batch[50] - loss: 0.009961  acc: 100.0000%(16/16)\n",
            "Batch[51] - loss: 0.016734  acc: 100.0000%(16/16)\n",
            "Batch[52] - loss: 0.010962  acc: 100.0000%(16/16)\n",
            "Batch[53] - loss: 0.009120  acc: 100.0000%(16/16)\n",
            "Batch[54] - loss: 0.007581  acc: 100.0000%(16/16)\n",
            "Batch[55] - loss: 0.011802  acc: 100.0000%(16/16)\n",
            "Batch[56] - loss: 0.004517  acc: 100.0000%(16/16)\n",
            "Batch[57] - loss: 0.011575  acc: 100.0000%(16/16)\n",
            "Batch[58] - loss: 0.017158  acc: 100.0000%(16/16)\n",
            "Batch[59] - loss: 0.013728  acc: 100.0000%(16/16)\n",
            "Batch[60] - loss: 0.006825  acc: 100.0000%(16/16)\n",
            "Batch[61] - loss: 0.024052  acc: 100.0000%(16/16)\n",
            "Batch[62] - loss: 0.020081  acc: 100.0000%(13/13)\n",
            "Average loss:0.016728 average acc:99.702385%\n",
            "Val set acc: 0.8053691275167785\n",
            "Best val set acc: 0.7919463087248322\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          NR    0.74359   0.76316   0.75325        38\n",
            "          FR    0.75676   0.75676   0.75676        37\n",
            "          TR    0.81081   0.81081   0.81081        37\n",
            "          UR    0.91667   0.89189   0.90411        37\n",
            "\n",
            "    accuracy                        0.80537       149\n",
            "   macro avg    0.80696   0.80565   0.80623       149\n",
            "weighted avg    0.80653   0.80537   0.80588       149\n",
            "\n",
            "save model!!!\n",
            "\n",
            "Epoch  13 / 18\n",
            "Batch[0] - loss: 0.012341  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 0.008306  acc: 100.0000%(16/16)\n",
            "Batch[2] - loss: 0.009055  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.016835  acc: 100.0000%(16/16)\n",
            "Batch[4] - loss: 0.007014  acc: 100.0000%(16/16)\n",
            "Batch[5] - loss: 0.038975  acc: 100.0000%(16/16)\n",
            "Batch[6] - loss: 0.019006  acc: 100.0000%(16/16)\n",
            "Batch[7] - loss: 0.007970  acc: 100.0000%(16/16)\n",
            "Batch[8] - loss: 0.017900  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.008815  acc: 100.0000%(16/16)\n",
            "Batch[10] - loss: 0.014700  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.003655  acc: 100.0000%(16/16)\n",
            "Batch[12] - loss: 0.014059  acc: 100.0000%(16/16)\n",
            "Batch[13] - loss: 0.006839  acc: 100.0000%(16/16)\n",
            "Batch[14] - loss: 0.012393  acc: 100.0000%(16/16)\n",
            "Batch[15] - loss: 0.044218  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.011535  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 0.013099  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.005534  acc: 100.0000%(16/16)\n",
            "Batch[19] - loss: 0.008113  acc: 100.0000%(16/16)\n",
            "Batch[20] - loss: 0.011334  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 0.012245  acc: 100.0000%(16/16)\n",
            "Batch[22] - loss: 0.008210  acc: 100.0000%(16/16)\n",
            "Batch[23] - loss: 0.015437  acc: 100.0000%(16/16)\n",
            "Batch[24] - loss: 0.009993  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.009974  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.007998  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 0.007984  acc: 100.0000%(16/16)\n",
            "Batch[28] - loss: 0.012763  acc: 100.0000%(16/16)\n",
            "Batch[29] - loss: 0.003803  acc: 100.0000%(16/16)\n",
            "Batch[30] - loss: 0.015042  acc: 100.0000%(16/16)\n",
            "Batch[31] - loss: 0.004935  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.005749  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 0.006144  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.005474  acc: 100.0000%(16/16)\n",
            "Batch[35] - loss: 0.012373  acc: 100.0000%(16/16)\n",
            "Batch[36] - loss: 0.006728  acc: 100.0000%(16/16)\n",
            "Batch[37] - loss: 0.008828  acc: 100.0000%(16/16)\n",
            "Batch[38] - loss: 0.009718  acc: 100.0000%(16/16)\n",
            "Batch[39] - loss: 0.010508  acc: 100.0000%(16/16)\n",
            "Batch[40] - loss: 0.004095  acc: 100.0000%(16/16)\n",
            "Batch[41] - loss: 0.013295  acc: 100.0000%(16/16)\n",
            "Batch[42] - loss: 0.009167  acc: 100.0000%(16/16)\n",
            "Batch[43] - loss: 0.020455  acc: 100.0000%(16/16)\n",
            "Batch[44] - loss: 0.109628  acc: 93.7500%(15/16)\n",
            "Batch[45] - loss: 0.017111  acc: 100.0000%(16/16)\n",
            "Batch[46] - loss: 0.004266  acc: 100.0000%(16/16)\n",
            "Batch[47] - loss: 0.004446  acc: 100.0000%(16/16)\n",
            "Batch[48] - loss: 0.008652  acc: 100.0000%(16/16)\n",
            "Batch[49] - loss: 0.008610  acc: 100.0000%(16/16)\n",
            "Batch[50] - loss: 0.012661  acc: 100.0000%(16/16)\n",
            "Batch[51] - loss: 0.007374  acc: 100.0000%(16/16)\n",
            "Batch[52] - loss: 0.009058  acc: 100.0000%(16/16)\n",
            "Batch[53] - loss: 0.006344  acc: 100.0000%(16/16)\n",
            "Batch[54] - loss: 0.017250  acc: 100.0000%(16/16)\n",
            "Batch[55] - loss: 0.005385  acc: 100.0000%(16/16)\n",
            "Batch[56] - loss: 0.007383  acc: 100.0000%(16/16)\n",
            "Batch[57] - loss: 0.010774  acc: 100.0000%(16/16)\n",
            "Batch[58] - loss: 0.074303  acc: 93.7500%(15/16)\n",
            "Batch[59] - loss: 0.013361  acc: 100.0000%(16/16)\n",
            "Batch[60] - loss: 0.009684  acc: 100.0000%(16/16)\n",
            "Batch[61] - loss: 0.007701  acc: 100.0000%(16/16)\n",
            "Batch[62] - loss: 0.006477  acc: 100.0000%(13/13)\n",
            "Average loss:0.013541 average acc:99.801590%\n",
            "Val set acc: 0.7986577181208053\n",
            "Best val set acc: 0.8053691275167785\n",
            "\n",
            "Epoch  14 / 18\n",
            "Batch[0] - loss: 0.002845  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 0.003305  acc: 100.0000%(16/16)\n",
            "Batch[2] - loss: 0.013257  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.008503  acc: 100.0000%(16/16)\n",
            "Batch[4] - loss: 0.005287  acc: 100.0000%(16/16)\n",
            "Batch[5] - loss: 0.006696  acc: 100.0000%(16/16)\n",
            "Batch[6] - loss: 0.006500  acc: 100.0000%(16/16)\n",
            "Batch[7] - loss: 0.004521  acc: 100.0000%(16/16)\n",
            "Batch[8] - loss: 0.013119  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.007401  acc: 100.0000%(16/16)\n",
            "Batch[10] - loss: 0.010905  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.009208  acc: 100.0000%(16/16)\n",
            "Batch[12] - loss: 0.006897  acc: 100.0000%(16/16)\n",
            "Batch[13] - loss: 0.006782  acc: 100.0000%(16/16)\n",
            "Batch[14] - loss: 0.005233  acc: 100.0000%(16/16)\n",
            "Batch[15] - loss: 0.015596  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.004361  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 0.009278  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.038393  acc: 100.0000%(16/16)\n",
            "Batch[19] - loss: 0.006209  acc: 100.0000%(16/16)\n",
            "Batch[20] - loss: 0.008321  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 0.005349  acc: 100.0000%(16/16)\n",
            "Batch[22] - loss: 0.004918  acc: 100.0000%(16/16)\n",
            "Batch[23] - loss: 0.002073  acc: 100.0000%(16/16)\n",
            "Batch[24] - loss: 0.046726  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.016073  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.003253  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 0.007422  acc: 100.0000%(16/16)\n",
            "Batch[28] - loss: 0.009554  acc: 100.0000%(16/16)\n",
            "Batch[29] - loss: 0.005042  acc: 100.0000%(16/16)\n",
            "Batch[30] - loss: 0.008802  acc: 100.0000%(16/16)\n",
            "Batch[31] - loss: 0.046285  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.009413  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 0.004681  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.008980  acc: 100.0000%(16/16)\n",
            "Batch[35] - loss: 0.011693  acc: 100.0000%(16/16)\n",
            "Batch[36] - loss: 0.003456  acc: 100.0000%(16/16)\n",
            "Batch[37] - loss: 0.014480  acc: 100.0000%(16/16)\n",
            "Batch[38] - loss: 0.005647  acc: 100.0000%(16/16)\n",
            "Batch[39] - loss: 0.058929  acc: 93.7500%(15/16)\n",
            "Batch[40] - loss: 0.012667  acc: 100.0000%(16/16)\n",
            "Batch[41] - loss: 0.004629  acc: 100.0000%(16/16)\n",
            "Batch[42] - loss: 0.010819  acc: 100.0000%(16/16)\n",
            "Batch[43] - loss: 0.002191  acc: 100.0000%(16/16)\n",
            "Batch[44] - loss: 0.003948  acc: 100.0000%(16/16)\n",
            "Batch[45] - loss: 0.005172  acc: 100.0000%(16/16)\n",
            "Batch[46] - loss: 0.004017  acc: 100.0000%(16/16)\n",
            "Batch[47] - loss: 0.012454  acc: 100.0000%(16/16)\n",
            "Batch[48] - loss: 0.009680  acc: 100.0000%(16/16)\n",
            "Batch[49] - loss: 0.002819  acc: 100.0000%(16/16)\n",
            "Batch[50] - loss: 0.005107  acc: 100.0000%(16/16)\n",
            "Batch[51] - loss: 0.008036  acc: 100.0000%(16/16)\n",
            "Batch[52] - loss: 0.003565  acc: 100.0000%(16/16)\n",
            "Batch[53] - loss: 0.013343  acc: 100.0000%(16/16)\n",
            "Batch[54] - loss: 0.010741  acc: 100.0000%(16/16)\n",
            "Batch[55] - loss: 0.010320  acc: 100.0000%(16/16)\n",
            "Batch[56] - loss: 0.004317  acc: 100.0000%(16/16)\n",
            "Batch[57] - loss: 0.043892  acc: 100.0000%(16/16)\n",
            "Batch[58] - loss: 0.096513  acc: 93.7500%(15/16)\n",
            "Batch[59] - loss: 0.017789  acc: 100.0000%(16/16)\n",
            "Batch[60] - loss: 0.015377  acc: 100.0000%(16/16)\n",
            "Batch[61] - loss: 0.003611  acc: 100.0000%(16/16)\n",
            "Batch[62] - loss: 0.017031  acc: 100.0000%(13/13)\n",
            "Average loss:0.012435 average acc:99.801590%\n",
            "Val set acc: 0.785234899328859\n",
            "Best val set acc: 0.8053691275167785\n",
            "\n",
            "Epoch  15 / 18\n",
            "Batch[0] - loss: 0.004010  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 0.005595  acc: 100.0000%(16/16)\n",
            "Batch[2] - loss: 0.007010  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.011292  acc: 100.0000%(16/16)\n",
            "Batch[4] - loss: 0.006292  acc: 100.0000%(16/16)\n",
            "Batch[5] - loss: 0.003309  acc: 100.0000%(16/16)\n",
            "Batch[6] - loss: 0.008188  acc: 100.0000%(16/16)\n",
            "Batch[7] - loss: 0.007070  acc: 100.0000%(16/16)\n",
            "Batch[8] - loss: 0.005452  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.014619  acc: 100.0000%(16/16)\n",
            "Batch[10] - loss: 0.006590  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.012756  acc: 100.0000%(16/16)\n",
            "Batch[12] - loss: 0.073405  acc: 93.7500%(15/16)\n",
            "Batch[13] - loss: 0.002927  acc: 100.0000%(16/16)\n",
            "Batch[14] - loss: 0.006556  acc: 100.0000%(16/16)\n",
            "Batch[15] - loss: 0.006399  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.010280  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 0.004921  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.005678  acc: 100.0000%(16/16)\n",
            "Batch[19] - loss: 0.012962  acc: 100.0000%(16/16)\n",
            "Batch[20] - loss: 0.004736  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 0.018605  acc: 100.0000%(16/16)\n",
            "Batch[22] - loss: 0.003411  acc: 100.0000%(16/16)\n",
            "Batch[23] - loss: 0.007173  acc: 100.0000%(16/16)\n",
            "Batch[24] - loss: 0.004026  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.004260  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.006977  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 0.004774  acc: 100.0000%(16/16)\n",
            "Batch[28] - loss: 0.012940  acc: 100.0000%(16/16)\n",
            "Batch[29] - loss: 0.004610  acc: 100.0000%(16/16)\n",
            "Batch[30] - loss: 0.004157  acc: 100.0000%(16/16)\n",
            "Batch[31] - loss: 0.006969  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.009690  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 0.003680  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.007589  acc: 100.0000%(16/16)\n",
            "Batch[35] - loss: 0.006037  acc: 100.0000%(16/16)\n",
            "Batch[36] - loss: 0.004640  acc: 100.0000%(16/16)\n",
            "Batch[37] - loss: 0.004255  acc: 100.0000%(16/16)\n",
            "Batch[38] - loss: 0.005716  acc: 100.0000%(16/16)\n",
            "Batch[39] - loss: 0.002888  acc: 100.0000%(16/16)\n",
            "Batch[40] - loss: 0.006624  acc: 100.0000%(16/16)\n",
            "Batch[41] - loss: 0.009000  acc: 100.0000%(16/16)\n",
            "Batch[42] - loss: 0.083824  acc: 100.0000%(16/16)\n",
            "Batch[43] - loss: 0.007665  acc: 100.0000%(16/16)\n",
            "Batch[44] - loss: 0.004332  acc: 100.0000%(16/16)\n",
            "Batch[45] - loss: 0.006955  acc: 100.0000%(16/16)\n",
            "Batch[46] - loss: 0.004547  acc: 100.0000%(16/16)\n",
            "Batch[47] - loss: 0.002850  acc: 100.0000%(16/16)\n",
            "Batch[48] - loss: 0.011337  acc: 100.0000%(16/16)\n",
            "Batch[49] - loss: 0.021190  acc: 100.0000%(16/16)\n",
            "Batch[50] - loss: 0.003677  acc: 100.0000%(16/16)\n",
            "Batch[51] - loss: 0.004218  acc: 100.0000%(16/16)\n",
            "Batch[52] - loss: 0.008336  acc: 100.0000%(16/16)\n",
            "Batch[53] - loss: 0.004530  acc: 100.0000%(16/16)\n",
            "Batch[54] - loss: 0.010529  acc: 100.0000%(16/16)\n",
            "Batch[55] - loss: 0.003050  acc: 100.0000%(16/16)\n",
            "Batch[56] - loss: 0.003570  acc: 100.0000%(16/16)\n",
            "Batch[57] - loss: 0.001727  acc: 100.0000%(16/16)\n",
            "Batch[58] - loss: 0.006955  acc: 100.0000%(16/16)\n",
            "Batch[59] - loss: 0.004695  acc: 100.0000%(16/16)\n",
            "Batch[60] - loss: 0.003845  acc: 100.0000%(16/16)\n",
            "Batch[61] - loss: 0.005860  acc: 100.0000%(16/16)\n",
            "Batch[62] - loss: 0.009536  acc: 100.0000%(13/13)\n",
            "Average loss:0.009068 average acc:99.900803%\n",
            "Val set acc: 0.8053691275167785\n",
            "Best val set acc: 0.8053691275167785\n",
            "\n",
            "Epoch  16 / 18\n",
            "Batch[0] - loss: 0.001951  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 0.002902  acc: 100.0000%(16/16)\n",
            "Batch[2] - loss: 0.006055  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.017673  acc: 100.0000%(16/16)\n",
            "Batch[4] - loss: 0.041458  acc: 100.0000%(16/16)\n",
            "Batch[5] - loss: 0.002716  acc: 100.0000%(16/16)\n",
            "Batch[6] - loss: 0.014809  acc: 100.0000%(16/16)\n",
            "Batch[7] - loss: 0.009589  acc: 100.0000%(16/16)\n",
            "Batch[8] - loss: 0.024329  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.009810  acc: 100.0000%(16/16)\n",
            "Batch[10] - loss: 0.006374  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.002796  acc: 100.0000%(16/16)\n",
            "Batch[12] - loss: 0.005384  acc: 100.0000%(16/16)\n",
            "Batch[13] - loss: 0.009339  acc: 100.0000%(16/16)\n",
            "Batch[14] - loss: 0.003799  acc: 100.0000%(16/16)\n",
            "Batch[15] - loss: 0.004465  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.002825  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 0.004084  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.011604  acc: 100.0000%(16/16)\n",
            "Batch[19] - loss: 0.004188  acc: 100.0000%(16/16)\n",
            "Batch[20] - loss: 0.003178  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 0.007680  acc: 100.0000%(16/16)\n",
            "Batch[22] - loss: 0.010826  acc: 100.0000%(16/16)\n",
            "Batch[23] - loss: 0.010471  acc: 100.0000%(16/16)\n",
            "Batch[24] - loss: 0.006506  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.001620  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.011303  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 0.010802  acc: 100.0000%(16/16)\n",
            "Batch[28] - loss: 0.001944  acc: 100.0000%(16/16)\n",
            "Batch[29] - loss: 0.004699  acc: 100.0000%(16/16)\n",
            "Batch[30] - loss: 0.016077  acc: 100.0000%(16/16)\n",
            "Batch[31] - loss: 0.006863  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.007457  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 0.006129  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.006855  acc: 100.0000%(16/16)\n",
            "Batch[35] - loss: 0.035836  acc: 100.0000%(16/16)\n",
            "Batch[36] - loss: 0.007394  acc: 100.0000%(16/16)\n",
            "Batch[37] - loss: 0.009192  acc: 100.0000%(16/16)\n",
            "Batch[38] - loss: 0.004712  acc: 100.0000%(16/16)\n",
            "Batch[39] - loss: 0.002666  acc: 100.0000%(16/16)\n",
            "Batch[40] - loss: 0.006142  acc: 100.0000%(16/16)\n",
            "Batch[41] - loss: 0.001990  acc: 100.0000%(16/16)\n",
            "Batch[42] - loss: 0.002043  acc: 100.0000%(16/16)\n",
            "Batch[43] - loss: 0.005885  acc: 100.0000%(16/16)\n",
            "Batch[44] - loss: 0.019728  acc: 100.0000%(16/16)\n",
            "Batch[45] - loss: 0.005672  acc: 100.0000%(16/16)\n",
            "Batch[46] - loss: 0.006602  acc: 100.0000%(16/16)\n",
            "Batch[47] - loss: 0.009177  acc: 100.0000%(16/16)\n",
            "Batch[48] - loss: 0.007533  acc: 100.0000%(16/16)\n",
            "Batch[49] - loss: 0.005069  acc: 100.0000%(16/16)\n",
            "Batch[50] - loss: 0.013803  acc: 100.0000%(16/16)\n",
            "Batch[51] - loss: 0.003240  acc: 100.0000%(16/16)\n",
            "Batch[52] - loss: 0.008499  acc: 100.0000%(16/16)\n",
            "Batch[53] - loss: 0.004086  acc: 100.0000%(16/16)\n",
            "Batch[54] - loss: 0.002116  acc: 100.0000%(16/16)\n",
            "Batch[55] - loss: 0.003463  acc: 100.0000%(16/16)\n",
            "Batch[56] - loss: 0.004466  acc: 100.0000%(16/16)\n",
            "Batch[57] - loss: 0.003642  acc: 100.0000%(16/16)\n",
            "Batch[58] - loss: 0.009302  acc: 100.0000%(16/16)\n",
            "Batch[59] - loss: 0.002739  acc: 100.0000%(16/16)\n",
            "Batch[60] - loss: 0.003926  acc: 100.0000%(16/16)\n",
            "Batch[61] - loss: 0.002802  acc: 100.0000%(16/16)\n",
            "Batch[62] - loss: 0.003156  acc: 100.0000%(13/13)\n",
            "Average loss:0.007832 average acc:100.000008%\n",
            "Reload the best model...\n",
            "0.0005\n",
            "Val set acc: 0.8053691275167785\n",
            "Best val set acc: 0.8053691275167785\n",
            "\n",
            "Epoch  17 / 18\n",
            "Batch[0] - loss: 0.007098  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 0.006653  acc: 100.0000%(16/16)\n",
            "Batch[2] - loss: 0.020534  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.005104  acc: 100.0000%(16/16)\n",
            "Batch[4] - loss: 0.009679  acc: 100.0000%(16/16)\n",
            "Batch[5] - loss: 0.007165  acc: 100.0000%(16/16)\n",
            "Batch[6] - loss: 0.016309  acc: 100.0000%(16/16)\n",
            "Batch[7] - loss: 0.003676  acc: 100.0000%(16/16)\n",
            "Batch[8] - loss: 0.004235  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.009708  acc: 100.0000%(16/16)\n",
            "Batch[10] - loss: 0.005839  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.007684  acc: 100.0000%(16/16)\n",
            "Batch[12] - loss: 0.009403  acc: 100.0000%(16/16)\n",
            "Batch[13] - loss: 0.007497  acc: 100.0000%(16/16)\n",
            "Batch[14] - loss: 0.009665  acc: 100.0000%(16/16)\n",
            "Batch[15] - loss: 0.011402  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.008918  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 0.017555  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.005259  acc: 100.0000%(16/16)\n",
            "Batch[19] - loss: 0.010635  acc: 100.0000%(16/16)\n",
            "Batch[20] - loss: 0.017167  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 0.007024  acc: 100.0000%(16/16)\n",
            "Batch[22] - loss: 0.006276  acc: 100.0000%(16/16)\n",
            "Batch[23] - loss: 0.002837  acc: 100.0000%(16/16)\n",
            "Batch[24] - loss: 0.047250  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.035103  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.008978  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 0.009195  acc: 100.0000%(16/16)\n",
            "Batch[28] - loss: 0.008697  acc: 100.0000%(16/16)\n",
            "Batch[29] - loss: 0.006327  acc: 100.0000%(16/16)\n",
            "Batch[30] - loss: 0.023806  acc: 100.0000%(16/16)\n",
            "Batch[31] - loss: 0.009026  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.007170  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 0.008085  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.004857  acc: 100.0000%(16/16)\n",
            "Batch[35] - loss: 0.007162  acc: 100.0000%(16/16)\n",
            "Batch[36] - loss: 0.006802  acc: 100.0000%(16/16)\n",
            "Batch[37] - loss: 0.002578  acc: 100.0000%(16/16)\n",
            "Batch[38] - loss: 0.004490  acc: 100.0000%(16/16)\n",
            "Batch[39] - loss: 0.029770  acc: 100.0000%(16/16)\n",
            "Batch[40] - loss: 0.005479  acc: 100.0000%(16/16)\n",
            "Batch[41] - loss: 0.016492  acc: 100.0000%(16/16)\n",
            "Batch[42] - loss: 0.005038  acc: 100.0000%(16/16)\n",
            "Batch[43] - loss: 0.012429  acc: 100.0000%(16/16)\n",
            "Batch[44] - loss: 0.005807  acc: 100.0000%(16/16)\n",
            "Batch[45] - loss: 0.013695  acc: 100.0000%(16/16)\n",
            "Batch[46] - loss: 0.003955  acc: 100.0000%(16/16)\n",
            "Batch[47] - loss: 0.006101  acc: 100.0000%(16/16)\n",
            "Batch[48] - loss: 0.007039  acc: 100.0000%(16/16)\n",
            "Batch[49] - loss: 0.005796  acc: 100.0000%(16/16)\n",
            "Batch[50] - loss: 0.007311  acc: 100.0000%(16/16)\n",
            "Batch[51] - loss: 0.005350  acc: 100.0000%(16/16)\n",
            "Batch[52] - loss: 0.005000  acc: 100.0000%(16/16)\n",
            "Batch[53] - loss: 0.007164  acc: 100.0000%(16/16)\n",
            "Batch[54] - loss: 0.004107  acc: 100.0000%(16/16)\n",
            "Batch[55] - loss: 0.011495  acc: 100.0000%(16/16)\n",
            "Batch[56] - loss: 0.006451  acc: 100.0000%(16/16)\n",
            "Batch[57] - loss: 0.004965  acc: 100.0000%(16/16)\n",
            "Batch[58] - loss: 0.014263  acc: 100.0000%(16/16)\n",
            "Batch[59] - loss: 0.006460  acc: 100.0000%(16/16)\n",
            "Batch[60] - loss: 0.003527  acc: 100.0000%(16/16)\n",
            "Batch[61] - loss: 0.016783  acc: 100.0000%(16/16)\n",
            "Batch[62] - loss: 0.021409  acc: 100.0000%(13/13)\n",
            "Average loss:0.010043 average acc:100.000008%\n",
            "Val set acc: 0.785234899328859\n",
            "Best val set acc: 0.8053691275167785\n",
            "\n",
            "Epoch  18 / 18\n",
            "Batch[0] - loss: 0.070848  acc: 93.7500%(15/16)\n",
            "Batch[1] - loss: 0.005631  acc: 100.0000%(16/16)\n",
            "Batch[2] - loss: 0.009786  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.002771  acc: 100.0000%(16/16)\n",
            "Batch[4] - loss: 0.023395  acc: 100.0000%(16/16)\n",
            "Batch[5] - loss: 0.053614  acc: 93.7500%(15/16)\n",
            "Batch[6] - loss: 0.008151  acc: 100.0000%(16/16)\n",
            "Batch[7] - loss: 0.001870  acc: 100.0000%(16/16)\n",
            "Batch[8] - loss: 0.009777  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.004350  acc: 100.0000%(16/16)\n",
            "Batch[10] - loss: 0.009463  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.013131  acc: 100.0000%(16/16)\n",
            "Batch[12] - loss: 0.006369  acc: 100.0000%(16/16)\n",
            "Batch[13] - loss: 0.005896  acc: 100.0000%(16/16)\n",
            "Batch[14] - loss: 0.009911  acc: 100.0000%(16/16)\n",
            "Batch[15] - loss: 0.015880  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.007207  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 0.009715  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.009863  acc: 100.0000%(16/16)\n",
            "Batch[19] - loss: 0.010472  acc: 100.0000%(16/16)\n",
            "Batch[20] - loss: 0.008274  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 0.006968  acc: 100.0000%(16/16)\n",
            "Batch[22] - loss: 0.005052  acc: 100.0000%(16/16)\n",
            "Batch[23] - loss: 0.006448  acc: 100.0000%(16/16)\n",
            "Batch[24] - loss: 0.003525  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.003341  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.009077  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 0.005977  acc: 100.0000%(16/16)\n",
            "Batch[28] - loss: 0.003792  acc: 100.0000%(16/16)\n",
            "Batch[29] - loss: 0.016992  acc: 100.0000%(16/16)\n",
            "Batch[30] - loss: 0.003608  acc: 100.0000%(16/16)\n",
            "Batch[31] - loss: 0.006184  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.009100  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 0.014731  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.008338  acc: 100.0000%(16/16)\n",
            "Batch[35] - loss: 0.004411  acc: 100.0000%(16/16)\n",
            "Batch[36] - loss: 0.003045  acc: 100.0000%(16/16)\n",
            "Batch[37] - loss: 0.007249  acc: 100.0000%(16/16)\n",
            "Batch[38] - loss: 0.012095  acc: 100.0000%(16/16)\n",
            "Batch[39] - loss: 0.003046  acc: 100.0000%(16/16)\n",
            "Batch[40] - loss: 0.022860  acc: 100.0000%(16/16)\n",
            "Batch[41] - loss: 0.009064  acc: 100.0000%(16/16)\n",
            "Batch[42] - loss: 0.006770  acc: 100.0000%(16/16)\n",
            "Batch[43] - loss: 0.011164  acc: 100.0000%(16/16)\n",
            "Batch[44] - loss: 0.011511  acc: 100.0000%(16/16)\n",
            "Batch[45] - loss: 0.013057  acc: 100.0000%(16/16)\n",
            "Batch[46] - loss: 0.004274  acc: 100.0000%(16/16)\n",
            "Batch[47] - loss: 0.006639  acc: 100.0000%(16/16)\n",
            "Batch[48] - loss: 0.005413  acc: 100.0000%(16/16)\n",
            "Batch[49] - loss: 0.014356  acc: 100.0000%(16/16)\n",
            "Batch[50] - loss: 0.009152  acc: 100.0000%(16/16)\n",
            "Batch[51] - loss: 0.004802  acc: 100.0000%(16/16)\n",
            "Batch[52] - loss: 0.028584  acc: 100.0000%(16/16)\n",
            "Batch[53] - loss: 0.008578  acc: 100.0000%(16/16)\n",
            "Batch[54] - loss: 0.008106  acc: 100.0000%(16/16)\n",
            "Batch[55] - loss: 0.010399  acc: 100.0000%(16/16)\n",
            "Batch[56] - loss: 0.007651  acc: 100.0000%(16/16)\n",
            "Batch[57] - loss: 0.018852  acc: 100.0000%(16/16)\n",
            "Batch[58] - loss: 0.006635  acc: 100.0000%(16/16)\n",
            "Batch[59] - loss: 0.006588  acc: 100.0000%(16/16)\n",
            "Batch[60] - loss: 0.005543  acc: 100.0000%(16/16)\n",
            "Batch[61] - loss: 0.003043  acc: 100.0000%(16/16)\n",
            "Batch[62] - loss: 0.002545  acc: 100.0000%(13/13)\n",
            "Average loss:0.010396 average acc:99.801590%\n",
            "Val set acc: 0.785234899328859\n",
            "Best val set acc: 0.8053691275167785\n",
            "================================\n",
            "task:  twitter16\n",
            "{'lr': 0.001, 'reg': 1e-06, 'embeding_size': 100, 'batch_size': 16, 'nb_filters': 100, 'kernel_sizes': [3, 4, 5], 'dropout': 0.5, 'epochs': 18, 'num_classes': 4, 'target_names': ['NR', 'FR', 'TR', 'UR'], 'save_path': 'checkpoint/weights.best.twitter16.pgan', 'maxlen': 50, 'n_heads': 8, 'embedding_weights': array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
            "         0.        ,  0.        ],\n",
            "       [ 0.02440675,  0.10759468,  0.05138169, ...,  0.18109576,\n",
            "         0.23645975,  0.23041733],\n",
            "       [ 0.20327775,  0.13702367, -0.08342742, ..., -0.01467553,\n",
            "        -0.06087741,  0.23976347],\n",
            "       ...,\n",
            "       [ 0.0844    , -0.023     ,  0.0015    , ..., -0.0029    ,\n",
            "         0.0138    ,  0.05      ],\n",
            "       [ 0.0546    ,  0.0691    , -0.0699    , ...,  0.0537    ,\n",
            "         0.0533    , -0.0008    ],\n",
            "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
            "         0.        ,  0.        ]], dtype=float32), 'A_us': <2213x1490 sparse matrix of type '<class 'numpy.float32'>'\n",
            "\twith 1482 stored elements in Compressed Sparse Column format>, 'A_uu': <2213x2213 sparse matrix of type '<class 'numpy.float64'>'\n",
            "\twith 11135 stored elements in Compressed Sparse Column format>}\n",
            "PGAN(\n",
            "  (word_embedding): Embedding(1364, 300, padding_idx=0)\n",
            "  (user_embedding): Embedding(1092, 100, padding_idx=0)\n",
            "  (source_embedding): Embedding(818, 100)\n",
            "  (convs): ModuleList(\n",
            "    (0): Conv1d(300, 100, kernel_size=(3,), stride=(1,))\n",
            "    (1): Conv1d(300, 100, kernel_size=(4,), stride=(1,))\n",
            "    (2): Conv1d(300, 100, kernel_size=(5,), stride=(1,))\n",
            "  )\n",
            "  (max_poolings): ModuleList(\n",
            "    (0): MaxPool1d(kernel_size=48, stride=48, padding=0, dilation=1, ceil_mode=False)\n",
            "    (1): MaxPool1d(kernel_size=47, stride=47, padding=0, dilation=1, ceil_mode=False)\n",
            "    (2): MaxPool1d(kernel_size=46, stride=46, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (linear): Linear(in_features=400, out_features=200, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (relu): ReLU()\n",
            "  (elu): ELU(alpha=1.0)\n",
            "  (fc_out): Sequential(\n",
            "    (0): Linear(in_features=500, out_features=100, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=100, out_features=4, bias=True)\n",
            "  )\n",
            "  (fc_user_out): Sequential(\n",
            "    (0): Linear(in_features=100, out_features=100, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=100, out_features=3, bias=True)\n",
            "  )\n",
            "  (fc_ruser_out): Sequential(\n",
            "    (0): Linear(in_features=100, out_features=100, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=100, out_features=3, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "Epoch  1 / 18\n",
            "Batch[0] - loss: 3.650699  acc: 12.5000%(2/16)\n",
            "Batch[1] - loss: 3.633595  acc: 12.5000%(2/16)\n",
            "Batch[2] - loss: 3.493782  acc: 25.0000%(4/16)\n",
            "Batch[3] - loss: 3.636045  acc: 18.7500%(3/16)\n",
            "Batch[4] - loss: 3.626108  acc: 18.7500%(3/16)\n",
            "Batch[5] - loss: 3.631130  acc: 6.2500%(1/16)\n",
            "Batch[6] - loss: 3.580086  acc: 25.0000%(4/16)\n",
            "Batch[7] - loss: 3.487132  acc: 43.7500%(7/16)\n",
            "Batch[8] - loss: 3.493214  acc: 31.2500%(5/16)\n",
            "Batch[9] - loss: 3.478811  acc: 37.5000%(6/16)\n",
            "Batch[10] - loss: 3.390605  acc: 50.0000%(8/16)\n",
            "Batch[11] - loss: 3.318794  acc: 25.0000%(4/16)\n",
            "Batch[12] - loss: 3.423770  acc: 43.7500%(7/16)\n",
            "Batch[13] - loss: 3.509410  acc: 31.2500%(5/16)\n",
            "Batch[14] - loss: 3.386972  acc: 25.0000%(4/16)\n",
            "Batch[15] - loss: 3.241268  acc: 50.0000%(8/16)\n",
            "Batch[16] - loss: 3.439003  acc: 31.2500%(5/16)\n",
            "Batch[17] - loss: 3.449783  acc: 56.2500%(9/16)\n",
            "Batch[18] - loss: 3.425535  acc: 18.7500%(3/16)\n",
            "Batch[19] - loss: 3.326566  acc: 31.2500%(5/16)\n",
            "Batch[20] - loss: 3.136626  acc: 62.5000%(10/16)\n",
            "Batch[21] - loss: 2.998718  acc: 43.7500%(7/16)\n",
            "Batch[22] - loss: 3.537755  acc: 12.5000%(2/16)\n",
            "Batch[23] - loss: 4.055321  acc: 43.7500%(7/16)\n",
            "Batch[24] - loss: 3.500090  acc: 43.7500%(7/16)\n",
            "Batch[25] - loss: 3.562221  acc: 43.7500%(7/16)\n",
            "Batch[26] - loss: 3.671896  acc: 31.2500%(5/16)\n",
            "Batch[27] - loss: 3.224599  acc: 50.0000%(8/16)\n",
            "Batch[28] - loss: 3.114870  acc: 37.5000%(6/16)\n",
            "Batch[29] - loss: 3.467442  acc: 43.7500%(7/16)\n",
            "Batch[30] - loss: 3.675333  acc: 12.5000%(2/16)\n",
            "Batch[31] - loss: 3.556028  acc: 25.0000%(4/16)\n",
            "Batch[32] - loss: 3.235339  acc: 31.2500%(5/16)\n",
            "Batch[33] - loss: 3.054069  acc: 50.0000%(8/16)\n",
            "Batch[34] - loss: 3.049175  acc: 50.0000%(4/8)\n",
            "Average loss:3.441765 average acc:33.571430%\n",
            "Val set acc: 0.34146341463414637\n",
            "Best val set acc: 0\n",
            "\n",
            "Epoch  2 / 18\n",
            "Batch[0] - loss: 3.322908  acc: 37.5000%(6/16)\n",
            "Batch[1] - loss: 3.143583  acc: 50.0000%(8/16)\n",
            "Batch[2] - loss: 3.107558  acc: 37.5000%(6/16)\n",
            "Batch[3] - loss: 3.083064  acc: 37.5000%(6/16)\n",
            "Batch[4] - loss: 3.045725  acc: 37.5000%(6/16)\n",
            "Batch[5] - loss: 3.370914  acc: 31.2500%(5/16)\n",
            "Batch[6] - loss: 3.136942  acc: 50.0000%(8/16)\n",
            "Batch[7] - loss: 3.155946  acc: 31.2500%(5/16)\n",
            "Batch[8] - loss: 3.036987  acc: 81.2500%(13/16)\n",
            "Batch[9] - loss: 3.133569  acc: 43.7500%(7/16)\n",
            "Batch[10] - loss: 3.070366  acc: 62.5000%(10/16)\n",
            "Batch[11] - loss: 3.138089  acc: 56.2500%(9/16)\n",
            "Batch[12] - loss: 2.958459  acc: 37.5000%(6/16)\n",
            "Batch[13] - loss: 3.241088  acc: 50.0000%(8/16)\n",
            "Batch[14] - loss: 3.059874  acc: 62.5000%(10/16)\n",
            "Batch[15] - loss: 2.946116  acc: 68.7500%(11/16)\n",
            "Batch[16] - loss: 3.500533  acc: 68.7500%(11/16)\n",
            "Batch[17] - loss: 3.052191  acc: 43.7500%(7/16)\n",
            "Batch[18] - loss: 2.925893  acc: 68.7500%(11/16)\n",
            "Batch[19] - loss: 2.732471  acc: 62.5000%(10/16)\n",
            "Batch[20] - loss: 2.828882  acc: 62.5000%(10/16)\n",
            "Batch[21] - loss: 3.385719  acc: 37.5000%(6/16)\n",
            "Batch[22] - loss: 2.974198  acc: 68.7500%(11/16)\n",
            "Batch[23] - loss: 3.144465  acc: 68.7500%(11/16)\n",
            "Batch[24] - loss: 2.953124  acc: 62.5000%(10/16)\n",
            "Batch[25] - loss: 2.567081  acc: 62.5000%(10/16)\n",
            "Batch[26] - loss: 3.173903  acc: 56.2500%(9/16)\n",
            "Batch[27] - loss: 2.704500  acc: 62.5000%(10/16)\n",
            "Batch[28] - loss: 3.303102  acc: 56.2500%(9/16)\n",
            "Batch[29] - loss: 3.526643  acc: 62.5000%(10/16)\n",
            "Batch[30] - loss: 2.958802  acc: 62.5000%(10/16)\n",
            "Batch[31] - loss: 3.232702  acc: 56.2500%(9/16)\n",
            "Batch[32] - loss: 2.939642  acc: 25.0000%(4/16)\n",
            "Batch[33] - loss: 2.967979  acc: 56.2500%(9/16)\n",
            "Batch[34] - loss: 3.198575  acc: 50.0000%(4/8)\n",
            "Average loss:3.086331 average acc:53.392857%\n",
            "Val set acc: 0.5975609756097561\n",
            "Best val set acc: 0\n",
            "\n",
            "Epoch  3 / 18\n",
            "Batch[0] - loss: 2.589305  acc: 62.5000%(10/16)\n",
            "Batch[1] - loss: 2.924397  acc: 75.0000%(12/16)\n",
            "Batch[2] - loss: 2.543240  acc: 81.2500%(13/16)\n",
            "Batch[3] - loss: 2.690139  acc: 68.7500%(11/16)\n",
            "Batch[4] - loss: 2.990222  acc: 62.5000%(10/16)\n",
            "Batch[5] - loss: 2.671062  acc: 81.2500%(13/16)\n",
            "Batch[6] - loss: 2.706077  acc: 56.2500%(9/16)\n",
            "Batch[7] - loss: 2.623801  acc: 75.0000%(12/16)\n",
            "Batch[8] - loss: 2.498713  acc: 75.0000%(12/16)\n",
            "Batch[9] - loss: 2.605472  acc: 75.0000%(12/16)\n",
            "Batch[10] - loss: 2.660414  acc: 68.7500%(11/16)\n",
            "Batch[11] - loss: 2.863418  acc: 50.0000%(8/16)\n",
            "Batch[12] - loss: 2.458927  acc: 81.2500%(13/16)\n",
            "Batch[13] - loss: 2.834616  acc: 50.0000%(8/16)\n",
            "Batch[14] - loss: 2.426080  acc: 56.2500%(9/16)\n",
            "Batch[15] - loss: 2.649558  acc: 93.7500%(15/16)\n",
            "Batch[16] - loss: 2.686926  acc: 75.0000%(12/16)\n",
            "Batch[17] - loss: 2.454932  acc: 68.7500%(11/16)\n",
            "Batch[18] - loss: 2.276394  acc: 81.2500%(13/16)\n",
            "Batch[19] - loss: 2.436273  acc: 75.0000%(12/16)\n",
            "Batch[20] - loss: 2.485055  acc: 75.0000%(12/16)\n",
            "Batch[21] - loss: 2.338044  acc: 81.2500%(13/16)\n",
            "Batch[22] - loss: 2.614141  acc: 93.7500%(15/16)\n",
            "Batch[23] - loss: 2.775276  acc: 62.5000%(10/16)\n",
            "Batch[24] - loss: 1.903256  acc: 75.0000%(12/16)\n",
            "Batch[25] - loss: 2.652517  acc: 81.2500%(13/16)\n",
            "Batch[26] - loss: 2.557918  acc: 81.2500%(13/16)\n",
            "Batch[27] - loss: 2.455775  acc: 81.2500%(13/16)\n",
            "Batch[28] - loss: 2.199536  acc: 75.0000%(12/16)\n",
            "Batch[29] - loss: 2.781930  acc: 87.5000%(14/16)\n",
            "Batch[30] - loss: 2.206400  acc: 75.0000%(12/16)\n",
            "Batch[31] - loss: 2.442034  acc: 81.2500%(13/16)\n",
            "Batch[32] - loss: 1.936558  acc: 93.7500%(15/16)\n",
            "Batch[33] - loss: 2.310103  acc: 87.5000%(14/16)\n",
            "Batch[34] - loss: 2.859072  acc: 50.0000%(4/8)\n",
            "Average loss:2.545931 average acc:74.107140%\n",
            "Val set acc: 0.6463414634146342\n",
            "Best val set acc: 0\n",
            "\n",
            "Epoch  4 / 18\n",
            "Batch[0] - loss: 2.010308  acc: 93.7500%(15/16)\n",
            "Batch[1] - loss: 2.147218  acc: 81.2500%(13/16)\n",
            "Batch[2] - loss: 2.020774  acc: 81.2500%(13/16)\n",
            "Batch[3] - loss: 1.847969  acc: 93.7500%(15/16)\n",
            "Batch[4] - loss: 1.904706  acc: 81.2500%(13/16)\n",
            "Batch[5] - loss: 1.944414  acc: 75.0000%(12/16)\n",
            "Batch[6] - loss: 1.845843  acc: 100.0000%(16/16)\n",
            "Batch[7] - loss: 1.506900  acc: 100.0000%(16/16)\n",
            "Batch[8] - loss: 1.908779  acc: 93.7500%(15/16)\n",
            "Batch[9] - loss: 1.897483  acc: 87.5000%(14/16)\n",
            "Batch[10] - loss: 2.082224  acc: 93.7500%(15/16)\n",
            "Batch[11] - loss: 1.990054  acc: 100.0000%(16/16)\n",
            "Batch[12] - loss: 1.855022  acc: 100.0000%(16/16)\n",
            "Batch[13] - loss: 1.927344  acc: 87.5000%(14/16)\n",
            "Batch[14] - loss: 1.898203  acc: 81.2500%(13/16)\n",
            "Batch[15] - loss: 1.971653  acc: 81.2500%(13/16)\n",
            "Batch[16] - loss: 1.602183  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 1.881827  acc: 75.0000%(12/16)\n",
            "Batch[18] - loss: 1.309865  acc: 93.7500%(15/16)\n",
            "Batch[19] - loss: 1.722751  acc: 93.7500%(15/16)\n",
            "Batch[20] - loss: 1.384154  acc: 93.7500%(15/16)\n",
            "Batch[21] - loss: 1.579629  acc: 87.5000%(14/16)\n",
            "Batch[22] - loss: 1.850050  acc: 81.2500%(13/16)\n",
            "Batch[23] - loss: 1.561603  acc: 93.7500%(15/16)\n",
            "Batch[24] - loss: 1.919047  acc: 93.7500%(15/16)\n",
            "Batch[25] - loss: 1.640325  acc: 87.5000%(14/16)\n",
            "Batch[26] - loss: 1.407216  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 1.423571  acc: 75.0000%(12/16)\n",
            "Batch[28] - loss: 1.573392  acc: 100.0000%(16/16)\n",
            "Batch[29] - loss: 1.589961  acc: 93.7500%(15/16)\n",
            "Batch[30] - loss: 1.405969  acc: 93.7500%(15/16)\n",
            "Batch[31] - loss: 1.584299  acc: 87.5000%(14/16)\n",
            "Batch[32] - loss: 1.604688  acc: 81.2500%(13/16)\n",
            "Batch[33] - loss: 1.123193  acc: 87.5000%(14/16)\n",
            "Batch[34] - loss: 0.700091  acc: 100.0000%(8/8)\n",
            "Average loss:1.703506 average acc:90.000000%\n",
            "Val set acc: 0.7439024390243902\n",
            "Best val set acc: 0\n",
            "\n",
            "Epoch  5 / 18\n",
            "Batch[0] - loss: 1.101409  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 1.105056  acc: 93.7500%(15/16)\n",
            "Batch[2] - loss: 0.874556  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.996891  acc: 100.0000%(16/16)\n",
            "Batch[4] - loss: 1.473032  acc: 100.0000%(16/16)\n",
            "Batch[5] - loss: 0.852090  acc: 100.0000%(16/16)\n",
            "Batch[6] - loss: 1.277057  acc: 87.5000%(14/16)\n",
            "Batch[7] - loss: 1.252030  acc: 100.0000%(16/16)\n",
            "Batch[8] - loss: 1.192828  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 1.167814  acc: 93.7500%(15/16)\n",
            "Batch[10] - loss: 1.124741  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 1.149099  acc: 93.7500%(15/16)\n",
            "Batch[12] - loss: 0.785109  acc: 100.0000%(16/16)\n",
            "Batch[13] - loss: 1.139794  acc: 87.5000%(14/16)\n",
            "Batch[14] - loss: 1.462877  acc: 93.7500%(15/16)\n",
            "Batch[15] - loss: 1.555554  acc: 87.5000%(14/16)\n",
            "Batch[16] - loss: 1.094480  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 0.903300  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.952665  acc: 100.0000%(16/16)\n",
            "Batch[19] - loss: 1.074589  acc: 93.7500%(15/16)\n",
            "Batch[20] - loss: 0.856099  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 1.052614  acc: 93.7500%(15/16)\n",
            "Batch[22] - loss: 1.363214  acc: 93.7500%(15/16)\n",
            "Batch[23] - loss: 0.651363  acc: 93.7500%(15/16)\n",
            "Batch[24] - loss: 1.044788  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 1.152381  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.758964  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 0.867082  acc: 93.7500%(15/16)\n",
            "Batch[28] - loss: 0.593808  acc: 100.0000%(16/16)\n",
            "Batch[29] - loss: 1.277959  acc: 87.5000%(14/16)\n",
            "Batch[30] - loss: 0.758705  acc: 100.0000%(16/16)\n",
            "Batch[31] - loss: 0.841804  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.743059  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 0.899767  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.476872  acc: 87.5000%(7/8)\n",
            "Average loss:1.024956 average acc:96.607140%\n",
            "Val set acc: 0.8048780487804879\n",
            "Best val set acc: 0\n",
            "\n",
            "Epoch  6 / 18\n",
            "Batch[0] - loss: 0.927625  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 0.678978  acc: 100.0000%(16/16)\n",
            "Batch[2] - loss: 0.448946  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.977626  acc: 93.7500%(15/16)\n",
            "Batch[4] - loss: 0.817615  acc: 100.0000%(16/16)\n",
            "Batch[5] - loss: 0.471724  acc: 100.0000%(16/16)\n",
            "Batch[6] - loss: 0.769556  acc: 100.0000%(16/16)\n",
            "Batch[7] - loss: 0.592960  acc: 93.7500%(15/16)\n",
            "Batch[8] - loss: 0.718218  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.859982  acc: 100.0000%(16/16)\n",
            "Batch[10] - loss: 0.398491  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.495583  acc: 100.0000%(16/16)\n",
            "Batch[12] - loss: 0.415730  acc: 100.0000%(16/16)\n",
            "Batch[13] - loss: 0.526847  acc: 100.0000%(16/16)\n",
            "Batch[14] - loss: 0.838054  acc: 100.0000%(16/16)\n",
            "Batch[15] - loss: 0.434394  acc: 93.7500%(15/16)\n",
            "Batch[16] - loss: 0.763013  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 0.477358  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.582339  acc: 100.0000%(16/16)\n",
            "Batch[19] - loss: 0.605214  acc: 100.0000%(16/16)\n",
            "Batch[20] - loss: 0.431435  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 0.588433  acc: 100.0000%(16/16)\n",
            "Batch[22] - loss: 0.294130  acc: 100.0000%(16/16)\n",
            "Batch[23] - loss: 0.536131  acc: 87.5000%(14/16)\n",
            "Batch[24] - loss: 0.604030  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.443302  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.455962  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 0.724157  acc: 93.7500%(15/16)\n",
            "Batch[28] - loss: 0.279338  acc: 100.0000%(16/16)\n",
            "Batch[29] - loss: 0.676954  acc: 93.7500%(15/16)\n",
            "Batch[30] - loss: 0.934338  acc: 93.7500%(15/16)\n",
            "Batch[31] - loss: 0.382262  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.293317  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 0.452643  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.551362  acc: 100.0000%(8/8)\n",
            "Average loss:0.584230 average acc:98.571426%\n",
            "Val set acc: 0.7682926829268293\n",
            "Best val set acc: 0\n",
            "\n",
            "Epoch  7 / 18\n",
            "Batch[0] - loss: 0.284373  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 0.431099  acc: 100.0000%(16/16)\n",
            "Batch[2] - loss: 0.463715  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.371644  acc: 100.0000%(16/16)\n",
            "Batch[4] - loss: 0.361724  acc: 100.0000%(16/16)\n",
            "Batch[5] - loss: 0.430734  acc: 100.0000%(16/16)\n",
            "Batch[6] - loss: 0.449407  acc: 100.0000%(16/16)\n",
            "Batch[7] - loss: 0.251586  acc: 100.0000%(16/16)\n",
            "Batch[8] - loss: 0.559438  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.151882  acc: 100.0000%(16/16)\n",
            "Batch[10] - loss: 0.409520  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.416078  acc: 93.7500%(15/16)\n",
            "Batch[12] - loss: 0.448445  acc: 100.0000%(16/16)\n",
            "Batch[13] - loss: 0.213634  acc: 100.0000%(16/16)\n",
            "Batch[14] - loss: 0.463553  acc: 100.0000%(16/16)\n",
            "Batch[15] - loss: 0.218918  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.254378  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 0.290360  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.313424  acc: 100.0000%(16/16)\n",
            "Batch[19] - loss: 0.357279  acc: 100.0000%(16/16)\n",
            "Batch[20] - loss: 0.476518  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 0.316935  acc: 100.0000%(16/16)\n",
            "Batch[22] - loss: 0.301182  acc: 100.0000%(16/16)\n",
            "Batch[23] - loss: 0.288602  acc: 100.0000%(16/16)\n",
            "Batch[24] - loss: 0.348237  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.411611  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.344645  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 0.697712  acc: 93.7500%(15/16)\n",
            "Batch[28] - loss: 0.352833  acc: 100.0000%(16/16)\n",
            "Batch[29] - loss: 0.255081  acc: 100.0000%(16/16)\n",
            "Batch[30] - loss: 0.349187  acc: 100.0000%(16/16)\n",
            "Batch[31] - loss: 0.172156  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.299993  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 0.203958  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.281912  acc: 100.0000%(8/8)\n",
            "Average loss:0.349764 average acc:99.642860%\n",
            "Val set acc: 0.7439024390243902\n",
            "Best val set acc: 0\n",
            "\n",
            "Epoch  8 / 18\n",
            "Batch[0] - loss: 0.160435  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 0.419911  acc: 93.7500%(15/16)\n",
            "Batch[2] - loss: 0.428408  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.409129  acc: 93.7500%(15/16)\n",
            "Batch[4] - loss: 0.399674  acc: 100.0000%(16/16)\n",
            "Batch[5] - loss: 0.100310  acc: 100.0000%(16/16)\n",
            "Batch[6] - loss: 0.252645  acc: 100.0000%(16/16)\n",
            "Batch[7] - loss: 0.245752  acc: 100.0000%(16/16)\n",
            "Batch[8] - loss: 0.269315  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.102604  acc: 100.0000%(16/16)\n",
            "Batch[10] - loss: 0.100480  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.130575  acc: 100.0000%(16/16)\n",
            "Batch[12] - loss: 0.126228  acc: 100.0000%(16/16)\n",
            "Batch[13] - loss: 0.251952  acc: 100.0000%(16/16)\n",
            "Batch[14] - loss: 0.425969  acc: 93.7500%(15/16)\n",
            "Batch[15] - loss: 0.320219  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.146792  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 0.213765  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.141990  acc: 100.0000%(16/16)\n",
            "Batch[19] - loss: 0.168675  acc: 100.0000%(16/16)\n",
            "Batch[20] - loss: 0.181879  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 0.241598  acc: 100.0000%(16/16)\n",
            "Batch[22] - loss: 0.368446  acc: 100.0000%(16/16)\n",
            "Batch[23] - loss: 0.284290  acc: 100.0000%(16/16)\n",
            "Batch[24] - loss: 0.204439  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.233917  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.176788  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 0.373206  acc: 100.0000%(16/16)\n",
            "Batch[28] - loss: 0.266341  acc: 93.7500%(15/16)\n",
            "Batch[29] - loss: 0.175849  acc: 100.0000%(16/16)\n",
            "Batch[30] - loss: 0.151748  acc: 100.0000%(16/16)\n",
            "Batch[31] - loss: 0.142618  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.493677  acc: 93.7500%(15/16)\n",
            "Batch[33] - loss: 0.096430  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.116383  acc: 100.0000%(8/8)\n",
            "Average loss:0.237784 average acc:99.107147%\n",
            "Val set acc: 0.8048780487804879\n",
            "Best val set acc: 0\n",
            "\n",
            "Epoch  9 / 18\n",
            "Batch[0] - loss: 0.046073  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 0.055117  acc: 100.0000%(16/16)\n",
            "Batch[2] - loss: 0.095020  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.146621  acc: 100.0000%(16/16)\n",
            "Batch[4] - loss: 0.222328  acc: 100.0000%(16/16)\n",
            "Batch[5] - loss: 0.067049  acc: 100.0000%(16/16)\n",
            "Batch[6] - loss: 0.074063  acc: 100.0000%(16/16)\n",
            "Batch[7] - loss: 0.194677  acc: 100.0000%(16/16)\n",
            "Batch[8] - loss: 0.296175  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.134989  acc: 100.0000%(16/16)\n",
            "Batch[10] - loss: 0.073434  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.214688  acc: 93.7500%(15/16)\n",
            "Batch[12] - loss: 0.100220  acc: 100.0000%(16/16)\n",
            "Batch[13] - loss: 0.162785  acc: 100.0000%(16/16)\n",
            "Batch[14] - loss: 0.046456  acc: 100.0000%(16/16)\n",
            "Batch[15] - loss: 0.058406  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.072795  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 0.148188  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.140047  acc: 100.0000%(16/16)\n",
            "Batch[19] - loss: 0.138964  acc: 93.7500%(15/16)\n",
            "Batch[20] - loss: 0.113968  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 0.072344  acc: 100.0000%(16/16)\n",
            "Batch[22] - loss: 0.144506  acc: 100.0000%(16/16)\n",
            "Batch[23] - loss: 0.095448  acc: 100.0000%(16/16)\n",
            "Batch[24] - loss: 0.058280  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.274618  acc: 93.7500%(15/16)\n",
            "Batch[26] - loss: 0.117368  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 0.050455  acc: 100.0000%(16/16)\n",
            "Batch[28] - loss: 0.304577  acc: 100.0000%(16/16)\n",
            "Batch[29] - loss: 0.120618  acc: 100.0000%(16/16)\n",
            "Batch[30] - loss: 0.148085  acc: 93.7500%(15/16)\n",
            "Batch[31] - loss: 0.128059  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.238063  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 0.170267  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.052077  acc: 100.0000%(8/8)\n",
            "Average loss:0.130766 average acc:99.285713%\n",
            "Val set acc: 0.7804878048780488\n",
            "Best val set acc: 0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          NR    0.73913   0.85000   0.79070        20\n",
            "          FR    0.77778   0.66667   0.71795        21\n",
            "          TR    0.77273   0.85000   0.80952        20\n",
            "          UR    0.84211   0.76190   0.80000        21\n",
            "\n",
            "    accuracy                        0.78049        82\n",
            "   macro avg    0.78294   0.78214   0.77954        82\n",
            "weighted avg    0.78359   0.78049   0.77904        82\n",
            "\n",
            "save model!!!\n",
            "\n",
            "Epoch  10 / 18\n",
            "Batch[0] - loss: 0.109880  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 0.076366  acc: 100.0000%(16/16)\n",
            "Batch[2] - loss: 0.030633  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.075716  acc: 100.0000%(16/16)\n",
            "Batch[4] - loss: 0.164891  acc: 100.0000%(16/16)\n",
            "Batch[5] - loss: 0.142439  acc: 100.0000%(16/16)\n",
            "Batch[6] - loss: 0.088481  acc: 100.0000%(16/16)\n",
            "Batch[7] - loss: 0.049001  acc: 100.0000%(16/16)\n",
            "Batch[8] - loss: 0.083622  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.169656  acc: 100.0000%(16/16)\n",
            "Batch[10] - loss: 0.094514  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.101739  acc: 100.0000%(16/16)\n",
            "Batch[12] - loss: 0.045576  acc: 100.0000%(16/16)\n",
            "Batch[13] - loss: 0.072190  acc: 100.0000%(16/16)\n",
            "Batch[14] - loss: 0.102614  acc: 100.0000%(16/16)\n",
            "Batch[15] - loss: 0.047984  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.058355  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 0.058528  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.178014  acc: 100.0000%(16/16)\n",
            "Batch[19] - loss: 0.053516  acc: 100.0000%(16/16)\n",
            "Batch[20] - loss: 0.048142  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 0.089823  acc: 100.0000%(16/16)\n",
            "Batch[22] - loss: 0.097245  acc: 100.0000%(16/16)\n",
            "Batch[23] - loss: 0.065061  acc: 100.0000%(16/16)\n",
            "Batch[24] - loss: 0.057115  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.059379  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.049791  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 0.108355  acc: 100.0000%(16/16)\n",
            "Batch[28] - loss: 0.032999  acc: 100.0000%(16/16)\n",
            "Batch[29] - loss: 0.086974  acc: 100.0000%(16/16)\n",
            "Batch[30] - loss: 0.064826  acc: 100.0000%(16/16)\n",
            "Batch[31] - loss: 0.053549  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.216753  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 0.092499  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.055475  acc: 100.0000%(8/8)\n",
            "Average loss:0.085191 average acc:100.000000%\n",
            "Val set acc: 0.8048780487804879\n",
            "Best val set acc: 0.7804878048780488\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          NR    0.75000   0.90000   0.81818        20\n",
            "          FR    0.77778   0.66667   0.71795        21\n",
            "          TR    0.75000   0.90000   0.81818        20\n",
            "          UR    1.00000   0.76190   0.86486        21\n",
            "\n",
            "    accuracy                        0.80488        82\n",
            "   macro avg    0.81944   0.80714   0.80479        82\n",
            "weighted avg    0.82114   0.80488   0.80447        82\n",
            "\n",
            "save model!!!\n",
            "\n",
            "Epoch  11 / 18\n",
            "Batch[0] - loss: 0.043331  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 0.091489  acc: 100.0000%(16/16)\n",
            "Batch[2] - loss: 0.030761  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.036967  acc: 100.0000%(16/16)\n",
            "Batch[4] - loss: 0.033242  acc: 100.0000%(16/16)\n",
            "Batch[5] - loss: 0.035887  acc: 100.0000%(16/16)\n",
            "Batch[6] - loss: 0.046600  acc: 100.0000%(16/16)\n",
            "Batch[7] - loss: 0.040667  acc: 100.0000%(16/16)\n",
            "Batch[8] - loss: 0.234412  acc: 93.7500%(15/16)\n",
            "Batch[9] - loss: 0.035250  acc: 100.0000%(16/16)\n",
            "Batch[10] - loss: 0.050498  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.042389  acc: 100.0000%(16/16)\n",
            "Batch[12] - loss: 0.021082  acc: 100.0000%(16/16)\n",
            "Batch[13] - loss: 0.062794  acc: 100.0000%(16/16)\n",
            "Batch[14] - loss: 0.035499  acc: 100.0000%(16/16)\n",
            "Batch[15] - loss: 0.062901  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.026834  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 0.026347  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.034624  acc: 100.0000%(16/16)\n",
            "Batch[19] - loss: 0.054304  acc: 100.0000%(16/16)\n",
            "Batch[20] - loss: 0.028753  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 0.022991  acc: 100.0000%(16/16)\n",
            "Batch[22] - loss: 0.041542  acc: 100.0000%(16/16)\n",
            "Batch[23] - loss: 0.030402  acc: 100.0000%(16/16)\n",
            "Batch[24] - loss: 0.057048  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.062162  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.036552  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 0.073422  acc: 100.0000%(16/16)\n",
            "Batch[28] - loss: 0.089379  acc: 93.7500%(15/16)\n",
            "Batch[29] - loss: 0.079496  acc: 100.0000%(16/16)\n",
            "Batch[30] - loss: 0.165608  acc: 100.0000%(16/16)\n",
            "Batch[31] - loss: 0.044830  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.036206  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 0.039155  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.110750  acc: 100.0000%(8/8)\n",
            "Average loss:0.056119 average acc:99.642860%\n",
            "Val set acc: 0.8170731707317073\n",
            "Best val set acc: 0.8048780487804879\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          NR    0.80952   0.85000   0.82927        20\n",
            "          FR    0.76190   0.76190   0.76190        21\n",
            "          TR    0.79167   0.95000   0.86364        20\n",
            "          UR    0.93750   0.71429   0.81081        21\n",
            "\n",
            "    accuracy                        0.81707        82\n",
            "   macro avg    0.82515   0.81905   0.81641        82\n",
            "weighted avg    0.82575   0.81707   0.81567        82\n",
            "\n",
            "save model!!!\n",
            "\n",
            "Epoch  12 / 18\n",
            "Batch[0] - loss: 0.037093  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 0.030908  acc: 100.0000%(16/16)\n",
            "Batch[2] - loss: 0.043063  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.046471  acc: 100.0000%(16/16)\n",
            "Batch[4] - loss: 0.020080  acc: 100.0000%(16/16)\n",
            "Batch[5] - loss: 0.054686  acc: 100.0000%(16/16)\n",
            "Batch[6] - loss: 0.023779  acc: 100.0000%(16/16)\n",
            "Batch[7] - loss: 0.038529  acc: 100.0000%(16/16)\n",
            "Batch[8] - loss: 0.038222  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.025835  acc: 100.0000%(16/16)\n",
            "Batch[10] - loss: 0.029517  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.020086  acc: 100.0000%(16/16)\n",
            "Batch[12] - loss: 0.015463  acc: 100.0000%(16/16)\n",
            "Batch[13] - loss: 0.018858  acc: 100.0000%(16/16)\n",
            "Batch[14] - loss: 0.019178  acc: 100.0000%(16/16)\n",
            "Batch[15] - loss: 0.042366  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.030847  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 0.055315  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.049020  acc: 100.0000%(16/16)\n",
            "Batch[19] - loss: 0.027490  acc: 100.0000%(16/16)\n",
            "Batch[20] - loss: 0.060091  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 0.040621  acc: 100.0000%(16/16)\n",
            "Batch[22] - loss: 0.038306  acc: 100.0000%(16/16)\n",
            "Batch[23] - loss: 0.016004  acc: 100.0000%(16/16)\n",
            "Batch[24] - loss: 0.028055  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.026812  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.056591  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 0.050983  acc: 100.0000%(16/16)\n",
            "Batch[28] - loss: 0.049923  acc: 100.0000%(16/16)\n",
            "Batch[29] - loss: 0.036850  acc: 100.0000%(16/16)\n",
            "Batch[30] - loss: 0.042452  acc: 100.0000%(16/16)\n",
            "Batch[31] - loss: 0.035773  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.069667  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 0.030962  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.041024  acc: 100.0000%(8/8)\n",
            "Average loss:0.036883 average acc:100.000000%\n",
            "Val set acc: 0.7926829268292683\n",
            "Best val set acc: 0.8170731707317073\n",
            "\n",
            "Epoch  13 / 18\n",
            "Batch[0] - loss: 0.032409  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 0.024588  acc: 100.0000%(16/16)\n",
            "Batch[2] - loss: 0.038633  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.035768  acc: 100.0000%(16/16)\n",
            "Batch[4] - loss: 0.016161  acc: 100.0000%(16/16)\n",
            "Batch[5] - loss: 0.015961  acc: 100.0000%(16/16)\n",
            "Batch[6] - loss: 0.008573  acc: 100.0000%(16/16)\n",
            "Batch[7] - loss: 0.029722  acc: 100.0000%(16/16)\n",
            "Batch[8] - loss: 0.070615  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.022843  acc: 100.0000%(16/16)\n",
            "Batch[10] - loss: 0.029299  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.059818  acc: 100.0000%(16/16)\n",
            "Batch[12] - loss: 0.041312  acc: 100.0000%(16/16)\n",
            "Batch[13] - loss: 0.009568  acc: 100.0000%(16/16)\n",
            "Batch[14] - loss: 0.053679  acc: 100.0000%(16/16)\n",
            "Batch[15] - loss: 0.019774  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.042836  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 0.015841  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.108608  acc: 93.7500%(15/16)\n",
            "Batch[19] - loss: 0.015173  acc: 100.0000%(16/16)\n",
            "Batch[20] - loss: 0.036969  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 0.013873  acc: 100.0000%(16/16)\n",
            "Batch[22] - loss: 0.134721  acc: 93.7500%(15/16)\n",
            "Batch[23] - loss: 0.031838  acc: 100.0000%(16/16)\n",
            "Batch[24] - loss: 0.025361  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.039671  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.028484  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 0.017061  acc: 100.0000%(16/16)\n",
            "Batch[28] - loss: 0.018369  acc: 100.0000%(16/16)\n",
            "Batch[29] - loss: 0.010254  acc: 100.0000%(16/16)\n",
            "Batch[30] - loss: 0.029238  acc: 100.0000%(16/16)\n",
            "Batch[31] - loss: 0.036098  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.027215  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 0.064660  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.027835  acc: 100.0000%(8/8)\n",
            "Average loss:0.035224 average acc:99.642860%\n",
            "Val set acc: 0.8292682926829268\n",
            "Best val set acc: 0.8170731707317073\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          NR    0.77273   0.85000   0.80952        20\n",
            "          FR    0.81818   0.85714   0.83721        21\n",
            "          TR    0.89474   0.85000   0.87179        20\n",
            "          UR    0.84211   0.76190   0.80000        21\n",
            "\n",
            "    accuracy                        0.82927        82\n",
            "   macro avg    0.83194   0.82976   0.82963        82\n",
            "weighted avg    0.83189   0.82927   0.82936        82\n",
            "\n",
            "save model!!!\n",
            "\n",
            "Epoch  14 / 18\n",
            "Batch[0] - loss: 0.024297  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 0.045792  acc: 100.0000%(16/16)\n",
            "Batch[2] - loss: 0.014977  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.042066  acc: 100.0000%(16/16)\n",
            "Batch[4] - loss: 0.020481  acc: 100.0000%(16/16)\n",
            "Batch[5] - loss: 0.016561  acc: 100.0000%(16/16)\n",
            "Batch[6] - loss: 0.020222  acc: 100.0000%(16/16)\n",
            "Batch[7] - loss: 0.048402  acc: 100.0000%(16/16)\n",
            "Batch[8] - loss: 0.063448  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.017117  acc: 100.0000%(16/16)\n",
            "Batch[10] - loss: 0.013206  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.016426  acc: 100.0000%(16/16)\n",
            "Batch[12] - loss: 0.018056  acc: 100.0000%(16/16)\n",
            "Batch[13] - loss: 0.066511  acc: 100.0000%(16/16)\n",
            "Batch[14] - loss: 0.013405  acc: 100.0000%(16/16)\n",
            "Batch[15] - loss: 0.016959  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.024409  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 0.017268  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.028323  acc: 100.0000%(16/16)\n",
            "Batch[19] - loss: 0.014189  acc: 100.0000%(16/16)\n",
            "Batch[20] - loss: 0.041161  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 0.031215  acc: 100.0000%(16/16)\n",
            "Batch[22] - loss: 0.008204  acc: 100.0000%(16/16)\n",
            "Batch[23] - loss: 0.030275  acc: 100.0000%(16/16)\n",
            "Batch[24] - loss: 0.017468  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.016773  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.014649  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 0.063890  acc: 100.0000%(16/16)\n",
            "Batch[28] - loss: 0.018234  acc: 100.0000%(16/16)\n",
            "Batch[29] - loss: 0.023165  acc: 100.0000%(16/16)\n",
            "Batch[30] - loss: 0.011450  acc: 100.0000%(16/16)\n",
            "Batch[31] - loss: 0.013011  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.021923  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 0.032350  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.046898  acc: 100.0000%(8/8)\n",
            "Average loss:0.026651 average acc:100.000000%\n",
            "Val set acc: 0.8292682926829268\n",
            "Best val set acc: 0.8292682926829268\n",
            "\n",
            "Epoch  15 / 18\n",
            "Batch[0] - loss: 0.048939  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 0.022225  acc: 100.0000%(16/16)\n",
            "Batch[2] - loss: 0.036918  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.040935  acc: 100.0000%(16/16)\n",
            "Batch[4] - loss: 0.006462  acc: 100.0000%(16/16)\n",
            "Batch[5] - loss: 0.006420  acc: 100.0000%(16/16)\n",
            "Batch[6] - loss: 0.025996  acc: 100.0000%(16/16)\n",
            "Batch[7] - loss: 0.011733  acc: 100.0000%(16/16)\n",
            "Batch[8] - loss: 0.032604  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.021893  acc: 100.0000%(16/16)\n",
            "Batch[10] - loss: 0.013605  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.008191  acc: 100.0000%(16/16)\n",
            "Batch[12] - loss: 0.010662  acc: 100.0000%(16/16)\n",
            "Batch[13] - loss: 0.029280  acc: 100.0000%(16/16)\n",
            "Batch[14] - loss: 0.038480  acc: 100.0000%(16/16)\n",
            "Batch[15] - loss: 0.025983  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.021652  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 0.021340  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.014759  acc: 100.0000%(16/16)\n",
            "Batch[19] - loss: 0.010681  acc: 100.0000%(16/16)\n",
            "Batch[20] - loss: 0.018108  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 0.015745  acc: 100.0000%(16/16)\n",
            "Batch[22] - loss: 0.019583  acc: 100.0000%(16/16)\n",
            "Batch[23] - loss: 0.026107  acc: 100.0000%(16/16)\n",
            "Batch[24] - loss: 0.012676  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.016227  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.034727  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 0.017993  acc: 100.0000%(16/16)\n",
            "Batch[28] - loss: 0.019261  acc: 100.0000%(16/16)\n",
            "Batch[29] - loss: 0.006306  acc: 100.0000%(16/16)\n",
            "Batch[30] - loss: 0.006864  acc: 100.0000%(16/16)\n",
            "Batch[31] - loss: 0.009293  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.007770  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 0.022490  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.017369  acc: 100.0000%(8/8)\n",
            "Average loss:0.019979 average acc:100.000000%\n",
            "Val set acc: 0.8414634146341463\n",
            "Best val set acc: 0.8292682926829268\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          NR    0.78261   0.90000   0.83721        20\n",
            "          FR    0.83333   0.71429   0.76923        21\n",
            "          TR    0.79167   0.95000   0.86364        20\n",
            "          UR    1.00000   0.80952   0.89474        21\n",
            "\n",
            "    accuracy                        0.84146        82\n",
            "   macro avg    0.85190   0.84345   0.84120        82\n",
            "weighted avg    0.85348   0.84146   0.84098        82\n",
            "\n",
            "save model!!!\n",
            "\n",
            "Epoch  16 / 18\n",
            "Batch[0] - loss: 0.029403  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 0.028921  acc: 100.0000%(16/16)\n",
            "Batch[2] - loss: 0.009895  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.007744  acc: 100.0000%(16/16)\n",
            "Batch[4] - loss: 0.018449  acc: 100.0000%(16/16)\n",
            "Batch[5] - loss: 0.017610  acc: 100.0000%(16/16)\n",
            "Batch[6] - loss: 0.027402  acc: 100.0000%(16/16)\n",
            "Batch[7] - loss: 0.009702  acc: 100.0000%(16/16)\n",
            "Batch[8] - loss: 0.021227  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.013329  acc: 100.0000%(16/16)\n",
            "Batch[10] - loss: 0.010322  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.009937  acc: 100.0000%(16/16)\n",
            "Batch[12] - loss: 0.005462  acc: 100.0000%(16/16)\n",
            "Batch[13] - loss: 0.015879  acc: 100.0000%(16/16)\n",
            "Batch[14] - loss: 0.008549  acc: 100.0000%(16/16)\n",
            "Batch[15] - loss: 0.011966  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.012879  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 0.038923  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.008591  acc: 100.0000%(16/16)\n",
            "Batch[19] - loss: 0.012113  acc: 100.0000%(16/16)\n",
            "Batch[20] - loss: 0.002900  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 0.009384  acc: 100.0000%(16/16)\n",
            "Batch[22] - loss: 0.039124  acc: 100.0000%(16/16)\n",
            "Batch[23] - loss: 0.014011  acc: 100.0000%(16/16)\n",
            "Batch[24] - loss: 0.022246  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.006291  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.016963  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 0.012015  acc: 100.0000%(16/16)\n",
            "Batch[28] - loss: 0.023287  acc: 100.0000%(16/16)\n",
            "Batch[29] - loss: 0.017689  acc: 100.0000%(16/16)\n",
            "Batch[30] - loss: 0.019736  acc: 100.0000%(16/16)\n",
            "Batch[31] - loss: 0.017148  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.010867  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 0.039143  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.021360  acc: 100.0000%(8/8)\n",
            "Average loss:0.016871 average acc:100.000000%\n",
            "Val set acc: 0.8170731707317073\n",
            "Best val set acc: 0.8414634146341463\n",
            "\n",
            "Epoch  17 / 18\n",
            "Batch[0] - loss: 0.017406  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 0.023890  acc: 100.0000%(16/16)\n",
            "Batch[2] - loss: 0.008934  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.010787  acc: 100.0000%(16/16)\n",
            "Batch[4] - loss: 0.032199  acc: 100.0000%(16/16)\n",
            "Batch[5] - loss: 0.012280  acc: 100.0000%(16/16)\n",
            "Batch[6] - loss: 0.014489  acc: 100.0000%(16/16)\n",
            "Batch[7] - loss: 0.014405  acc: 100.0000%(16/16)\n",
            "Batch[8] - loss: 0.009355  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.041608  acc: 100.0000%(16/16)\n",
            "Batch[10] - loss: 0.004709  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.015537  acc: 100.0000%(16/16)\n",
            "Batch[12] - loss: 0.007347  acc: 100.0000%(16/16)\n",
            "Batch[13] - loss: 0.011286  acc: 100.0000%(16/16)\n",
            "Batch[14] - loss: 0.010742  acc: 100.0000%(16/16)\n",
            "Batch[15] - loss: 0.008101  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.012985  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 0.017957  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.009952  acc: 100.0000%(16/16)\n",
            "Batch[19] - loss: 0.003183  acc: 100.0000%(16/16)\n",
            "Batch[20] - loss: 0.013777  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 0.017780  acc: 100.0000%(16/16)\n",
            "Batch[22] - loss: 0.016804  acc: 100.0000%(16/16)\n",
            "Batch[23] - loss: 0.016748  acc: 100.0000%(16/16)\n",
            "Batch[24] - loss: 0.017182  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.019917  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.004439  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 0.011581  acc: 100.0000%(16/16)\n",
            "Batch[28] - loss: 0.006644  acc: 100.0000%(16/16)\n",
            "Batch[29] - loss: 0.009259  acc: 100.0000%(16/16)\n",
            "Batch[30] - loss: 0.004762  acc: 100.0000%(16/16)\n",
            "Batch[31] - loss: 0.026733  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.002675  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 0.005546  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.006609  acc: 100.0000%(8/8)\n",
            "Average loss:0.013360 average acc:100.000000%\n",
            "Val set acc: 0.8170731707317073\n",
            "Best val set acc: 0.8414634146341463\n",
            "\n",
            "Epoch  18 / 18\n",
            "Batch[0] - loss: 0.010827  acc: 100.0000%(16/16)\n",
            "Batch[1] - loss: 0.025442  acc: 100.0000%(16/16)\n",
            "Batch[2] - loss: 0.013852  acc: 100.0000%(16/16)\n",
            "Batch[3] - loss: 0.018639  acc: 100.0000%(16/16)\n",
            "Batch[4] - loss: 0.009376  acc: 100.0000%(16/16)\n",
            "Batch[5] - loss: 0.005237  acc: 100.0000%(16/16)\n",
            "Batch[6] - loss: 0.012913  acc: 100.0000%(16/16)\n",
            "Batch[7] - loss: 0.005637  acc: 100.0000%(16/16)\n",
            "Batch[8] - loss: 0.009823  acc: 100.0000%(16/16)\n",
            "Batch[9] - loss: 0.006089  acc: 100.0000%(16/16)\n",
            "Batch[10] - loss: 0.007297  acc: 100.0000%(16/16)\n",
            "Batch[11] - loss: 0.024559  acc: 100.0000%(16/16)\n",
            "Batch[12] - loss: 0.018630  acc: 100.0000%(16/16)\n",
            "Batch[13] - loss: 0.006327  acc: 100.0000%(16/16)\n",
            "Batch[14] - loss: 0.017666  acc: 100.0000%(16/16)\n",
            "Batch[15] - loss: 0.013634  acc: 100.0000%(16/16)\n",
            "Batch[16] - loss: 0.010059  acc: 100.0000%(16/16)\n",
            "Batch[17] - loss: 0.010754  acc: 100.0000%(16/16)\n",
            "Batch[18] - loss: 0.007530  acc: 100.0000%(16/16)\n",
            "Batch[19] - loss: 0.019621  acc: 100.0000%(16/16)\n",
            "Batch[20] - loss: 0.006971  acc: 100.0000%(16/16)\n",
            "Batch[21] - loss: 0.006250  acc: 100.0000%(16/16)\n",
            "Batch[22] - loss: 0.004275  acc: 100.0000%(16/16)\n",
            "Batch[23] - loss: 0.025377  acc: 100.0000%(16/16)\n",
            "Batch[24] - loss: 0.013645  acc: 100.0000%(16/16)\n",
            "Batch[25] - loss: 0.005315  acc: 100.0000%(16/16)\n",
            "Batch[26] - loss: 0.018095  acc: 100.0000%(16/16)\n",
            "Batch[27] - loss: 0.010024  acc: 100.0000%(16/16)\n",
            "Batch[28] - loss: 0.008841  acc: 100.0000%(16/16)\n",
            "Batch[29] - loss: 0.008084  acc: 100.0000%(16/16)\n",
            "Batch[30] - loss: 0.009209  acc: 100.0000%(16/16)\n",
            "Batch[31] - loss: 0.015611  acc: 100.0000%(16/16)\n",
            "Batch[32] - loss: 0.010376  acc: 100.0000%(16/16)\n",
            "Batch[33] - loss: 0.033368  acc: 100.0000%(16/16)\n",
            "Batch[34] - loss: 0.002435  acc: 100.0000%(8/8)\n",
            "Average loss:0.012337 average acc:100.000000%\n",
            "Val set acc: 0.8170731707317073\n",
            "Best val set acc: 0.8414634146341463\n",
            "================================\n",
            "task:  weibo\n",
            "{'lr': 0.001, 'reg': 1e-06, 'embeding_size': 100, 'batch_size': 128, 'nb_filters': 100, 'kernel_sizes': [3, 4, 5], 'dropout': 0.5, 'epochs': 18, 'num_classes': 2, 'target_names': ['NR', 'FR'], 'save_path': 'checkpoint/weights.best.weibo.pgan', 'maxlen': 50, 'n_heads': 7, 'embedding_weights': array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
            "         0.        ,  0.        ],\n",
            "       [ 0.02440675,  0.10759468,  0.05138169, ...,  0.18109576,\n",
            "         0.23645975,  0.23041733],\n",
            "       [ 0.1242    ,  0.1674    ,  0.0332    , ..., -0.0367    ,\n",
            "         0.0757    , -0.0261    ],\n",
            "       ...,\n",
            "       [-0.0005    ,  0.1042    ,  0.0187    , ..., -0.0083    ,\n",
            "        -0.0446    ,  0.0073    ],\n",
            "       [ 0.1876246 , -0.13593139,  0.17295381, ...,  0.02023736,\n",
            "        -0.01646991, -0.23309551],\n",
            "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
            "         0.        ,  0.        ]], dtype=float32), 'A_us': <1092x818 sparse matrix of type '<class 'numpy.float32'>'\n",
            "\twith 809 stored elements in Compressed Sparse Column format>, 'A_uu': <1092x1092 sparse matrix of type '<class 'numpy.float64'>'\n",
            "\twith 4369 stored elements in Compressed Sparse Column format>}\n",
            "PGAN(\n",
            "  (word_embedding): Embedding(15666, 300, padding_idx=0)\n",
            "  (user_embedding): Embedding(10052, 100, padding_idx=0)\n",
            "  (source_embedding): Embedding(4664, 100)\n",
            "  (convs): ModuleList(\n",
            "    (0): Conv1d(300, 100, kernel_size=(3,), stride=(1,))\n",
            "    (1): Conv1d(300, 100, kernel_size=(4,), stride=(1,))\n",
            "    (2): Conv1d(300, 100, kernel_size=(5,), stride=(1,))\n",
            "  )\n",
            "  (max_poolings): ModuleList(\n",
            "    (0): MaxPool1d(kernel_size=48, stride=48, padding=0, dilation=1, ceil_mode=False)\n",
            "    (1): MaxPool1d(kernel_size=47, stride=47, padding=0, dilation=1, ceil_mode=False)\n",
            "    (2): MaxPool1d(kernel_size=46, stride=46, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (linear): Linear(in_features=400, out_features=200, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (relu): ReLU()\n",
            "  (elu): ELU(alpha=1.0)\n",
            "  (fc_out): Sequential(\n",
            "    (0): Linear(in_features=500, out_features=100, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=100, out_features=2, bias=True)\n",
            "  )\n",
            "  (fc_user_out): Sequential(\n",
            "    (0): Linear(in_features=100, out_features=100, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=100, out_features=3, bias=True)\n",
            "  )\n",
            "  (fc_ruser_out): Sequential(\n",
            "    (0): Linear(in_features=100, out_features=100, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=100, out_features=3, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "Epoch  1 / 18\n",
            "Batch[0] - loss: 2.967798  acc: 52.3438%(67/128)\n",
            "Batch[1] - loss: 2.950964  acc: 46.0938%(59/128)\n",
            "Batch[2] - loss: 2.879043  acc: 50.7812%(65/128)\n",
            "Batch[3] - loss: 2.832844  acc: 55.4688%(71/128)\n",
            "Batch[4] - loss: 2.818242  acc: 54.6875%(70/128)\n",
            "Batch[5] - loss: 2.864703  acc: 52.3438%(67/128)\n",
            "Batch[6] - loss: 2.799407  acc: 57.0312%(73/128)\n",
            "Batch[7] - loss: 2.772932  acc: 59.3750%(76/128)\n",
            "Batch[8] - loss: 2.795443  acc: 53.1250%(68/128)\n",
            "Batch[9] - loss: 2.719719  acc: 61.7188%(79/128)\n",
            "Batch[10] - loss: 2.756007  acc: 64.0625%(82/128)\n",
            "Batch[11] - loss: 2.738253  acc: 60.9375%(78/128)\n",
            "Batch[12] - loss: 2.683817  acc: 64.8438%(83/128)\n",
            "Batch[13] - loss: 2.642544  acc: 64.8438%(83/128)\n",
            "Batch[14] - loss: 2.685394  acc: 63.2812%(81/128)\n",
            "Batch[15] - loss: 2.661049  acc: 67.9688%(87/128)\n",
            "Batch[16] - loss: 2.630734  acc: 64.8438%(83/128)\n",
            "Batch[17] - loss: 2.678582  acc: 64.0625%(82/128)\n",
            "Batch[18] - loss: 2.608927  acc: 71.0938%(91/128)\n",
            "Batch[19] - loss: 2.704033  acc: 63.2812%(81/128)\n",
            "Batch[20] - loss: 2.658213  acc: 67.1875%(86/128)\n",
            "Batch[21] - loss: 2.671607  acc: 66.4062%(85/128)\n",
            "Batch[22] - loss: 2.539464  acc: 72.6562%(93/128)\n",
            "Batch[23] - loss: 2.529457  acc: 67.9688%(87/128)\n",
            "Batch[24] - loss: 2.515256  acc: 74.6667%(56/75)\n",
            "Average loss:2.724177 average acc:61.642914%\n",
            "Val set acc: 0.7922912205567452\n",
            "Best val set acc: 0\n",
            "\n",
            "Epoch  2 / 18\n",
            "Batch[0] - loss: 2.449867  acc: 75.7812%(97/128)\n",
            "Batch[1] - loss: 2.370916  acc: 79.6875%(102/128)\n",
            "Batch[2] - loss: 2.449889  acc: 71.8750%(92/128)\n",
            "Batch[3] - loss: 2.316508  acc: 84.3750%(108/128)\n",
            "Batch[4] - loss: 2.353531  acc: 83.5938%(107/128)\n",
            "Batch[5] - loss: 2.506536  acc: 81.2500%(104/128)\n",
            "Batch[6] - loss: 2.375120  acc: 79.6875%(102/128)\n",
            "Batch[7] - loss: 2.330148  acc: 82.8125%(106/128)\n",
            "Batch[8] - loss: 2.374189  acc: 79.6875%(102/128)\n",
            "Batch[9] - loss: 2.418688  acc: 77.3438%(99/128)\n",
            "Batch[10] - loss: 2.300020  acc: 80.4688%(103/128)\n",
            "Batch[11] - loss: 2.363860  acc: 82.8125%(106/128)\n",
            "Batch[12] - loss: 2.243860  acc: 85.9375%(110/128)\n",
            "Batch[13] - loss: 2.265563  acc: 87.5000%(112/128)\n",
            "Batch[14] - loss: 2.150648  acc: 91.4062%(117/128)\n",
            "Batch[15] - loss: 2.315237  acc: 80.4688%(103/128)\n",
            "Batch[16] - loss: 2.187610  acc: 85.9375%(110/128)\n",
            "Batch[17] - loss: 2.110634  acc: 90.6250%(116/128)\n",
            "Batch[18] - loss: 2.095326  acc: 88.2812%(113/128)\n",
            "Batch[19] - loss: 2.058764  acc: 85.9375%(110/128)\n",
            "Batch[20] - loss: 2.086265  acc: 86.7188%(111/128)\n",
            "Batch[21] - loss: 1.887727  acc: 96.0938%(123/128)\n",
            "Batch[22] - loss: 2.039797  acc: 92.9688%(119/128)\n",
            "Batch[23] - loss: 2.061410  acc: 89.8438%(115/128)\n",
            "Batch[24] - loss: 2.039051  acc: 92.0000%(69/75)\n",
            "Average loss:2.246046 average acc:84.523750%\n",
            "Val set acc: 0.9036402569593148\n",
            "Best val set acc: 0\n",
            "\n",
            "Epoch  3 / 18\n",
            "Batch[0] - loss: 1.897050  acc: 92.1875%(118/128)\n",
            "Batch[1] - loss: 1.790661  acc: 96.8750%(124/128)\n",
            "Batch[2] - loss: 1.790307  acc: 96.8750%(124/128)\n",
            "Batch[3] - loss: 1.822354  acc: 95.3125%(122/128)\n",
            "Batch[4] - loss: 1.714137  acc: 95.3125%(122/128)\n",
            "Batch[5] - loss: 1.658644  acc: 97.6562%(125/128)\n",
            "Batch[6] - loss: 1.681894  acc: 98.4375%(126/128)\n",
            "Batch[7] - loss: 1.583004  acc: 96.8750%(124/128)\n",
            "Batch[8] - loss: 1.605333  acc: 97.6562%(125/128)\n",
            "Batch[9] - loss: 1.530762  acc: 96.8750%(124/128)\n",
            "Batch[10] - loss: 1.527914  acc: 97.6562%(125/128)\n",
            "Batch[11] - loss: 1.605160  acc: 96.8750%(124/128)\n",
            "Batch[12] - loss: 1.535144  acc: 97.6562%(125/128)\n",
            "Batch[13] - loss: 1.482388  acc: 96.8750%(124/128)\n",
            "Batch[14] - loss: 1.554479  acc: 96.8750%(124/128)\n",
            "Batch[15] - loss: 1.484517  acc: 97.6562%(125/128)\n",
            "Batch[16] - loss: 1.402221  acc: 97.6562%(125/128)\n",
            "Batch[17] - loss: 1.347173  acc: 98.4375%(126/128)\n",
            "Batch[18] - loss: 1.329160  acc: 96.8750%(124/128)\n",
            "Batch[19] - loss: 1.329633  acc: 99.2188%(127/128)\n",
            "Batch[20] - loss: 1.361905  acc: 96.8750%(124/128)\n",
            "Batch[21] - loss: 1.190586  acc: 99.2188%(127/128)\n",
            "Batch[22] - loss: 1.356727  acc: 99.2188%(127/128)\n",
            "Batch[23] - loss: 1.319201  acc: 98.4375%(126/128)\n",
            "Batch[24] - loss: 1.221652  acc: 100.0000%(75/75)\n",
            "Average loss:1.524880 average acc:97.343750%\n",
            "Val set acc: 0.9207708779443254\n",
            "Best val set acc: 0\n",
            "\n",
            "Epoch  4 / 18\n",
            "Batch[0] - loss: 1.230207  acc: 99.2188%(127/128)\n",
            "Batch[1] - loss: 1.112749  acc: 100.0000%(128/128)\n",
            "Batch[2] - loss: 1.060807  acc: 100.0000%(128/128)\n",
            "Batch[3] - loss: 1.067490  acc: 99.2188%(127/128)\n",
            "Batch[4] - loss: 1.072384  acc: 98.4375%(126/128)\n",
            "Batch[5] - loss: 0.987569  acc: 100.0000%(128/128)\n",
            "Batch[6] - loss: 0.859156  acc: 100.0000%(128/128)\n",
            "Batch[7] - loss: 0.986275  acc: 100.0000%(128/128)\n",
            "Batch[8] - loss: 0.841837  acc: 100.0000%(128/128)\n",
            "Batch[9] - loss: 0.953089  acc: 99.2188%(127/128)\n",
            "Batch[10] - loss: 0.832654  acc: 100.0000%(128/128)\n",
            "Batch[11] - loss: 0.918326  acc: 98.4375%(126/128)\n",
            "Batch[12] - loss: 0.870653  acc: 99.2188%(127/128)\n",
            "Batch[13] - loss: 0.794640  acc: 100.0000%(128/128)\n",
            "Batch[14] - loss: 0.820163  acc: 99.2188%(127/128)\n",
            "Batch[15] - loss: 0.778895  acc: 99.2188%(127/128)\n",
            "Batch[16] - loss: 0.722563  acc: 98.4375%(126/128)\n",
            "Batch[17] - loss: 0.694694  acc: 100.0000%(128/128)\n",
            "Batch[18] - loss: 0.696528  acc: 99.2188%(127/128)\n",
            "Batch[19] - loss: 0.743951  acc: 99.2188%(127/128)\n",
            "Batch[20] - loss: 0.663871  acc: 100.0000%(128/128)\n",
            "Batch[21] - loss: 0.690398  acc: 99.2188%(127/128)\n",
            "Batch[22] - loss: 0.711823  acc: 99.2188%(127/128)\n",
            "Batch[23] - loss: 0.629677  acc: 100.0000%(128/128)\n",
            "Batch[24] - loss: 0.695581  acc: 98.6667%(74/75)\n",
            "Average loss:0.857439 average acc:99.446671%\n",
            "Val set acc: 0.9271948608137045\n",
            "Best val set acc: 0\n",
            "\n",
            "Epoch  5 / 18\n",
            "Batch[0] - loss: 0.511093  acc: 100.0000%(128/128)\n",
            "Batch[1] - loss: 0.448351  acc: 100.0000%(128/128)\n",
            "Batch[2] - loss: 0.521638  acc: 100.0000%(128/128)\n",
            "Batch[3] - loss: 0.609785  acc: 99.2188%(127/128)\n",
            "Batch[4] - loss: 0.458701  acc: 99.2188%(127/128)\n",
            "Batch[5] - loss: 0.507000  acc: 100.0000%(128/128)\n",
            "Batch[6] - loss: 0.381274  acc: 100.0000%(128/128)\n",
            "Batch[7] - loss: 0.370561  acc: 100.0000%(128/128)\n",
            "Batch[8] - loss: 0.452577  acc: 99.2188%(127/128)\n",
            "Batch[9] - loss: 0.326423  acc: 100.0000%(128/128)\n",
            "Batch[10] - loss: 0.311939  acc: 100.0000%(128/128)\n",
            "Batch[11] - loss: 0.395136  acc: 100.0000%(128/128)\n",
            "Batch[12] - loss: 0.403953  acc: 100.0000%(128/128)\n",
            "Batch[13] - loss: 0.352921  acc: 100.0000%(128/128)\n",
            "Batch[14] - loss: 0.345482  acc: 100.0000%(128/128)\n",
            "Batch[15] - loss: 0.301924  acc: 99.2188%(127/128)\n",
            "Batch[16] - loss: 0.334616  acc: 100.0000%(128/128)\n",
            "Batch[17] - loss: 0.290833  acc: 100.0000%(128/128)\n",
            "Batch[18] - loss: 0.306630  acc: 99.2188%(127/128)\n",
            "Batch[19] - loss: 0.311366  acc: 100.0000%(128/128)\n",
            "Batch[20] - loss: 0.309685  acc: 100.0000%(128/128)\n",
            "Batch[21] - loss: 0.288461  acc: 99.2188%(127/128)\n",
            "Batch[22] - loss: 0.293804  acc: 100.0000%(128/128)\n",
            "Batch[23] - loss: 0.285139  acc: 100.0000%(128/128)\n",
            "Batch[24] - loss: 0.296126  acc: 100.0000%(75/75)\n",
            "Average loss:0.376617 average acc:99.812500%\n",
            "Val set acc: 0.9314775160599572\n",
            "Best val set acc: 0\n",
            "\n",
            "Epoch  6 / 18\n",
            "Batch[0] - loss: 0.235192  acc: 100.0000%(128/128)\n",
            "Batch[1] - loss: 0.223705  acc: 100.0000%(128/128)\n",
            "Batch[2] - loss: 0.256117  acc: 99.2188%(127/128)\n",
            "Batch[3] - loss: 0.186686  acc: 100.0000%(128/128)\n",
            "Batch[4] - loss: 0.251885  acc: 98.4375%(126/128)\n",
            "Batch[5] - loss: 0.193455  acc: 100.0000%(128/128)\n",
            "Batch[6] - loss: 0.186319  acc: 100.0000%(128/128)\n",
            "Batch[7] - loss: 0.189911  acc: 99.2188%(127/128)\n",
            "Batch[8] - loss: 0.172234  acc: 100.0000%(128/128)\n",
            "Batch[9] - loss: 0.182098  acc: 100.0000%(128/128)\n",
            "Batch[10] - loss: 0.171256  acc: 100.0000%(128/128)\n",
            "Batch[11] - loss: 0.188974  acc: 98.4375%(126/128)\n",
            "Batch[12] - loss: 0.165393  acc: 99.2188%(127/128)\n",
            "Batch[13] - loss: 0.200089  acc: 99.2188%(127/128)\n",
            "Batch[14] - loss: 0.131598  acc: 100.0000%(128/128)\n",
            "Batch[15] - loss: 0.176493  acc: 100.0000%(128/128)\n",
            "Batch[16] - loss: 0.124569  acc: 100.0000%(128/128)\n",
            "Batch[17] - loss: 0.143055  acc: 100.0000%(128/128)\n",
            "Batch[18] - loss: 0.151076  acc: 98.4375%(126/128)\n",
            "Batch[19] - loss: 0.118534  acc: 100.0000%(128/128)\n",
            "Batch[20] - loss: 0.124719  acc: 99.2188%(127/128)\n",
            "Batch[21] - loss: 0.146562  acc: 100.0000%(128/128)\n",
            "Batch[22] - loss: 0.140787  acc: 100.0000%(128/128)\n",
            "Batch[23] - loss: 0.142739  acc: 100.0000%(128/128)\n",
            "Batch[24] - loss: 0.112298  acc: 100.0000%(75/75)\n",
            "Average loss:0.172630 average acc:99.656250%\n",
            "Val set acc: 0.9464668094218416\n",
            "Best val set acc: 0\n",
            "\n",
            "Epoch  7 / 18\n",
            "Batch[0] - loss: 0.115479  acc: 100.0000%(128/128)\n",
            "Batch[1] - loss: 0.123326  acc: 100.0000%(128/128)\n",
            "Batch[2] - loss: 0.108029  acc: 99.2188%(127/128)\n",
            "Batch[3] - loss: 0.102635  acc: 100.0000%(128/128)\n",
            "Batch[4] - loss: 0.104052  acc: 100.0000%(128/128)\n",
            "Batch[5] - loss: 0.076591  acc: 100.0000%(128/128)\n",
            "Batch[6] - loss: 0.095198  acc: 100.0000%(128/128)\n",
            "Batch[7] - loss: 0.092397  acc: 100.0000%(128/128)\n",
            "Batch[8] - loss: 0.110545  acc: 99.2188%(127/128)\n",
            "Batch[9] - loss: 0.084382  acc: 100.0000%(128/128)\n",
            "Batch[10] - loss: 0.099617  acc: 100.0000%(128/128)\n",
            "Batch[11] - loss: 0.100797  acc: 98.4375%(126/128)\n",
            "Batch[12] - loss: 0.070210  acc: 100.0000%(128/128)\n",
            "Batch[13] - loss: 0.105359  acc: 99.2188%(127/128)\n",
            "Batch[14] - loss: 0.076852  acc: 100.0000%(128/128)\n",
            "Batch[15] - loss: 0.089519  acc: 99.2188%(127/128)\n",
            "Batch[16] - loss: 0.095127  acc: 99.2188%(127/128)\n",
            "Batch[17] - loss: 0.090847  acc: 99.2188%(127/128)\n",
            "Batch[18] - loss: 0.073069  acc: 100.0000%(128/128)\n",
            "Batch[19] - loss: 0.054308  acc: 100.0000%(128/128)\n",
            "Batch[20] - loss: 0.092918  acc: 100.0000%(128/128)\n",
            "Batch[21] - loss: 0.057469  acc: 100.0000%(128/128)\n",
            "Batch[22] - loss: 0.061917  acc: 99.2188%(127/128)\n",
            "Batch[23] - loss: 0.059828  acc: 100.0000%(128/128)\n",
            "Batch[24] - loss: 0.068474  acc: 100.0000%(75/75)\n",
            "Average loss:0.088358 average acc:99.718750%\n",
            "Val set acc: 0.9464668094218416\n",
            "Best val set acc: 0\n",
            "\n",
            "Epoch  8 / 18\n",
            "Batch[0] - loss: 0.052923  acc: 100.0000%(128/128)\n",
            "Batch[1] - loss: 0.065230  acc: 99.2188%(127/128)\n",
            "Batch[2] - loss: 0.046724  acc: 100.0000%(128/128)\n",
            "Batch[3] - loss: 0.063509  acc: 100.0000%(128/128)\n",
            "Batch[4] - loss: 0.042058  acc: 100.0000%(128/128)\n",
            "Batch[5] - loss: 0.057364  acc: 100.0000%(128/128)\n",
            "Batch[6] - loss: 0.054221  acc: 100.0000%(128/128)\n",
            "Batch[7] - loss: 0.049678  acc: 100.0000%(128/128)\n",
            "Batch[8] - loss: 0.052065  acc: 100.0000%(128/128)\n",
            "Batch[9] - loss: 0.058485  acc: 99.2188%(127/128)\n",
            "Batch[10] - loss: 0.059202  acc: 100.0000%(128/128)\n",
            "Batch[11] - loss: 0.058264  acc: 100.0000%(128/128)\n",
            "Batch[12] - loss: 0.074380  acc: 98.4375%(126/128)\n",
            "Batch[13] - loss: 0.039767  acc: 100.0000%(128/128)\n",
            "Batch[14] - loss: 0.055074  acc: 100.0000%(128/128)\n",
            "Batch[15] - loss: 0.059397  acc: 100.0000%(128/128)\n",
            "Batch[16] - loss: 0.028708  acc: 100.0000%(128/128)\n",
            "Batch[17] - loss: 0.046228  acc: 100.0000%(128/128)\n",
            "Batch[18] - loss: 0.057930  acc: 100.0000%(128/128)\n",
            "Batch[19] - loss: 0.047951  acc: 100.0000%(128/128)\n",
            "Batch[20] - loss: 0.042432  acc: 100.0000%(128/128)\n",
            "Batch[21] - loss: 0.037944  acc: 100.0000%(128/128)\n",
            "Batch[22] - loss: 0.043306  acc: 100.0000%(128/128)\n",
            "Batch[23] - loss: 0.055847  acc: 100.0000%(128/128)\n",
            "Batch[24] - loss: 0.031591  acc: 100.0000%(75/75)\n",
            "Average loss:0.051211 average acc:99.875000%\n",
            "Val set acc: 0.9443254817987152\n",
            "Best val set acc: 0\n",
            "\n",
            "Epoch  9 / 18\n",
            "Batch[0] - loss: 0.032829  acc: 100.0000%(128/128)\n",
            "Batch[1] - loss: 0.032398  acc: 100.0000%(128/128)\n",
            "Batch[2] - loss: 0.029060  acc: 100.0000%(128/128)\n",
            "Batch[3] - loss: 0.024183  acc: 100.0000%(128/128)\n",
            "Batch[4] - loss: 0.033546  acc: 100.0000%(128/128)\n",
            "Batch[5] - loss: 0.050713  acc: 99.2188%(127/128)\n",
            "Batch[6] - loss: 0.027842  acc: 100.0000%(128/128)\n",
            "Batch[7] - loss: 0.036080  acc: 100.0000%(128/128)\n",
            "Batch[8] - loss: 0.030508  acc: 100.0000%(128/128)\n",
            "Batch[9] - loss: 0.047323  acc: 100.0000%(128/128)\n",
            "Batch[10] - loss: 0.032345  acc: 100.0000%(128/128)\n",
            "Batch[11] - loss: 0.038697  acc: 100.0000%(128/128)\n",
            "Batch[12] - loss: 0.048481  acc: 100.0000%(128/128)\n",
            "Batch[13] - loss: 0.019471  acc: 100.0000%(128/128)\n",
            "Batch[14] - loss: 0.027967  acc: 100.0000%(128/128)\n",
            "Batch[15] - loss: 0.031013  acc: 100.0000%(128/128)\n",
            "Batch[16] - loss: 0.035114  acc: 99.2188%(127/128)\n",
            "Batch[17] - loss: 0.031342  acc: 100.0000%(128/128)\n",
            "Batch[18] - loss: 0.054446  acc: 98.4375%(126/128)\n",
            "Batch[19] - loss: 0.036244  acc: 99.2188%(127/128)\n",
            "Batch[20] - loss: 0.027496  acc: 100.0000%(128/128)\n",
            "Batch[21] - loss: 0.028076  acc: 100.0000%(128/128)\n",
            "Batch[22] - loss: 0.032819  acc: 100.0000%(128/128)\n",
            "Batch[23] - loss: 0.026361  acc: 100.0000%(128/128)\n",
            "Batch[24] - loss: 0.041684  acc: 100.0000%(75/75)\n",
            "Average loss:0.034242 average acc:99.843750%\n",
            "Val set acc: 0.9443254817987152\n",
            "Best val set acc: 0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          NR    0.96035   0.92766   0.94372       235\n",
            "          FR    0.92917   0.96121   0.94492       232\n",
            "\n",
            "    accuracy                        0.94433       467\n",
            "   macro avg    0.94476   0.94443   0.94432       467\n",
            "weighted avg    0.94486   0.94433   0.94432       467\n",
            "\n",
            "save model!!!\n",
            "\n",
            "Epoch  10 / 18\n",
            "Batch[0] - loss: 0.024322  acc: 100.0000%(128/128)\n",
            "Batch[1] - loss: 0.028884  acc: 100.0000%(128/128)\n",
            "Batch[2] - loss: 0.022713  acc: 100.0000%(128/128)\n",
            "Batch[3] - loss: 0.023673  acc: 100.0000%(128/128)\n",
            "Batch[4] - loss: 0.020721  acc: 100.0000%(128/128)\n",
            "Batch[5] - loss: 0.026695  acc: 100.0000%(128/128)\n",
            "Batch[6] - loss: 0.020746  acc: 100.0000%(128/128)\n",
            "Batch[7] - loss: 0.023458  acc: 100.0000%(128/128)\n",
            "Batch[8] - loss: 0.024322  acc: 100.0000%(128/128)\n",
            "Batch[9] - loss: 0.020865  acc: 100.0000%(128/128)\n",
            "Batch[10] - loss: 0.028365  acc: 100.0000%(128/128)\n",
            "Batch[11] - loss: 0.024727  acc: 100.0000%(128/128)\n",
            "Batch[12] - loss: 0.035218  acc: 99.2188%(127/128)\n",
            "Batch[13] - loss: 0.016712  acc: 100.0000%(128/128)\n",
            "Batch[14] - loss: 0.017345  acc: 100.0000%(128/128)\n",
            "Batch[15] - loss: 0.022700  acc: 100.0000%(128/128)\n",
            "Batch[16] - loss: 0.027485  acc: 100.0000%(128/128)\n",
            "Batch[17] - loss: 0.019626  acc: 100.0000%(128/128)\n",
            "Batch[18] - loss: 0.018341  acc: 100.0000%(128/128)\n",
            "Batch[19] - loss: 0.021074  acc: 100.0000%(128/128)\n",
            "Batch[20] - loss: 0.023691  acc: 100.0000%(128/128)\n",
            "Batch[21] - loss: 0.031481  acc: 99.2188%(127/128)\n",
            "Batch[22] - loss: 0.018918  acc: 100.0000%(128/128)\n",
            "Batch[23] - loss: 0.017143  acc: 100.0000%(128/128)\n",
            "Batch[24] - loss: 0.017882  acc: 100.0000%(75/75)\n",
            "Average loss:0.023084 average acc:99.937500%\n",
            "Val set acc: 0.9464668094218416\n",
            "Best val set acc: 0.9443254817987152\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          NR    0.96053   0.93191   0.94600       235\n",
            "          FR    0.93305   0.96121   0.94692       232\n",
            "\n",
            "    accuracy                        0.94647       467\n",
            "   macro avg    0.94679   0.94656   0.94646       467\n",
            "weighted avg    0.94688   0.94647   0.94646       467\n",
            "\n",
            "save model!!!\n",
            "\n",
            "Epoch  11 / 18\n",
            "Batch[0] - loss: 0.029703  acc: 99.2188%(127/128)\n",
            "Batch[1] - loss: 0.017389  acc: 100.0000%(128/128)\n",
            "Batch[2] - loss: 0.016768  acc: 100.0000%(128/128)\n",
            "Batch[3] - loss: 0.018406  acc: 100.0000%(128/128)\n",
            "Batch[4] - loss: 0.018017  acc: 100.0000%(128/128)\n",
            "Batch[5] - loss: 0.029113  acc: 100.0000%(128/128)\n",
            "Batch[6] - loss: 0.030175  acc: 100.0000%(128/128)\n",
            "Batch[7] - loss: 0.015479  acc: 100.0000%(128/128)\n",
            "Batch[8] - loss: 0.017362  acc: 100.0000%(128/128)\n",
            "Batch[9] - loss: 0.022975  acc: 100.0000%(128/128)\n",
            "Batch[10] - loss: 0.012597  acc: 100.0000%(128/128)\n",
            "Batch[11] - loss: 0.013103  acc: 100.0000%(128/128)\n",
            "Batch[12] - loss: 0.015641  acc: 100.0000%(128/128)\n",
            "Batch[13] - loss: 0.016203  acc: 100.0000%(128/128)\n",
            "Batch[14] - loss: 0.014203  acc: 100.0000%(128/128)\n",
            "Batch[15] - loss: 0.021149  acc: 100.0000%(128/128)\n",
            "Batch[16] - loss: 0.013832  acc: 100.0000%(128/128)\n",
            "Batch[17] - loss: 0.016823  acc: 100.0000%(128/128)\n",
            "Batch[18] - loss: 0.015872  acc: 100.0000%(128/128)\n",
            "Batch[19] - loss: 0.016669  acc: 100.0000%(128/128)\n",
            "Batch[20] - loss: 0.066023  acc: 99.2188%(127/128)\n",
            "Batch[21] - loss: 0.016010  acc: 100.0000%(128/128)\n",
            "Batch[22] - loss: 0.014885  acc: 100.0000%(128/128)\n",
            "Batch[23] - loss: 0.009523  acc: 100.0000%(128/128)\n",
            "Batch[24] - loss: 0.013051  acc: 100.0000%(75/75)\n",
            "Average loss:0.019639 average acc:99.937500%\n",
            "Val set acc: 0.9336188436830836\n",
            "Best val set acc: 0.9464668094218416\n",
            "\n",
            "Epoch  12 / 18\n",
            "Batch[0] - loss: 0.012454  acc: 100.0000%(128/128)\n",
            "Batch[1] - loss: 0.011138  acc: 100.0000%(128/128)\n",
            "Batch[2] - loss: 0.025491  acc: 99.2188%(127/128)\n",
            "Batch[3] - loss: 0.012064  acc: 100.0000%(128/128)\n",
            "Batch[4] - loss: 0.010418  acc: 100.0000%(128/128)\n",
            "Batch[5] - loss: 0.010703  acc: 100.0000%(128/128)\n",
            "Batch[6] - loss: 0.011061  acc: 100.0000%(128/128)\n",
            "Batch[7] - loss: 0.011640  acc: 100.0000%(128/128)\n",
            "Batch[8] - loss: 0.019858  acc: 100.0000%(128/128)\n",
            "Batch[9] - loss: 0.017239  acc: 100.0000%(128/128)\n",
            "Batch[10] - loss: 0.010044  acc: 100.0000%(128/128)\n",
            "Batch[11] - loss: 0.014034  acc: 100.0000%(128/128)\n",
            "Batch[12] - loss: 0.011497  acc: 100.0000%(128/128)\n",
            "Batch[13] - loss: 0.009520  acc: 100.0000%(128/128)\n",
            "Batch[14] - loss: 0.016192  acc: 100.0000%(128/128)\n",
            "Batch[15] - loss: 0.012851  acc: 100.0000%(128/128)\n",
            "Batch[16] - loss: 0.019553  acc: 100.0000%(128/128)\n",
            "Batch[17] - loss: 0.016989  acc: 100.0000%(128/128)\n",
            "Batch[18] - loss: 0.012012  acc: 100.0000%(128/128)\n",
            "Batch[19] - loss: 0.016699  acc: 100.0000%(128/128)\n",
            "Batch[20] - loss: 0.010407  acc: 100.0000%(128/128)\n",
            "Batch[21] - loss: 0.011136  acc: 100.0000%(128/128)\n",
            "Batch[22] - loss: 0.017929  acc: 100.0000%(128/128)\n",
            "Batch[23] - loss: 0.009644  acc: 100.0000%(128/128)\n",
            "Batch[24] - loss: 0.016756  acc: 100.0000%(75/75)\n",
            "Average loss:0.013893 average acc:99.968750%\n",
            "Val set acc: 0.9357601713062098\n",
            "Best val set acc: 0.9464668094218416\n",
            "\n",
            "Epoch  13 / 18\n",
            "Batch[0] - loss: 0.010603  acc: 100.0000%(128/128)\n",
            "Batch[1] - loss: 0.012330  acc: 100.0000%(128/128)\n",
            "Batch[2] - loss: 0.010514  acc: 100.0000%(128/128)\n",
            "Batch[3] - loss: 0.010623  acc: 100.0000%(128/128)\n",
            "Batch[4] - loss: 0.011598  acc: 100.0000%(128/128)\n",
            "Batch[5] - loss: 0.011639  acc: 100.0000%(128/128)\n",
            "Batch[6] - loss: 0.011839  acc: 100.0000%(128/128)\n",
            "Batch[7] - loss: 0.016316  acc: 100.0000%(128/128)\n",
            "Batch[8] - loss: 0.009051  acc: 100.0000%(128/128)\n",
            "Batch[9] - loss: 0.009158  acc: 100.0000%(128/128)\n",
            "Batch[10] - loss: 0.007368  acc: 100.0000%(128/128)\n",
            "Batch[11] - loss: 0.010441  acc: 100.0000%(128/128)\n",
            "Batch[12] - loss: 0.007690  acc: 100.0000%(128/128)\n",
            "Batch[13] - loss: 0.009337  acc: 100.0000%(128/128)\n",
            "Batch[14] - loss: 0.015698  acc: 100.0000%(128/128)\n",
            "Batch[15] - loss: 0.010516  acc: 100.0000%(128/128)\n",
            "Batch[16] - loss: 0.011762  acc: 100.0000%(128/128)\n",
            "Batch[17] - loss: 0.006152  acc: 100.0000%(128/128)\n",
            "Batch[18] - loss: 0.009410  acc: 100.0000%(128/128)\n",
            "Batch[19] - loss: 0.008204  acc: 100.0000%(128/128)\n",
            "Batch[20] - loss: 0.009579  acc: 100.0000%(128/128)\n",
            "Batch[21] - loss: 0.010831  acc: 100.0000%(128/128)\n",
            "Batch[22] - loss: 0.008405  acc: 100.0000%(128/128)\n",
            "Batch[23] - loss: 0.009512  acc: 100.0000%(128/128)\n",
            "Batch[24] - loss: 0.008569  acc: 100.0000%(75/75)\n",
            "Average loss:0.010286 average acc:100.000000%\n",
            "Val set acc: 0.9464668094218416\n",
            "Best val set acc: 0.9464668094218416\n",
            "\n",
            "Epoch  14 / 18\n",
            "Batch[0] - loss: 0.011673  acc: 100.0000%(128/128)\n",
            "Batch[1] - loss: 0.008494  acc: 100.0000%(128/128)\n",
            "Batch[2] - loss: 0.010241  acc: 100.0000%(128/128)\n",
            "Batch[3] - loss: 0.007173  acc: 100.0000%(128/128)\n",
            "Batch[4] - loss: 0.004375  acc: 100.0000%(128/128)\n",
            "Batch[5] - loss: 0.007457  acc: 100.0000%(128/128)\n",
            "Batch[6] - loss: 0.009198  acc: 100.0000%(128/128)\n",
            "Batch[7] - loss: 0.006113  acc: 100.0000%(128/128)\n",
            "Batch[8] - loss: 0.006768  acc: 100.0000%(128/128)\n",
            "Batch[9] - loss: 0.012182  acc: 100.0000%(128/128)\n",
            "Batch[10] - loss: 0.007854  acc: 100.0000%(128/128)\n",
            "Batch[11] - loss: 0.006914  acc: 100.0000%(128/128)\n",
            "Batch[12] - loss: 0.006848  acc: 100.0000%(128/128)\n",
            "Batch[13] - loss: 0.008637  acc: 100.0000%(128/128)\n",
            "Batch[14] - loss: 0.009870  acc: 100.0000%(128/128)\n",
            "Batch[15] - loss: 0.008159  acc: 100.0000%(128/128)\n",
            "Batch[16] - loss: 0.008719  acc: 100.0000%(128/128)\n",
            "Batch[17] - loss: 0.012587  acc: 100.0000%(128/128)\n",
            "Batch[18] - loss: 0.008297  acc: 100.0000%(128/128)\n",
            "Batch[19] - loss: 0.006226  acc: 100.0000%(128/128)\n",
            "Batch[20] - loss: 0.008562  acc: 100.0000%(128/128)\n",
            "Batch[21] - loss: 0.009349  acc: 100.0000%(128/128)\n",
            "Batch[22] - loss: 0.007940  acc: 100.0000%(128/128)\n",
            "Batch[23] - loss: 0.017305  acc: 100.0000%(128/128)\n",
            "Batch[24] - loss: 0.010800  acc: 100.0000%(75/75)\n",
            "Average loss:0.008870 average acc:100.000000%\n",
            "Reload the best model...\n",
            "0.0005\n",
            "Val set acc: 0.9464668094218416\n",
            "Best val set acc: 0.9464668094218416\n",
            "\n",
            "Epoch  15 / 18\n",
            "Batch[0] - loss: 0.017945  acc: 100.0000%(128/128)\n",
            "Batch[1] - loss: 0.018848  acc: 100.0000%(128/128)\n",
            "Batch[2] - loss: 0.020746  acc: 100.0000%(128/128)\n",
            "Batch[3] - loss: 0.015832  acc: 100.0000%(128/128)\n",
            "Batch[4] - loss: 0.017657  acc: 100.0000%(128/128)\n",
            "Batch[5] - loss: 0.020180  acc: 100.0000%(128/128)\n",
            "Batch[6] - loss: 0.019025  acc: 100.0000%(128/128)\n",
            "Batch[7] - loss: 0.015296  acc: 100.0000%(128/128)\n",
            "Batch[8] - loss: 0.026456  acc: 100.0000%(128/128)\n",
            "Batch[9] - loss: 0.016792  acc: 100.0000%(128/128)\n",
            "Batch[10] - loss: 0.016599  acc: 100.0000%(128/128)\n",
            "Batch[11] - loss: 0.018114  acc: 100.0000%(128/128)\n",
            "Batch[12] - loss: 0.022600  acc: 100.0000%(128/128)\n",
            "Batch[13] - loss: 0.013976  acc: 100.0000%(128/128)\n",
            "Batch[14] - loss: 0.015877  acc: 100.0000%(128/128)\n",
            "Batch[15] - loss: 0.018905  acc: 100.0000%(128/128)\n",
            "Batch[16] - loss: 0.016024  acc: 100.0000%(128/128)\n",
            "Batch[17] - loss: 0.021939  acc: 100.0000%(128/128)\n",
            "Batch[18] - loss: 0.013921  acc: 100.0000%(128/128)\n",
            "Batch[19] - loss: 0.021515  acc: 100.0000%(128/128)\n",
            "Batch[20] - loss: 0.019431  acc: 100.0000%(128/128)\n",
            "Batch[21] - loss: 0.022809  acc: 100.0000%(128/128)\n",
            "Batch[22] - loss: 0.015735  acc: 100.0000%(128/128)\n",
            "Batch[23] - loss: 0.014658  acc: 100.0000%(128/128)\n",
            "Batch[24] - loss: 0.025840  acc: 100.0000%(75/75)\n",
            "Average loss:0.018669 average acc:100.000000%\n",
            "Val set acc: 0.9464668094218416\n",
            "Best val set acc: 0.9464668094218416\n",
            "\n",
            "Epoch  16 / 18\n",
            "Batch[0] - loss: 0.016933  acc: 100.0000%(128/128)\n",
            "Batch[1] - loss: 0.016276  acc: 100.0000%(128/128)\n",
            "Batch[2] - loss: 0.026387  acc: 100.0000%(128/128)\n",
            "Batch[3] - loss: 0.013403  acc: 100.0000%(128/128)\n",
            "Batch[4] - loss: 0.015719  acc: 100.0000%(128/128)\n",
            "Batch[5] - loss: 0.017825  acc: 100.0000%(128/128)\n",
            "Batch[6] - loss: 0.017473  acc: 100.0000%(128/128)\n",
            "Batch[7] - loss: 0.011949  acc: 100.0000%(128/128)\n",
            "Batch[8] - loss: 0.012272  acc: 100.0000%(128/128)\n",
            "Batch[9] - loss: 0.012890  acc: 100.0000%(128/128)\n",
            "Batch[10] - loss: 0.017561  acc: 100.0000%(128/128)\n",
            "Batch[11] - loss: 0.020831  acc: 99.2188%(127/128)\n",
            "Batch[12] - loss: 0.015981  acc: 100.0000%(128/128)\n",
            "Batch[13] - loss: 0.014106  acc: 100.0000%(128/128)\n",
            "Batch[14] - loss: 0.014462  acc: 100.0000%(128/128)\n",
            "Batch[15] - loss: 0.013438  acc: 100.0000%(128/128)\n",
            "Batch[16] - loss: 0.011072  acc: 100.0000%(128/128)\n",
            "Batch[17] - loss: 0.018146  acc: 100.0000%(128/128)\n",
            "Batch[18] - loss: 0.016517  acc: 100.0000%(128/128)\n",
            "Batch[19] - loss: 0.015270  acc: 100.0000%(128/128)\n",
            "Batch[20] - loss: 0.011932  acc: 100.0000%(128/128)\n",
            "Batch[21] - loss: 0.014302  acc: 100.0000%(128/128)\n",
            "Batch[22] - loss: 0.013413  acc: 100.0000%(128/128)\n",
            "Batch[23] - loss: 0.015096  acc: 100.0000%(128/128)\n",
            "Batch[24] - loss: 0.010996  acc: 100.0000%(75/75)\n",
            "Average loss:0.015370 average acc:99.968750%\n",
            "Val set acc: 0.9443254817987152\n",
            "Best val set acc: 0.9464668094218416\n",
            "\n",
            "Epoch  17 / 18\n",
            "Batch[0] - loss: 0.012317  acc: 100.0000%(128/128)\n",
            "Batch[1] - loss: 0.019063  acc: 100.0000%(128/128)\n",
            "Batch[2] - loss: 0.013291  acc: 100.0000%(128/128)\n",
            "Batch[3] - loss: 0.019308  acc: 100.0000%(128/128)\n",
            "Batch[4] - loss: 0.015478  acc: 100.0000%(128/128)\n",
            "Batch[5] - loss: 0.010132  acc: 100.0000%(128/128)\n",
            "Batch[6] - loss: 0.012522  acc: 100.0000%(128/128)\n",
            "Batch[7] - loss: 0.017512  acc: 100.0000%(128/128)\n",
            "Batch[8] - loss: 0.011597  acc: 100.0000%(128/128)\n",
            "Batch[9] - loss: 0.016913  acc: 100.0000%(128/128)\n",
            "Batch[10] - loss: 0.011151  acc: 100.0000%(128/128)\n",
            "Batch[11] - loss: 0.017976  acc: 100.0000%(128/128)\n",
            "Batch[12] - loss: 0.010213  acc: 100.0000%(128/128)\n",
            "Batch[13] - loss: 0.012042  acc: 100.0000%(128/128)\n",
            "Batch[14] - loss: 0.013491  acc: 100.0000%(128/128)\n",
            "Batch[15] - loss: 0.015995  acc: 100.0000%(128/128)\n",
            "Batch[16] - loss: 0.014453  acc: 100.0000%(128/128)\n",
            "Batch[17] - loss: 0.012283  acc: 100.0000%(128/128)\n",
            "Batch[18] - loss: 0.012779  acc: 100.0000%(128/128)\n",
            "Batch[19] - loss: 0.010140  acc: 100.0000%(128/128)\n",
            "Batch[20] - loss: 0.011568  acc: 100.0000%(128/128)\n",
            "Batch[21] - loss: 0.010489  acc: 100.0000%(128/128)\n",
            "Batch[22] - loss: 0.011174  acc: 100.0000%(128/128)\n",
            "Batch[23] - loss: 0.011552  acc: 100.0000%(128/128)\n",
            "Batch[24] - loss: 0.009864  acc: 100.0000%(75/75)\n",
            "Average loss:0.013332 average acc:100.000000%\n",
            "Reload the best model...\n",
            "0.00025\n",
            "Val set acc: 0.9464668094218416\n",
            "Best val set acc: 0.9464668094218416\n",
            "\n",
            "Epoch  18 / 18\n",
            "Batch[0] - loss: 0.013529  acc: 100.0000%(128/128)\n",
            "Batch[1] - loss: 0.015383  acc: 100.0000%(128/128)\n",
            "Batch[2] - loss: 0.018409  acc: 100.0000%(128/128)\n",
            "Batch[3] - loss: 0.025829  acc: 100.0000%(128/128)\n",
            "Batch[4] - loss: 0.029949  acc: 100.0000%(128/128)\n",
            "Batch[5] - loss: 0.014015  acc: 100.0000%(128/128)\n",
            "Batch[6] - loss: 0.013056  acc: 100.0000%(128/128)\n",
            "Batch[7] - loss: 0.011086  acc: 100.0000%(128/128)\n",
            "Batch[8] - loss: 0.014607  acc: 100.0000%(128/128)\n",
            "Batch[9] - loss: 0.015943  acc: 100.0000%(128/128)\n",
            "Batch[10] - loss: 0.022971  acc: 100.0000%(128/128)\n",
            "Batch[11] - loss: 0.014727  acc: 100.0000%(128/128)\n",
            "Batch[12] - loss: 0.014730  acc: 100.0000%(128/128)\n",
            "Batch[13] - loss: 0.019120  acc: 100.0000%(128/128)\n",
            "Batch[14] - loss: 0.025228  acc: 100.0000%(128/128)\n",
            "Batch[15] - loss: 0.020895  acc: 99.2188%(127/128)\n",
            "Batch[16] - loss: 0.015564  acc: 100.0000%(128/128)\n",
            "Batch[17] - loss: 0.016176  acc: 100.0000%(128/128)\n",
            "Batch[18] - loss: 0.025559  acc: 100.0000%(128/128)\n",
            "Batch[19] - loss: 0.022049  acc: 100.0000%(128/128)\n",
            "Batch[20] - loss: 0.020753  acc: 100.0000%(128/128)\n",
            "Batch[21] - loss: 0.019708  acc: 99.2188%(127/128)\n",
            "Batch[22] - loss: 0.016149  acc: 100.0000%(128/128)\n",
            "Batch[23] - loss: 0.011451  acc: 100.0000%(128/128)\n",
            "Batch[24] - loss: 0.015995  acc: 100.0000%(75/75)\n",
            "Average loss:0.018115 average acc:99.937500%\n",
            "Val set acc: 0.9421841541755889\n",
            "Best val set acc: 0.9464668094218416\n",
            "================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttiBQ8_bi3Tb"
      },
      "source": [
        "# **4. Result Visualization**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EWXm23NX6cJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "1c9c9376-ec63-4bc9-ea0a-27eaf3529633"
      },
      "source": [
        "df_twt15_vis = df_twt15.drop(['accuracy', 'macro avg', 'weighted avg'])\n",
        "df_twt15_vis = df_twt15_vis.drop(['support', 'precision', 'recall'], axis = 1)\n",
        "sns.heatmap(df_twt15_vis, annot = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f3d20faea10>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7LWZtNBY_lF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "outputId": "41a04468-31e8-493a-f953-6ae5fa933895"
      },
      "source": [
        "df_twt16_vis = df_twt16.drop(['accuracy', 'macro avg', 'weighted avg'])\n",
        "df_twt16_vis = df_twt16_vis.drop(['support', 'precision', 'recall'], axis = 1)\n",
        "sns.heatmap(df_twt16_vis, annot = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f3d1e5d8f90>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAAD4CAYAAADbyJysAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAce0lEQVR4nO3deZQV1bn38e+vGeIEyigyiHolCRgBJ9REFPWqEAfEKXpNDEmUmKjRJCSRe43JRQ3mjRqTVxKDr0ZRb5w1GFFRBFTUCCqDoCiD0W5QcL5Oofv08/7RBTndQp/TcLq7Tvn7uPbiVNU+Vc9ZC5+1eWrXLkUEZmbW+ipaOwAzM6vjhGxmlhJOyGZmKeGEbGaWEk7IZmYp0ba5LzC016GexmFmRXmsaro29xzVby4vOue067rLZl+vlDxCNjNLiWYfIZuZtajaXGtHsMmckM0sW3I1rR3BJnNCNrNMiaht7RA2mROymWVLrROymVk6eIRsZpYSvqlnZpYSHiGbmaVDeJaFmVlK+KaemVlKuGRhZpYSvqlnZpYSHiGbmaWEb+qZmaWEb+qZmaVDhGvIZmbp4BqymVlKuGRhZpYSHiGbmaVErrq1I9hkTshmli0uWZiZpYRLFmb/MmTYPpw7/iwqKir421+mcvPEW+od375Xd8Zd8RO267wd77/7Phf9YAJrVr3J9r2686trx6MK0bZtW+7889389ca/tdKvsLKV5RGypDZAp4h4M9luD4wGfhgR/Zs3PCs3FRUV/OiSH/DDU37KmlVruGbqH5g97Uleefkf6/ucdeGZPHDHQzxw+zT2/MpgvjvudC7+waW8tfptzjzmHKrXVrPlVltwwyPX8vi0J3nrjbda8RdZ2SnjhFzR2EFJJwNvAwskzZJ0OLAcGAGc2gLxWZnpv8cXqXqlilWvrqKmuobpf53BAUd8uV6fnfr15dnZzwHw7Ox5HHB43fGa6hqq19bdkGn3ufZUVKhlg7dMiFx10S1tGk3IwAXAXhHRE/ghcC/wvYgYFRHPNnt0Vna69ejK6pVr1m+vWbWGrj261uuzdPEyDhwxFIADRxzA1h22pmOnjgB079mN6x+6hjvn/IWbJ97q0bE1XdQW3wqQNFzSEklLJZ2/geN9JU2XtEDSTEm9k/0HS5qX1z6RdGyh6xVKyGsjYilAkoBfjoh7C/4Ks0ZMvOhPDN5vINc+eDWD9xvE6lVrqM3VPe66euUaRh92Bid/5TSGn3g4nbp2auVorezU1hbfGpGUaydSVxEYAJwiaUCDbpcBkyNiIDAemAAQETMiYnBEDAYOAT4CphUKvVANubukH+Vtb5e/HRFXbOSHjAHGAOy67RfosXWvQnFYRqx5/U269+y2frvbDt148/U36/V56423uOCMXwKw5VZbcNCRQ/ng/Q8/1WfFkhUM2nd3Zt73aLPHbRlSulkWQ4ClEbEcQNItwEhgcV6fAcC6nDgDuGcD5zkBuD8iPip0wUIj5GuADnmt4fYGRcSkiNg7IvZ2Mv5seXHei/TeuRc79OlB23ZtOXTkwTw+7Yl6fbbt1BGprj789XP+g6m3PABAtx260n6L9gBss+02DByyO68ue61lf4CVvxKNkIFeQP5fwMpkX775wHHJ51FAB0ldGvQ5GfhLMaE3OkKOiP8u5iRm6+Rytfz2gv/L5f/zayoqKrjv1vt55aV/8J2xo3lx/hJmP/Qke3x5MGPGfQcC5j+1gCv+6/cA9N21L2dfeCZBIMRfrr6N5S+uaOVfZGWnCSPk/H/NJyZFxKQmXG0scJWk0cCjQBWwfrk5STsAuwMPFhVPRDQW7IWNfDci4qJCFxja69CNX8DMLM9jVdM3e2rNx/ddWXTO2fLI8zZ6PUn7A7+MiCOS7XEAETFhI/23AV6MiN55+84FdouIMRv6TkOFShYfbqABfAf4WTEXMDNrUaWbZTEH6Cdp5+T5i5OBKfkdJHWVtC6PjgOua3COUyiyXAGFSxaX5124A3Au8C3gFuDyjX3PzKzVlOjBkIiokXQ2deWGNsB1EbFI0nhgbkRMAYYBEyQFdSWLs9Z9X9JOQB9gVrHXLOZJvc7U3UU8FbgB2DMi3in2AmZmLaqEa1lExFRgaoN9F+Z9vgO4YyPffYVP3wRsVKMJWdJvqLuDOAnYPSI+aMrJzcxaXFYfnQZ+DPSk7om9lZLeT9r/Snq/+cMzM2uiEj6p19IK1ZALJWwzs3SpqWntCDaZl980s2xpZCpv2jkhm1m2lHEN2QnZzLLFCdnMLCVSeLOuWE7IZpYtuVzhPinlhGxm2eKShZlZSjghm5mlhGvIZmbpELWeh2xmlg4uWZiZpYRnWZiZpYRHyGZmKeGEbGaWEl5cyMwsJTxCNjNLCU9727g5b73c3JcwM/sXz7IwM0uHcMnCzCwlXLIwM0sJr2VhZpYSHiGbmaVEjW/qmZmlg0sWZmYpUcYli4rWDsDMrJSitrboVoik4ZKWSFoq6fwNHO8rabqkBZJmSuqdd2xHSdMkvSBpsaSdCl3PCdnMsqU2im+NkNQGmAiMAAYAp0ga0KDbZcDkiBgIjAcm5B2bDPwmIvoDQ4DVhUJ3QjazbClRQqYuiS6NiOURsRa4BRjZoM8A4JHk84x1x5PE3TYiHgKIiA8i4qNCF3RCNrNsyeWKbpLGSJqb18bknakX8FredmWyL9984Ljk8yigg6QuwOeBdyXdJek5Sb9JRtyN8k09M8uUprxTLyImAZM243JjgaskjQYeBaqAHHW5dSiwB/AqcCswGri2sZM5IZtZtpRulkUV0Cdvu3eyb72IWEkyQpa0DXB8RLwrqRKYFxHLk2P3APtRICG7ZGFm2VJbW3xr3Bygn6SdJbUHTgam5HeQ1FXSujw6Drgu77vbSeqWbB8CLC50QSdkM8uWEt3Ui4ga4GzgQeAF4LaIWCRpvKRjkm7DgCWSXgK2By5JvpujrpwxXdJCQMA1hUJXNPPrTrbYYsfynaVtZi3qk09e1eae43/PHF50zulw9QObfb1Scg3ZzDIlcn502swsHcr40WknZDPLlKZMe0sbJ2QzyxYnZDOzlCjfErITspllS9SUb0Z2QjazbCnffLzpD4ZI2rGUgVh2HHbYQSxYMINFix5l7Njvf+r4jjv24v77/8KcOQ8ybdqt9OrVA4CBAwcwc+bdPPvsw8yZ8yAnnHB0S4duGRC1UXRLm4IPhkjan7oVjh6NiNWSBgLnA0Mjok+jX8YPhnzWVFRU8PzzszjyyFOprFzF7Nn3ctpp5/Diiy+v73PzzX/k/vunc9NNdzBs2Jc57bST+Pa3z2PXXXcmIli27BV22GF7nnjiPgYPPoT33nu/FX+RtaRSPBjyzvHDis45ne6cmaoHQxodIUv6DXXPZh8P3CfpYmAa8HegX/OHZ+Vmn30Gs2zZK6xY8SrV1dXcfvu9HH304fX69O/fj5kzZwMwc+YTHHXUYQAsXbqCZcteAWDVqjdYs+ZNunbt3KLxW/kr5xFyoZLFkcAeEXEKcDhwHrBfRPwuIj5p9uis7PTs2YPKypXrt6uqVtGz5/b1+ixcuJiRI0cAMHLkcDp27EDnztvV67P33oNo374dy5f/o/mDtmypbUJLmUIJ+ZN1iTci3gFejohXCp00f9HnXO6DEoRpWXL++ZcwdOi+PPXUVIYO3Y/KylXk8h537dGjO9dddyVjxoyluddaseyJmuJb2hSaZbGLpPzl5nbO346IYzbwnXqLPruG/NmycuXr9O7dc/12r147sHLlG/X6rFr1Bief/F0Att56K449dsT6OnGHDttw991/5he/+A1PP/1cywVumREpHPkWq1BCbvj+qMubKxDLhrlz57Prrjuz0059qKp6nRNPPJpvfvMH9fp06dKJt99+l4jgpz89i8mTbwWgXbt23HbbNdx8813cfffU1gjfsiDDCXlFRLzaIpFYJuRyOc477+fce++NtGnThhtuuJUXXniJCy/8Ec88s5D77nuIAw/cn4su+hkRweOP/51zz/05ACeccBQHHDCEzp234xvfOAGAM874MQsWFFzX22y9ch4hNzrtTdKzEbFn8vnOiDi+qRdwycLMilWKaW+rDz2o6JzTffqsVE17KzRCzg92l+YMxMysFCKXqhzbJIUScmzks5lZKpVzyaJQQh4k6X3qRspbJp9JtiMiOjZrdGZmTRS1GR0hR0SblgrEzKwUsjxCNjMrKxEZHSGbmZUbj5DNzFKiNsOzLMzMykpmb+qZmZUbJ2Qzs5Qo5wUCN/kVTmZmaRS1KroVImm4pCWSlko6fwPH+0qaLmmBpJmSeucdy0mal7QpDb+7IR4hm1mmlGram6Q2wETgMKASmCNpSkTkr3Z1GTA5Im6QdAgwAfhGcuzjiBjclGt6hGxmmZLLqehWwBBgaUQsj4i1wC18ekniAcAjyecZGzjeJE7IZpYpESq65b/dKGlj8k7VC3gtb7sy2ZdvPnBc8nkU0EFSl2R7i+ScT0k6tpjYXbIws0xpyiyL/LcbbaKxwFWSRgOPAlVALjnWNyKqJO0CPCJpYUQsa+xkTshmliklnGVRBfTJ2+6d7Mu7VqwkGSFL2gY4PiLeTY5VJX8ulzQT2ANoNCG7ZGFmmVLCWRZzgH6SdpbUHjgZqDdbQlJXSevy6DjgumR/J0mfW9cH+ApQ8NU3HiGbWabkakszzoyIGklnAw8CbYDrImKRpPHA3IiYAgwDJkgK6koWZyVf7w/8SVItdQPfSxvMztigRl/hVAp+hZOZFasUr3BasNPRReecga/cm6rH+jxCNrNMqfXym2Zm6eD1kM3MUqKc17Jo9oRcU5sr3MnMrERcsjAzS4lSzbJoDU7IZpYpZVyxcEI2s2xxycLMLCU8y8LMLCXK+KXTTshmli2BR8hmZqlQ45KFmVk6eIRsZpYSriGbmaWER8hmZinhEbKZWUrkPEI2M0uHJrzjNHWckM0sU2o9QjYzSwcvLmRmlhK+qWdmlhK1csnCzCwVyvkdRU7IZpYpnmVhZpYSnmVhZpYSnmVhZpYS5VyyaPLrWSVVSDq1OYKxbDji8GEsev5RXlz8OD/9yVmfOr7jjr2Y9sCtPPvMQ0x/6HZ69doBgEGDduPxR6cwf94jPPvMQ5x44jEtHbplQG0TWiGShktaImmppPM3cLyvpOmSFkiaKal3g+MdJVVKuqqY2DeakJMTjZN0laTDVeccYDlwUjEnt8+eiooKfv+7Szjq6K+z+6CD+drXjqV//371+vyfX1/IjTffwZ57HcbFl1zJJRePA+Cjjz5m9LfPZdDgQzjyqK9zxWW/ZNttO7bGz7AyllPxrTGS2gATgRHAAOAUSQMadLsMmBwRA4HxwIQGxy8CHi029sZGyDcCXwAWAqcDM4ATgGMjYmSxF7DPliH77MGyZa+wYsWrVFdXc9ttf+WYo4+o16d//37MmDEbgBkzZ3PM0YcD8PLLy1m6dAUAq1a9weo1b9GtW5eW/QFW9ko4Qh4CLI2I5RGxFrgFaJj7BgCPJJ9n5B+XtBewPTCt2NgbS8i7RMToiPgTcEpy4SMiYl6xJ7fPnp69evBa5cr125VVq+jZs0e9PgsWLGbUsSMAOPbYEXTs2IHOnTvV67PP3oNp374dy5a90uwxW7Y0JSFLGiNpbl4bk3eqXsBreduVyb5884Hjks+jgA6SukiqAC4HxjYl9sYScvW6DxGRAyoj4pNiTpr/I2trP2xKPPYZ8NOfXcSBB+7HnKcf5MCh+1FZuYpc7l/T+Xv06M711/+e00//ERHlfM/cWkOoCS1iUkTsndcmNfFyY4GDJD0HHARUUfdsyveBqRFR2ZSTNTbLYrCk95PPArZMtgVERGy0uJf8qEkAbdv38v9RnyErq16nT++e67d799qBlStfr9dn1ao3OPGkMwDYeuutOG7Ukbz3Xt1ftQ4dtmHKXyfz8wt/zd+ffrblArfMKOFaFlVAn7zt3sm+9SJiJckIWdI2wPER8a6k/YGhkr4PbAO0l/RBRHzqxmC+xkbI8yOiY9I6RETbvM++02IbNGfuPHbddWd22qkP7dq146STRnLv3+qX0Lp06YSS9QbO/9k5XH/DLQC0a9eOO2+/lptuuoO77rqvxWO3bMg1oRUwB+gnaWdJ7YGTgSn5HSR1TcoTAOOA6wAi4tSI2DEidqJuFD25UDKGxhOyR7bWZLlcjnPPu4Cp9/0Pzy+YyR133MvixS/xy1+M5aijDgPgoIO+zOLnH2Pxosfo3r0rv5rwewBOPPFohg7dl9NOO4m5c6Yxd840Bg3arTV/jpWhWhXfGhMRNcDZwIPAC8BtEbFI0nhJ6+ZkDgOWSHqJuht4l2xO7NpYjU5SJXBFI8Fu9Fg+lyzMrFg1a6s2+7GO3+749aJzzg9fvSlVj5E0VkNuQ13tI1UBm5k1JqvrIa+KiPEtFomZWQmU8z/JG0vIHhmbWdkp57UsGkvIh7ZYFGZmJZLJBeoj4u2WDMTMrBRqy7ho4eU3zSxTsnpTz8ys7JTv+NgJ2cwyxiNkM7OUqFH5jpGdkM0sU8o3HTshm1nGuGRhZpYSnvZmZpYS5ZuOnZDNLGNcsjAzS4lcGY+RnZDNLFM8QjYzS4nwCNnMLB08QjYzSwlPezMzS4nyTcdOyGaWMTVlnJKdkM0sU3xTrxEfr3ysuS9hZraeb+qZmaWER8hmZinhEbKZWUrkwiNkM7NUKOd5yBWtHYCZWSlFE/4rRNJwSUskLZV0/gaO95U0XdICSTMl9c7b/6ykeZIWSTqzmNidkM0sU2qb0BojqQ0wERgBDABOkTSgQbfLgMkRMRAYD0xI9q8C9o+IwcC+wPmSehaK3QnZzDKllii6FTAEWBoRyyNiLXALMLJBnwHAI8nnGeuOR8TaiPhnsv9zFJlrnZDNLFOaUrKQNEbS3Lw2Ju9UvYDX8rYrk3355gPHJZ9HAR0kdQGQ1EfSguQcv46IlYVi9009M8uUpsyyiIhJwKTNuNxY4CpJo4FHgSogl5z7NWBgUqq4R9IdEfFGYydzQjazTCnhLIsqoE/edu9k33rJqPc4AEnbAMdHxLsN+0h6HhgK3NHYBV2yMLNMKdVNPWAO0E/SzpLaAycDU/I7SOoqaV0eHQdcl+zvLWnL5HMn4ABgSaELOiGbWaaUatpbRNQAZwMPAi8At0XEIknjJR2TdBsGLJH0ErA9cEmyvz/wd0nzgVnAZRGxsFDsimZ+qqX6zeXlO0vbzFpUu667aHPP8dUdv1p0zpn66tTNvl4puYZsZpnS3IPM5uSEbGaZkivjR6edkM0sU8p5LQsnZDPLFJcszMxSwiNkM7OU8BtDzMxSwgvUm5mlhEsWZmYp4YRslufxp+Zy6ZVXk6ut5fijh3P6N06qd3zl62/w81/9lrfffY9tO3bg0gt/Qo/u3QC4fOK1PPrE09RGsP8+ezDuvDORUvUwlaVcOc+y2KS1LCQdJumhUgdj5S+Xy3Hx5RP54+UXMeXmPzH14ZksW/GPen0uu+r/cczwQ7l78h/53rf+gyuvvh6A5xYu5rmFi7lr8h+458Y/suiFl5jzXMHH/83qKeEC9S2u0YQs6RBJL0n6QNJNknaXNBe4FPhjy4Ro5WThCy+xY++e9Om1A+3atWPEoQfxyGNP1euzbMWrDNlrMABD9hzEjMeeBEASa9eupbqmhrXV1VTX5OjSebsW/w1W3kr5Tr2WVmiEfDkwBuhC3TqeTwLXR8ReEXFXcwdn5Wf1mjfXlx8Atu/eldVr3qrX5wv9duHhWbMBeHjWE3z40ce8+977DP5Sf/bZcyAHH3MqBx9zKl/Zd0/+bacdWzR+K3+5qC26pU2hhBwRMTMi/hkR9wBVEXFVSwRm2TX2rNOZ+9xCThh9FnPnLWT7bl2oqKjg1cqVLH/lNabffSOP3HMTTz8zn2fmPd/a4VqZiYiiW9oUuqm3naTj8rbb5W9vbJScvJdqDMAfLr+Y0087ZbMDtfLQvVtXXl+9Zv32G6vfpHu3Lg36dOF3E34OwEcffczDMx+nY4dtuGPKAwza7YtstdWWAByw397MX/QCew3+Usv9ACt7aawNF6vQCHkWcFRemwUcnbSjNvaliJgUEXtHxN5Oxp8tX/ri53m1ciWVK1+nurqa+6fP4uAD9qvX551336O2tu6fi9fceCujjjwcgB2278bceQupqclRXVPD3HkL2aVvn09dw6wx5VxDLjRCbvjvxVrgTeDxiFjRPCFZOWvbtg3/+cPv8d0fXUAul2PUUYez6y59ueqayez2xc9z8ND9mPPcAq68+noksdegL3HBj78PwOEHH8DTz85n1GnfQ4ID9t2bYQ2SuVkhtSksRRSr0TeGSPrFBnZ3Bo4AfhkRtxS6gN8YYmbFKsUbQ3bbft+ic86iN/6eqknujY6QI+K/N7RfUmfgYaBgQjYza0lpnD1RrE16Ui8i3pYfnzKzFCrnksUmJWRJBwPvlDgWM7PNlsabdcVqNCFLWgif+nWdgZXAac0VlJnZpsryCLnh1LYA3oqID5spHjOzzZLZEXJE/KOx42ZmaZOLXGuHsMm8/KaZZUoaH4kulhOymWVKlh+dNjMrK6VcXEjScElLJC2VdP4GjveVNF3SAkkzJfVO9g+W9KSkRcmxrxUTuxOymWVKbUTRrTGS2gATgRHAAOAUSQMadLsMmBwRA4HxwIRk/0fAaRGxGzAcuFJSwcW9nZDNLFNKuLjQEGBpRCyPiLXUPZk8skGfAcAjyecZ645HxEsR8XLyeSWwGuhGAU7IZpYpTVmgXtIYSXPz2pi8U/UCXsvbrkz25ZsPrFuSeBTQQVK99WYlDQHaA8sKxe6bemaWKU2ZZRERk4BJm3G5scBVkkYDjwJVwPp5d5J2AG4EvhlReJENJ2Qzy5QSPqlXBeQvyN072bdeUo44DkDSNsDxEfFust0RuA/4r4io/2LJjXDJwswypYSzLOYA/STtLKk9cDIwJb+DpK6S1uXRccB1yf72wN3U3fC7o9jYnZDNLFNqiaJbYyKiBjgbeBB4AbgtIhZJGi/pmKTbMGCJpJeA7YFLkv0nAQcCoyXNS9rgQrE3ukB9KXiBejMrVikWqO+49S5F55z3P1yeqmWEXUM2s0z5zC1Qb2aWVlleftPMrKx4cSEzs5TI7HrIZmblxiNkM7OUKOcacrNPezPbEEljksdWzSzhB0OstYwp3MXss8UJ2cwsJZyQzcxSwgnZWovrx2YN+KaemVlKeIRsZpYSTshmZinhhGxFk/QDSS9IujN5xfk/JY1t7bjMssJP6llTfB/4d2At0Bc4tiUvLqltsmi4WSZ5hGxFkXQ1sAtwP3BqRMwBqgt856C8tyU8J6lDsv9nkhZKmi/p0mTfYElPSVog6W5JnZL9MyVdKWkucK6kvSTNkvSMpAeTl0iaZYJHyFaUiDhT0nDg4Ih4s8ivjQXOiojZyQsgP5E0AhgJ7BsRH0nqnPSdDJwTEbMkjQd+AZyXHGsfEXtLagfMAkZGxBpJX6PulTnfLtHPNGtVTsjWnGYDV0i6GbgrIiol/Tvw54j4CCAi3pa0LbBdRMxKvncDcHveeW5N/vwC8CXgIUkAbYBVLfA7zFqEE7KVjKSzgDOSza9GxKWS7gO+CsyWdMQmnvrDdZcAFkXE/psZqlkquYZsJRMREyNicNJWSvq3iFgYEb+m7pXqXwQeAr4laSsASZ0j4j3gHUlDk1N9g7rSRENLgG6S9k++207Sbs3+w8xaiEfI1mSSegBzgY5AraTzgAER8X6DrudJOhioBRYB90fEP5PXoc+VtBaYCvwn8E3g6iRRLwe+1fC6EbFW0gnA75MyR1vgyuTcZmXPj06bmaWESxZmZinhhGxmlhJOyGZmKeGEbGaWEk7IZmYp4YRsZpYSTshmZinx/wHnd/hb1M39/QAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "zxfJI0JdZEXt",
        "outputId": "8514d933-6b09-4dea-8f82-b25a5616c99a"
      },
      "source": [
        "df_wb_vis = df_wb.drop(['accuracy', 'macro avg', 'weighted avg'])\n",
        "df_wb_vis = df_wb_vis.drop(['support', 'precision', 'recall'], axis = 1)\n",
        "sns.heatmap(df_wb_vis, annot = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f3d20dd4a90>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}